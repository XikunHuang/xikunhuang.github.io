<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://xikunhuang.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://xikunhuang.github.io/" rel="alternate" type="text/html" /><updated>2020-08-03T06:26:59+00:00</updated><id>https://xikunhuang.github.io/feed.xml</id><title type="html">Xikun</title><subtitle>A personal website.</subtitle><author><name>  黄锡昆</name></author><entry><title type="html">个人主页搭建记录之GitHub Pages/Actions + Jekyll + Minimal Mistakes + Pandoc</title><link href="https://xikunhuang.github.io/%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/" rel="alternate" type="text/html" title="个人主页搭建记录之GitHub Pages/Actions + Jekyll + Minimal Mistakes + Pandoc" /><published>2020-07-31T00:00:00+00:00</published><updated>2020-07-31T00:00:00+00:00</updated><id>https://xikunhuang.github.io/%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95</id><content type="html" xml:base="https://xikunhuang.github.io/%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/">&lt;h1 data-number=&quot;1&quot; id=&quot;为什么要搭建个人主页&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;1&lt;/span&gt; 为什么要搭建个人主页&lt;/h1&gt;
&lt;p&gt;想了一下主要有以下几个原因:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;提醒自己及时&lt;strong&gt;记录&lt;/strong&gt;所看所思所想.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;练习&lt;/strong&gt;写作能力.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;分享&lt;/strong&gt;自己学习到的知识.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;最近越发觉得及时记录自己学到的知识是很重要的一件事, 一方面是好记性不如烂笔头(况且记性还越来越差-.-), 及时记录下来之后方便以后查阅, 因为有些问题过段时间后可能会再次遇到, 查阅之前的记录会大大缩短解决问题需要的时间. 另一方面是在记录的过程中可以训练自己写作的能力, 比如用清晰简洁的语言描述遇到的问题和相应的解决方案. 在写作的过程中可能还会加深对相关知识的理解, 因为写作的过程会不断的思考. 而且我认为写作是需要不断练习才能驾轻就熟的. 此外, 分享也是我创建个人主页的原因. 我在网上寻找相关知识或一些问题的解决方案时发现了很多优质的博客, 他们把自己的知识无偿的分享出来, 从中我学到了很多, 我非常认同这种分享精神, 所以我也想把自己的一些经验分享出来, 如果能帮到别人, 那将是令我非常开心的一件事.&lt;/p&gt;
&lt;h1 data-number=&quot;2&quot; id=&quot;希望有一个什么样的个人主页&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;2&lt;/span&gt; 希望有一个什么样的个人主页&lt;/h1&gt;
&lt;p&gt;我对自己的主页有以下几个期待:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;专注于内容&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;方便维护&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;页面简洁&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;有一些方便的功能如文章标签, 搜索, 评论等&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;可以被搜索引擎检索到&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我希望自己的主页专注于内容, 一方面是在主页里以博客为主, 另一方面是希望可以很简单的将博客发布到个人网页, 这里的简单是指不必改变自己现在的写作环境(Markdown+Pandoc)以及写完后不需要做什么额外的操作, 维护个人主页不需要花费很多的精力. 相比于外表非常酷炫的网页, 我更喜欢简洁的风格. 此外, 为了更好地使用个人主页, 我认为文章标签, 搜索, 评论等功能还是有必要的. 当然, 如果搜索引擎检索不到自己的网页, 那么分享的功能会大打折扣.&lt;/p&gt;
&lt;h1 data-number=&quot;3&quot; id=&quot;搭建过程&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;3&lt;/span&gt; 搭建过程&lt;/h1&gt;
&lt;h2 data-number=&quot;3.1&quot; id=&quot;工具的选择&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;3.1&lt;/span&gt; 工具的选择&lt;/h2&gt;
&lt;p&gt;我的电脑系统是Ubuntu18.04, 我最终采用的方案是: &lt;strong&gt;GitHub Pages/Actions + Jekyll + Minimal Mistakes + Pandoc&lt;/strong&gt;, 原因如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;GitHub Pages&lt;/p&gt;
&lt;p&gt;我暂时不想自己购买云服务器, 所以GitHub Pages自然成了一个很好的选择.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hexo or Jekyll&lt;/p&gt;
&lt;p&gt;之前用Hexo搭建过, 后来没有坚持更新(第一次搭建都会遇到的问题?哈哈), 还有一个重要的原因是我觉得发布博客有点费事. 所以这次我就试了试Jekyll, 它专注于内容的特性吸引了我.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;为什么不用Gem包github-pages&lt;/p&gt;
&lt;p&gt;github-pages限制较多, 只能使用部分插件和版本, 主题使用也受限, 虽然可以通过remote theme解决部分主题的问题, 但是本地测试还是麻烦.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Minimal Mistakes&lt;/p&gt;
&lt;p&gt;页面简洁, 功能丰富, 文档清晰, 用户众多, 并且一直被主题作者很好地维护着.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;GitHub Actions&lt;/p&gt;
&lt;p&gt;我的理解是免费给用户配了一台远程服务器. 解决了上述gh-pages的限制, 逻辑上就是将源文件放在gh-pages分支中, 本地修改推送到gh-pages分支, 然后gh-pages分支的更新会触发GitHub Actions工作流配置的一些操作, 这里是在远程服务器上下载Pandoc, Ruby, Jekyll. 将gh-pages分支中的源文件处理成静态网站文件推送到master分支.&lt;/p&gt;
&lt;p&gt;题外话, 如果是为非username.github.io的仓库创建网页(比如为某个仓库myproject创建文档), 那么可以将源文件放在gh-pages分支中, 然后将生成后的静态网页文件部署在master分支中的docs文件夹内. 在myproject仓库设置中设置github pages网址为https://username.github.io/docs. 也可以直接将网站部署在gh-pages分支上, 这样需要在myproject仓库设置中设置github pages网址为gh-pages分支.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pandoc&lt;/p&gt;
&lt;p&gt;文件转换的瑞士军刀, 非常强大的软件. 我选择它的主要原因是在处理Markdown文件时对&lt;span class=&quot;math inline&quot;&gt;\(\LaTeX\)&lt;/span&gt;数学公式支持的很好, 可以使用和&lt;span class=&quot;math inline&quot;&gt;\(\LaTeX\)&lt;/span&gt;一样的数学环境语法处理行内公式和数学环境(单美元符号和双美元符号).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 data-number=&quot;3.2&quot; id=&quot;具体搭建过程&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;3.2&lt;/span&gt; 具体搭建过程&lt;/h2&gt;
&lt;p&gt;搜索关键词GitHub Pages + Jekyll, 可以找到很多搭建教程, 比如这几个. 我的搭建过程和这些教程里介绍的差不多, 所以这里我就不写的那么详细了, 我主要是记录一下我做的一些更改, 包括使用Pandoc作为markdown解释器和使用GitHub Actions进行网站的自动部署.&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;创建GitHub仓库&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在GitHub上新建一个名为 username.github.io 的空的仓库, 这里username替换为自己的github名称. 然后在该仓库中创建一个名为gh-pages的分支. 将gh-pages分支clone到本地, 以后的操作全部在gh-pages分支中. master分支只是用来发布最后生成的静态网站(由GithHub Actions工作流自动完成).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;安装本地测试环境&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;本地安装&lt;a href=&quot;https://www.ruby-lang.org/en/&quot;&gt;Ruby&lt;/a&gt;, &lt;a href=&quot;https://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt;, &lt;a href=&quot;https://pandoc.org/&quot;&gt;Pandoc&lt;/a&gt;用于测试, 具体安装方法可以参考对应的官方网站安装指南.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Jekyll主题 Minimal Mistakes&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;GitHub下载&lt;a href=&quot;https://github.com/mmistakes/minimal-mistakes&quot;&gt;Minimal Mistakes&lt;/a&gt;的仓库, 将&lt;a href=&quot;https://github.com/mmistakes/minimal-mistakes/tree/master/docs&quot;&gt;docs文件夹&lt;/a&gt;内的内容全部放到之前clone下来的username.github.io文件夹内.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;配置Minimal Mistakes主题&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;添加&lt;a href=&quot;https://jekyllrb.com/docs/&quot;&gt;jekyll-pandoc&lt;/a&gt;插件, Gemfile和_config.yml文件中都要添加. 然后在_config.yml中修改markdown引擎为Pandoc, 并进行Pandoc命令的配置. 具体配置见后文.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;_config.yml中主题选择使用theme而不是remote theme.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Gemfile中注释掉 gem “github-pages”, group: :jekyll_plugins, 替换为 gem “jekyll”.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;本地测试&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在username.github.io文件夹根目录下终端执行以下命令:&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb1&quot;&gt;&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span id=&quot;cb1-1&quot;&gt;&lt;a href=&quot;#cb1-1&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;co&quot;&gt;# 安装gem包&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb1-2&quot;&gt;&lt;a href=&quot;#cb1-2&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;ex&quot;&gt;bundle&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb1-3&quot;&gt;&lt;a href=&quot;#cb1-3&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;co&quot;&gt;# 运行服务&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb1-4&quot;&gt;&lt;a href=&quot;#cb1-4&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;ex&quot;&gt;bundle&lt;/span&gt; exec jekyll serve&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;成功的话会生成_sites文件夹, 浏览器访问 https://localhost:4000 即可查看网页.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;创建GitHub Actions自动部署流程&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在username.github.io文件夹下创建 ./github/workflows/gh-pages.yml 文件, 我使用的配置为:&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb2&quot;&gt;&lt;pre class=&quot;sourceCode yaml&quot;&gt;&lt;code class=&quot;sourceCode yaml&quot;&gt;&lt;span id=&quot;cb2-1&quot;&gt;&lt;a href=&quot;#cb2-1&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;fu&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; Build and deploy jekyll site&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-2&quot;&gt;&lt;a href=&quot;#cb2-2&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-3&quot;&gt;&lt;a href=&quot;#cb2-3&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;fu&quot;&gt;on&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-4&quot;&gt;&lt;a href=&quot;#cb2-4&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;push&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-5&quot;&gt;&lt;a href=&quot;#cb2-5&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;branches&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-6&quot;&gt;&lt;a href=&quot;#cb2-6&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;            &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; gh-pages&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-7&quot;&gt;&lt;a href=&quot;#cb2-7&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-8&quot;&gt;&lt;a href=&quot;#cb2-8&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;fu&quot;&gt;jobs&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-9&quot;&gt;&lt;a href=&quot;#cb2-9&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;jekyll&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-10&quot;&gt;&lt;a href=&quot;#cb2-10&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;runs-on&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; ubuntu-18.04&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-11&quot;&gt;&lt;a href=&quot;#cb2-11&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;steps&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-12&quot;&gt;&lt;a href=&quot;#cb2-12&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;            &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; &lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; 📂 setup&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-13&quot;&gt;&lt;a href=&quot;#cb2-13&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;                &lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;uses&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; actions/checkout@v2&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-14&quot;&gt;&lt;a href=&quot;#cb2-14&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-15&quot;&gt;&lt;a href=&quot;#cb2-15&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;            &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; &lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; setup pandoc&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-16&quot;&gt;&lt;a href=&quot;#cb2-16&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;                &lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;uses&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; r-lib/actions/setup-pandoc@v1&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-17&quot;&gt;&lt;a href=&quot;#cb2-17&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;                &lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;with&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-18&quot;&gt;&lt;a href=&quot;#cb2-18&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;                &lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;pandoc-version&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;#39;2.7.3&amp;#39;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-19&quot;&gt;&lt;a href=&quot;#cb2-19&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-20&quot;&gt;&lt;a href=&quot;#cb2-20&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;            &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; &lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; 💎 setup ruby&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-21&quot;&gt;&lt;a href=&quot;#cb2-21&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;                &lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;uses&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; ruby/setup-ruby@v1&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-22&quot;&gt;&lt;a href=&quot;#cb2-22&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;                &lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;with&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-23&quot;&gt;&lt;a href=&quot;#cb2-23&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;                &lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;ruby-version&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; &lt;/span&gt;&lt;span class=&quot;fl&quot;&gt;2.6&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-24&quot;&gt;&lt;a href=&quot;#cb2-24&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-25&quot;&gt;&lt;a href=&quot;#cb2-25&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;            &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; &lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; 🔨 install dependencies &amp;amp; build site&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-26&quot;&gt;&lt;a href=&quot;#cb2-26&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;                &lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;uses&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; limjh16/jekyll-action-ts@v2&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-27&quot;&gt;&lt;a href=&quot;#cb2-27&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;                &lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;with&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-28&quot;&gt;&lt;a href=&quot;#cb2-28&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;                &lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;enable_cache&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; &lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;true&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-29&quot;&gt;&lt;a href=&quot;#cb2-29&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-30&quot;&gt;&lt;a href=&quot;#cb2-30&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;            &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; &lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; 🚀 deploy&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-31&quot;&gt;&lt;a href=&quot;#cb2-31&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;                &lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;uses&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; peaceiris/actions-gh-pages@v3&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-32&quot;&gt;&lt;a href=&quot;#cb2-32&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;                &lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;with&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-33&quot;&gt;&lt;a href=&quot;#cb2-33&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;                &lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;github_token&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; $&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-34&quot;&gt;&lt;a href=&quot;#cb2-34&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;                &lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;publish_dir&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; ./_site&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb2-35&quot;&gt;&lt;a href=&quot;#cb2-35&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;                &lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;publish_branch&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; master&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;上述配置文件参考自 &lt;a href=&quot;https://github.com/limjh16/jekyll-action-ts&quot;&gt;limjh16/jekyll-action-ts&lt;/a&gt;, &lt;a href=&quot;https://github.com/peaceiris/actions-gh-pages&quot;&gt;peaceiris/actions-gh-pages&lt;/a&gt; 和 &lt;a href=&quot;https://github.com/r-lib/actions&quot;&gt;r-lib/actions&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;推送到GitHub&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在username.github.io文件夹下终端运行git推送到远程gh-pages分支, 然后就可以去GitHub网站上的username.github.io仓库中的Actions页面查看进度, 成功完成之后就可以 https://username.github.io 访问主页了.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb3&quot;&gt;&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span id=&quot;cb3-1&quot;&gt;&lt;a href=&quot;#cb3-1&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;co&quot;&gt;# 确保是在gh-pages分支&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb3-2&quot;&gt;&lt;a href=&quot;#cb3-2&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;fu&quot;&gt;git&lt;/span&gt; checkout gh-pages&lt;/span&gt;
&lt;span id=&quot;cb3-3&quot;&gt;&lt;a href=&quot;#cb3-3&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;co&quot;&gt;# 添加&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb3-4&quot;&gt;&lt;a href=&quot;#cb3-4&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;fu&quot;&gt;git&lt;/span&gt; add .&lt;/span&gt;
&lt;span id=&quot;cb3-5&quot;&gt;&lt;a href=&quot;#cb3-5&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;co&quot;&gt;# 提交&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb3-6&quot;&gt;&lt;a href=&quot;#cb3-6&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;fu&quot;&gt;git&lt;/span&gt; commit -m &lt;span class=&quot;st&quot;&gt;&amp;quot;initial commit&amp;quot;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb3-7&quot;&gt;&lt;a href=&quot;#cb3-7&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;co&quot;&gt;# 推送&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb3-8&quot;&gt;&lt;a href=&quot;#cb3-8&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;fu&quot;&gt;git&lt;/span&gt; push origin gh-pages&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 data-number=&quot;4&quot; id=&quot;发布博客的流程&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;4&lt;/span&gt; 发布博客的流程&lt;/h1&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;使用VSCode写markdown文件, 搭配Pandoc预览.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;将写好的balbla.md文件重命名为Year-Month-Day-blabla.md的形式, 放到主页存放博客的文件夹内(_posts)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;这一步可选. 发布到GitHub Pages之前在本地预览效果, 在主页源文件根目录终端运行:&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb4&quot;&gt;&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span id=&quot;cb4-1&quot;&gt;&lt;a href=&quot;#cb4-1&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;ex&quot;&gt;bundle&lt;/span&gt; exec jekyll serve&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;使用git将博客发布到GitHub Pages. 在主页源文件根目录执行git add, commit, push origin gh-pages操作.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 data-number=&quot;5&quot; id=&quot;遇到的问题及解决方案&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;5&lt;/span&gt; 遇到的问题及解决方案&lt;/h1&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;Markdown文件中使用&lt;span class=&quot;math inline&quot;&gt;\(\LaTeX\)&lt;/span&gt; 数学公式的语法以及后续的渲染.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;使用 jekyll-pandoc解决, Gemfile和_config.yml中添加上该插件. 并且在_config.yml中配置Pandoc&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb5&quot;&gt;&lt;pre class=&quot;sourceCode yaml&quot;&gt;&lt;code class=&quot;sourceCode yaml&quot;&gt;&lt;span id=&quot;cb5-1&quot;&gt;&lt;a href=&quot;#cb5-1&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;co&quot;&gt;# Conversion&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb5-2&quot;&gt;&lt;a href=&quot;#cb5-2&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;fu&quot;&gt;markdown&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; Pandoc&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb5-3&quot;&gt;&lt;a href=&quot;#cb5-3&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;fu&quot;&gt;highlighter&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; rouge&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb5-4&quot;&gt;&lt;a href=&quot;#cb5-4&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;fu&quot;&gt;lsi&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; &lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;false&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb5-5&quot;&gt;&lt;a href=&quot;#cb5-5&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;fu&quot;&gt;excerpt_separator&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;&amp;lt;!--more--&amp;gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;co&quot;&gt; # excerpt &amp;quot;\n\n&amp;quot;, set in archive-single.html&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb5-6&quot;&gt;&lt;a href=&quot;#cb5-6&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;fu&quot;&gt;incremental&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; &lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;false&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb5-7&quot;&gt;&lt;a href=&quot;#cb5-7&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id=&quot;cb5-8&quot;&gt;&lt;a href=&quot;#cb5-8&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;co&quot;&gt;# Markdown Processing&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb5-9&quot;&gt;&lt;a href=&quot;#cb5-9&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;fu&quot;&gt;pandoc&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb5-10&quot;&gt;&lt;a href=&quot;#cb5-10&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;extensions&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;:&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb5-11&quot;&gt;&lt;a href=&quot;#cb5-11&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; mathjax&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb5-12&quot;&gt;&lt;a href=&quot;#cb5-12&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; from=markdown&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb5-13&quot;&gt;&lt;a href=&quot;#cb5-13&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; to=html&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb5-14&quot;&gt;&lt;a href=&quot;#cb5-14&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; number-sections&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb5-15&quot;&gt;&lt;a href=&quot;#cb5-15&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;at&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;at&quot;&gt; shift-heading-level-by=-1&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;_includes/scripts.html 添加mathjax的链接. 关于mathjax的设置可以参考 &lt;a href=&quot;http://docs.mathjax.org/en/latest/web/components/combined.html#tex-chtml-full&quot;&gt;mathjax官方文档&lt;/a&gt;, mathjax对&lt;span class=&quot;math inline&quot;&gt;\(\LaTeX\)&lt;/span&gt;的支持可以参考 &lt;a href=&quot;http://docs.mathjax.org/en/latest/input/tex/macros/index.html&quot;&gt;Supported TeX/LaTeX commands&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb6&quot;&gt;&lt;pre class=&quot;sourceCode html&quot;&gt;&lt;code class=&quot;sourceCode html&quot;&gt;&lt;span id=&quot;cb6-1&quot;&gt;&lt;a href=&quot;#cb6-1&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id=&quot;cb6-2&quot;&gt;&lt;a href=&quot;#cb6-2&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;{% if page.mathjax %}&lt;/span&gt;
&lt;span id=&quot;cb6-3&quot;&gt;&lt;a href=&quot;#cb6-3&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;kw&quot;&gt;&amp;lt;script&amp;gt;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb6-4&quot;&gt;&lt;a href=&quot;#cb6-4&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;MathJax &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; {&lt;/span&gt;
&lt;span id=&quot;cb6-5&quot;&gt;&lt;a href=&quot;#cb6-5&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;    &lt;span class=&quot;dt&quot;&gt;tex&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;:&lt;/span&gt; {&lt;/span&gt;
&lt;span id=&quot;cb6-6&quot;&gt;&lt;a href=&quot;#cb6-6&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;        &lt;span class=&quot;dt&quot;&gt;inlineMath&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;:&lt;/span&gt; [[&lt;span class=&quot;st&quot;&gt;&amp;#39;$&amp;#39;&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;$&amp;#39;&lt;/span&gt;]&lt;span class=&quot;op&quot;&gt;,&lt;/span&gt; [&lt;span class=&quot;st&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;sc&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;(&amp;#39;&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;sc&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;)&amp;#39;&lt;/span&gt;]]&lt;/span&gt;
&lt;span id=&quot;cb6-7&quot;&gt;&lt;a href=&quot;#cb6-7&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;    }&lt;span class=&quot;op&quot;&gt;,&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb6-8&quot;&gt;&lt;a href=&quot;#cb6-8&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;    &lt;span class=&quot;dt&quot;&gt;chtml&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;:&lt;/span&gt; {&lt;/span&gt;
&lt;span id=&quot;cb6-9&quot;&gt;&lt;a href=&quot;#cb6-9&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;        &lt;span class=&quot;dt&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;fl&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;co&quot;&gt;// global scaling factor for all expressions&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb6-10&quot;&gt;&lt;a href=&quot;#cb6-10&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;    }&lt;/span&gt;
&lt;span id=&quot;cb6-11&quot;&gt;&lt;a href=&quot;#cb6-11&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;}&lt;span class=&quot;op&quot;&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb6-12&quot;&gt;&lt;a href=&quot;#cb6-12&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;kw&quot;&gt;&amp;lt;/script&amp;gt;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb6-13&quot;&gt;&lt;a href=&quot;#cb6-13&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id=&quot;cb6-14&quot;&gt;&lt;a href=&quot;#cb6-14&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;kw&quot;&gt;&amp;lt;script&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; type=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;text/javascript&amp;quot;&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; id=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;MathJax-script&amp;quot;&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt; async&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb6-15&quot;&gt;&lt;a href=&quot;#cb6-15&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;ot&quot;&gt;    src=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb6-16&quot;&gt;&lt;a href=&quot;#cb6-16&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;span class=&quot;kw&quot;&gt;&amp;lt;/script&amp;gt;&lt;/span&gt;&lt;/span&gt;
&lt;span id=&quot;cb6-17&quot;&gt;&lt;a href=&quot;#cb6-17&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;{% endif %}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;GitHub Actions 工作流中添加安装Pandoc这一步.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Markdown切换为Pandoc时文章目录生成错误.&lt;/p&gt;
&lt;p&gt;原因是Pandoc和Kramdown在Markdown文件中添加Html标签的语法不同, Minimal Mistakes支持Kramdown的语法. 我的解决方法是手动修改include/toc.html文件使得处理后的结果与Kramdown处理后的结果相同. 关键词: , liquid过滤器, filter replace.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb7&quot;&gt;&lt;pre class=&quot;sourceCode html&quot;&gt;&lt;code class=&quot;sourceCode html&quot;&gt;&lt;span id=&quot;cb7-1&quot;&gt;&lt;a href=&quot;#cb7-1&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id=&quot;cb7-2&quot;&gt;&lt;a href=&quot;#cb7-2&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;{% comment %}&lt;/span&gt;
&lt;span id=&quot;cb7-3&quot;&gt;&lt;a href=&quot;#cb7-3&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;    ## delete toc__menu, add it later using liquid command.&lt;/span&gt;
&lt;span id=&quot;cb7-4&quot;&gt;&lt;a href=&quot;#cb7-4&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;    {% if include.class and include.class != blank %}&lt;/span&gt;
&lt;span id=&quot;cb7-5&quot;&gt;&lt;a href=&quot;#cb7-5&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;        {% capture my_toc %}{:.{{ include.class }}}&lt;/span&gt;
&lt;span id=&quot;cb7-6&quot;&gt;&lt;a href=&quot;#cb7-6&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;        {{ my_toc | lstrip }}{% endcapture %}&lt;/span&gt;
&lt;span id=&quot;cb7-7&quot;&gt;&lt;a href=&quot;#cb7-7&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;    {% endif %}&lt;/span&gt;
&lt;span id=&quot;cb7-8&quot;&gt;&lt;a href=&quot;#cb7-8&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;{% endcomment %}&lt;/span&gt;
&lt;span id=&quot;cb7-9&quot;&gt;&lt;a href=&quot;#cb7-9&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id=&quot;cb7-10&quot;&gt;&lt;a href=&quot;#cb7-10&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;{% capture mytoc_html %}&lt;/span&gt;
&lt;span id=&quot;cb7-11&quot;&gt;&lt;a href=&quot;#cb7-11&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;{{ my_toc | markdownify | strip }}&lt;/span&gt;
&lt;span id=&quot;cb7-12&quot;&gt;&lt;a href=&quot;#cb7-12&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;{% endcapture %}&lt;/span&gt;
&lt;span id=&quot;cb7-13&quot;&gt;&lt;a href=&quot;#cb7-13&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt;{{ mytoc_html | replace_first: &amp;#39;ul&amp;#39;, &amp;#39;ul class=&amp;quot;toc__menu&amp;quot;&amp;#39; }}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;我对Minimal Mistakes主题作的其他修改&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;修改about文件&lt;/p&gt;
&lt;p&gt;在_pages/about.md中修改.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;添加图标&lt;/p&gt;
&lt;p&gt;使用的是&lt;a href=&quot;https://fontawesome.com/&quot;&gt;Font Awesome&lt;/a&gt;图标.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;个人头像&lt;/p&gt;
&lt;p&gt;使用&lt;a href=&quot;https://app.diagrams.net/&quot;&gt;draw.io&lt;/a&gt;画的.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;页脚添加Sitemap&lt;/p&gt;
&lt;p&gt;在 _includes/footer.html文件中修改.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;SEO, &lt;a href=&quot;https://www.google.com/webmasters/#?modal_active=none&quot;&gt;google webmaster tool&lt;/a&gt;, &lt;a href=&quot;https://www.bing.com/webmasters&quot;&gt;bing webmaster tool&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;评论系统, &lt;a href=&quot;https://disqus.com/&quot;&gt;Disqus&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;中文定制&lt;/p&gt;
&lt;p&gt;修改 _data/ui-text.yml, _data/navigation.yml文件&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;文章摘录&lt;/p&gt;
&lt;p&gt;excerpt 设置, excerpt_separator. 在_includes/archive-single.html中修改.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pandoc syntax highlight style and css&lt;/p&gt;
&lt;p&gt;我用了个非常简单粗暴的解决方法, 先随便创建一个包含代码块的md文件test.md, 然后运行&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb8&quot;&gt;&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span id=&quot;cb8-1&quot;&gt;&lt;a href=&quot;#cb8-1&quot; aria-hidden=&quot;true&quot;&gt;&lt;/a&gt; &lt;span class=&quot;ex&quot;&gt;pandoc&lt;/span&gt; test.md -s --highlight-style=zenburn -o test.html&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;将test.html里的样式代码复制出来保存成pandoc-zenburn-syntax-highlight.css文件放在assets/css/文件夹内. 在_includes/head/custom.html中添加刚刚创建的css文件链接.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>  黄锡昆</name></author><category term="jekyll" /><category term="github_pages" /></entry><entry><title type="html">MLAPP笔记-高斯模型</title><link href="https://xikunhuang.github.io/%E7%AC%94%E8%AE%B0/MLAPP%E7%AC%94%E8%AE%B0-%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/" rel="alternate" type="text/html" title="MLAPP笔记-高斯模型" /><published>2019-01-05T00:00:00+00:00</published><updated>2019-01-05T00:00:00+00:00</updated><id>https://xikunhuang.github.io/%E7%AC%94%E8%AE%B0/MLAPP%E7%AC%94%E8%AE%B0-%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B</id><content type="html" xml:base="https://xikunhuang.github.io/%E7%AC%94%E8%AE%B0/MLAPP%E7%AC%94%E8%AE%B0-%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/">&lt;h1 data-number=&quot;1&quot; id=&quot;简介&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;1&lt;/span&gt; 简介&lt;/h1&gt;
&lt;p&gt;本章讨论多元高斯或者称多元正态(MVN).&lt;/p&gt;
&lt;h2 data-number=&quot;1.1&quot; id=&quot;基础&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;1.1&lt;/span&gt; 基础&lt;/h2&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(D\)&lt;/span&gt;维MVN的pdf为&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[\mathcal{N}(x|\mu,\Sigma) = \frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}}\exp[-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;上述pdf式中指数里的表达式去掉&lt;span class=&quot;math inline&quot;&gt;\(-\frac{1}{2}\)&lt;/span&gt;后实际上就是&lt;span class=&quot;math inline&quot;&gt;\(x\)&lt;/span&gt;和&lt;span class=&quot;math inline&quot;&gt;\(\mu\)&lt;/span&gt;之间的&lt;a href=&quot;https://en.wikipedia.org/wiki/Mahalanobis_distance&quot;&gt;Mahalanobis距离&lt;/a&gt;, 关于这个距离更多的知识可以参考Wikipedia.&lt;/p&gt;
&lt;p&gt;现在考虑这样一个问题: MVN的pdf的等值线是什么样的呢?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果协方差矩阵&lt;span class=&quot;math inline&quot;&gt;\(\Sigma\)&lt;/span&gt;是对角阵, 那么Mahalanobis距离就变成了&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[(x-\mu)^T\Sigma^{-1}(x-\mu) = \sum_{i=1}^{D}\frac{1}{\lambda_i}(x_i-\mu_i)^2 = \sum_{i=1}^{D}\frac{1}{\lambda_i} y_i^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&quot;math inline&quot;&gt;\(\lambda_i\)&lt;/span&gt;是协方差矩阵的对角元素, &lt;span class=&quot;math inline&quot;&gt;\(y_i = x_i-\mu_i\)&lt;/span&gt; 在二维情形下这个等值线就是椭圆. &lt;span class=&quot;math inline&quot;&gt;\(\frac{1}{\lambda_1}y_1^2 + \frac{1}{\lambda_2} y_2^2 = k\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果协方差不是对角阵, 那么由于它是实对称矩阵, 因此可以正交分解为 &lt;span class=&quot;math inline&quot;&gt;\(\Sigma = U\Lambda U^T\)&lt;/span&gt; 此时&lt;span class=&quot;math inline&quot;&gt;\(y_i = u_i^T(x-\mu)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由此也可以看出Mahalanobis距离与欧氏距离的联系.&lt;/p&gt;
&lt;h2 data-number=&quot;1.2&quot; id=&quot;mle-for-an-mvn&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;1.2&lt;/span&gt; MLE for an MVN&lt;/h2&gt;
&lt;p&gt;最大似然估计是估计MVN的参数的方法之一. 但是最大似然估计有过拟合的缺点,后续会讨论MVN参数的Bayes推断, 这种方法可以消除过拟合, 并且可以为估计提供置信度.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;: 如果&lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt;个&lt;span class=&quot;math inline&quot;&gt;\(iid\)&lt;/span&gt;样本&lt;span class=&quot;math display&quot;&gt;\[\mathbb{x}_i\sim\mathcal{N}(\mu,\Sigma)\]&lt;/span&gt;, 那么MVN参数的最大似然估计为 &lt;span class=&quot;math display&quot;&gt;\[\hat{\mu}_{mle} = \frac{1}{N}\sum_{i=1}^{N}\mathbb{x}_i = \bar{\mathbb{x}}\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[\hat{\Sigma}_{mle} = \frac{1}{N}\sum_{i=1}^{N}(\mathbb{x_i}-\bar{\mathbb{x}})(\mathbb{x_i}-\bar{\mathbb{x}})^T\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 data-number=&quot;1.3&quot; id=&quot;多元高斯的一个有趣性质&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;1.3&lt;/span&gt; 多元高斯的一个有趣性质&lt;/h2&gt;
&lt;p&gt;对于给定数据, 我们从数据中可能仅仅能可靠地估计出均值与方差. 这个时候我们需要一个分布能够具有估计出的均值与方差, 同时又尽量做出最少的假设, 多元高斯就是我们需要的分布. &lt;strong&gt;对于给定的均值与方差,多元高斯分布是熵最大的分布&lt;/strong&gt;&lt;/p&gt;
&lt;h2 data-number=&quot;1.4&quot; id=&quot;高斯判别分析&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;1.4&lt;/span&gt; 高斯判别分析&lt;/h2&gt;
&lt;p&gt;多元高斯的一个重要应用是在生成式模型中定义类条件分布, 即 &lt;span class=&quot;math display&quot;&gt;\[p(\mathbb{x}|y=c,\theta)=\mathcal{N}(\mathbb{x}|\mu_c,\Sigma_c)\]&lt;/span&gt; 这个被成为高斯判别分析.(实际上是生成模型, 名字容易引起误导). 如果&lt;span class=&quot;math inline&quot;&gt;\(\Sigma_c\)&lt;/span&gt;是对角矩阵, 那么就相当于属性条件独立了, 即等价于朴素贝叶斯. 对于新来的数据, 我们根据下式来进行分类 &lt;span class=&quot;math display&quot;&gt;\[\hat{y}(\mathbb{x}) = \arg\max_c[\log p(y=c|\theta) + \log p(\mathbb{x}|y=c,\theta)] \]&lt;/span&gt; 其中在计算第二项时, 我们实际上是在计算&lt;span class=&quot;math inline&quot;&gt;\(\mathbb{x}\)&lt;/span&gt;与每个类的中心的距离, 只不过是Mahalanobis距离, 这个可以被看成是 nearest centroids classifier.&lt;/p&gt;
&lt;h3 data-number=&quot;1.4.1&quot; id=&quot;quadratic-discriminant-analysisqda&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;1.4.1&lt;/span&gt; Quadratic discriminant analysis(QDA)&lt;/h3&gt;
&lt;p&gt;高斯判别分析分类准则展开之后分类边界是关于&lt;span class=&quot;math inline&quot;&gt;\(\mathbb{x}\)&lt;/span&gt;的二次函数, 这个被称为QDA.&lt;/p&gt;
&lt;h3 data-number=&quot;1.4.2&quot; id=&quot;linear-discriminant-analysislda&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;1.4.2&lt;/span&gt; Linear discriminant analysis(LDA)&lt;/h3&gt;
&lt;p&gt;但是如果假定不同类的协方差矩阵相同, 那么这时候分类边界变成了&lt;span class=&quot;math inline&quot;&gt;\(\mathbb{x}\)&lt;/span&gt;的线性函数, 这个被称为LDA.&lt;/p&gt;
&lt;h3 data-number=&quot;1.4.3&quot; id=&quot;mle-for-discriminant-analysis&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;1.4.3&lt;/span&gt; MLE for discriminant analysis&lt;/h3&gt;
&lt;p&gt;可以利用最大似然估计方法来估计高斯判别分析准则中的参数. &lt;span class=&quot;math display&quot;&gt;\[\hat{\pi}_c = \frac{N_c}{N}\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[\hat{\mu}_c = \frac{1}{N_c}\sum_{i:y_i=c}\mathbb{x}_i\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[\hat{\Sigma_c} = \frac{1}{N_c}\sum_{i:y_i=c}(\mathbb{x}_i-\hat{\mu}_c)(\mathbb{x}_i-\hat{\mu}_c)^T\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 data-number=&quot;1.4.4&quot; id=&quot;防止过拟合的方法&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;1.4.4&lt;/span&gt; 防止过拟合的方法&lt;/h3&gt;
&lt;p&gt;MLE虽然简单快速, 但是有过拟合的缺点. 比如当&lt;span class=&quot;math inline&quot;&gt;\(N_c&amp;lt;D\)&lt;/span&gt;时, MLE估计出的协方差矩阵是奇异矩阵, 如果真实协方差矩阵是满秩的, 那么就过拟合了. 对此有一些方法来防止过拟合:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;假设每个类的协方差矩阵是对角阵, 即假设属性之间条件独立, 这等价于朴素贝叶斯.&lt;/li&gt;
&lt;li&gt;假设所有类的协方差矩阵相同, 这等价于LDA.&lt;/li&gt;
&lt;li&gt;假设协方差矩阵是对角阵, 且所有类协方差矩阵相同.&lt;/li&gt;
&lt;li&gt;使用一般的协方差矩阵, 但是引入先验.&lt;/li&gt;
&lt;li&gt;使用MAP估计&lt;/li&gt;
&lt;li&gt;将数据映射到低维空间, 然后在低维空间上使用判别分析.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 data-number=&quot;1.5&quot; id=&quot;inference-in-jointly-gaussian-distributions&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;1.5&lt;/span&gt; Inference in jointly Gaussian distributions&lt;/h2&gt;
&lt;p&gt;本小节讨论给定联合分布&lt;span class=&quot;math inline&quot;&gt;\(p(\mathbb{x}_1,\mathbb{x}_2)\)&lt;/span&gt;, 边际分布&lt;span class=&quot;math inline&quot;&gt;\(p(\mathbb{x}_1)\)&lt;/span&gt;和条件分布 &lt;span class=&quot;math inline&quot;&gt;\(p(\mathbb{x}_1\mid\mathbb{x}_2)\)&lt;/span&gt; 是怎样的呢? 如果联合分布是多元高斯分布, 那么边际分布和条件分布仍然是高斯分布. 有如下结论:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;假设 &lt;span class=&quot;math inline&quot;&gt;\(\mathbb{x}=(\mathbb{x}_1,\mathbb{x}_2)\)&lt;/span&gt; 是多元高斯分布, 参数为 &lt;span class=&quot;math display&quot;&gt;\[\mu = \left(\begin{matrix} \mu_1 \\ \mu_2 \end{matrix}\right)\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[\Sigma = \left(\begin{matrix} \Sigma_{11} &amp;amp; \Sigma_{12} \\ \Sigma_{21} &amp;amp; \Sigma_{22} \end{matrix}\right)\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[\Lambda = \Sigma^{-1} = \left(\begin{matrix} \Lambda_{11} &amp;amp; \Lambda_{12} \\ \Lambda_{21} &amp;amp; \Lambda_{22} \end{matrix}\right)\]&lt;/span&gt; 那么边际分布为 &lt;span class=&quot;math display&quot;&gt;\[p(\mathbb{x}_1) = \mathcal{N}(\mathbb{x}_1|\mu_1, \Sigma_{11})\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[p(\mathbb{x}_2) = \mathcal{N}(\mathbb{x}_2|\mu_2, \Sigma_{22})\]&lt;/span&gt; 条件分布为 &lt;span class=&quot;math display&quot;&gt;\[p(\mathbb{x}_1|\mathbb{x}_2) = \mathcal{N}(\mathbb{x}_1 | \mu_{1|2}, \Sigma_{1|2})\]&lt;/span&gt; 其中 &lt;span class=&quot;math display&quot;&gt;\[\begin{align}
\mu_{1|2} &amp;amp; = \mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(\mathbb{x}_2-\mu_2) \\
&amp;amp; = \mu_1 - \Lambda_{11}^{-1}\Lambda_{12}(\mathbb{x}_2-\mu_2) \\
&amp;amp; = \Sigma_{1|2}(\Lambda_{11}\mu_1 - \Lambda_{12}(\mathbb{x}_2-\mu_2))
\end{align}\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[\Sigma_{1|2} = \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21} = \Lambda_{11}^{-1}\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;上述结论非常重要. 同时注意到上面用到了不同的参数表示形式. 某些情况下用 moment parameters比较方便, 某些情况下用 canonical parameters 比较方便.&lt;/p&gt;
&lt;h2 data-number=&quot;1.6&quot; id=&quot;linear-gaussian-systems&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;1.6&lt;/span&gt; Linear Gaussian systems&lt;/h2&gt;
&lt;p&gt;假定有两个变量&lt;span class=&quot;math inline&quot;&gt;\(\mathbb{x}\)&lt;/span&gt;和&lt;span class=&quot;math inline&quot;&gt;\(\mathbb{y}\)&lt;/span&gt;, &lt;span class=&quot;math inline&quot;&gt;\(\mathbb{x}\in \mathbb{R}^{D_x}\)&lt;/span&gt;是隐变量, &lt;span class=&quot;math inline&quot;&gt;\(\mathbb{y}\in \mathbb{R}^{D_y}\)&lt;/span&gt;是 noisy observation of &lt;span class=&quot;math inline&quot;&gt;\(\mathbb{x}\)&lt;/span&gt;. 假定我们有如下先验和似然 &lt;span class=&quot;math display&quot;&gt;\[p(\mathbb{x}) = \mathcal{N}(\mathbb{x}|\mu_x, \Sigma_x)\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[p(\mathbb{y}|\mathbb{x}) = \mathcal{N}(\mathbb{y}|A\mathbb{x} + b, \Sigma_y)\]&lt;/span&gt; 这被成为一个线性高斯系统, 现在想要求&lt;span class=&quot;math inline&quot;&gt;\(\mathbb{x}\)&lt;/span&gt;的后验分布&lt;span class=&quot;math inline&quot;&gt;\(p(\mathbb{x}|\mathbb  {y})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;有如下结论 &lt;span class=&quot;math display&quot;&gt;\[p(\mathbb{x}|\mathbb{y}) = \mathcal{N}(\mathbb{x}|\mu_{x|y}, \Sigma_{x|y})\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[\Sigma_{x|y}^{-1} = \Sigma_{x}^{-1} + A^T\Sigma_{y}^{-1}A\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[\mu_{x|y} = \Sigma_{x|y}[A^T\Sigma_{y}^{-1}(\mathbb{y} - b) + \Sigma_{x}^{-1}\mu_{x}]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;此外 &lt;span class=&quot;math display&quot;&gt;\[p(\mathbb{y}) = \mathcal{N}(\mathbb{y}|A\mu_x + b, \Sigma_y+A\Sigma_xA^T)\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 data-number=&quot;1.7&quot; id=&quot;inferring-the-parameters-for-an-mvn&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;1.7&lt;/span&gt; Inferring the parameters for an MVN&lt;/h2&gt;
&lt;p&gt;之前小节提到用最大似然方法估计MVN的参数, 但是具有过拟合的缺点. 这节介绍的是利用贝叶斯方法来推断MVN的参数. 即引入共轭先验分布, 然后计算后验分布, 将后验分布的mode或mean作为参数估计值. 本节考虑了三个情况:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;知道协方差矩阵, 推断均值&lt;/li&gt;
&lt;li&gt;知道均值, 推断协方差矩阵&lt;/li&gt;
&lt;li&gt;均值与协方差矩阵都未知, 同时推断&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 data-number=&quot;1.7.1&quot; id=&quot;知道协方差矩阵-推断均值&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;1.7.1&lt;/span&gt; 知道协方差矩阵, 推断均值&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;这种情况下共轭先验为高斯分布&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 data-number=&quot;1.7.2&quot; id=&quot;知道均值-推断协方差矩阵&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;1.7.2&lt;/span&gt; 知道均值, 推断协方差矩阵&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;这种情况下共轭先验为 inverse Wishart distribution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 data-number=&quot;1.7.3&quot; id=&quot;均值与协方差矩阵都未知-同时推断&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;1.7.3&lt;/span&gt; 均值与协方差矩阵都未知, 同时推断&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;这种情况下共轭先验为 Normal-inverse-wishart distribution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;具体结果按下不表. 值得一提的是Bayes这种计算后验分布的方法与频率派最大似然估计方法有着形式上的紧密联系. 如果先验分布是uninformative的话, 那么贝叶斯方法计算的结果与频率学派计算的结果相同. 但是只是形式上的相同, 对于结果的解释不同.&lt;/p&gt;</content><author><name>  黄锡昆</name></author><category term="笔记" /><category term="mlapp" /><category term="机器学习" /></entry><entry><title type="html">MLAPP笔记-离散数据生成模型</title><link href="https://xikunhuang.github.io/%E7%AC%94%E8%AE%B0/MLAPP%E7%AC%94%E8%AE%B0-%E7%A6%BB%E6%95%A3%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/" rel="alternate" type="text/html" title="MLAPP笔记-离散数据生成模型" /><published>2018-11-28T00:00:00+00:00</published><updated>2018-11-28T00:00:00+00:00</updated><id>https://xikunhuang.github.io/%E7%AC%94%E8%AE%B0/MLAPP%E7%AC%94%E8%AE%B0-%E7%A6%BB%E6%95%A3%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B</id><content type="html" xml:base="https://xikunhuang.github.io/%E7%AC%94%E8%AE%B0/MLAPP%E7%AC%94%E8%AE%B0-%E7%A6%BB%E6%95%A3%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/">&lt;h1 data-number=&quot;1&quot; id=&quot;简介&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;1&lt;/span&gt; 简介&lt;/h1&gt;
&lt;p&gt;先来说一下&lt;strong&gt;生成式模型(generative models)&lt;/strong&gt; 和 &lt;strong&gt;判别式模型(discriminative models)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;对于分类问题, 我们的目标是基于有限的训练样本集尽可能准确地估计出后验概率&lt;span class=&quot;math inline&quot;&gt;\(p(c\vert x)\)&lt;/span&gt;, 而估计该后验概率有两种方式:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;判别式模型: 直接建模&lt;span class=&quot;math inline&quot;&gt;\(p(c \vert x)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;生成式模型: 先对联合分布&lt;span class=&quot;math inline&quot;&gt;\(p(x,c)\)&lt;/span&gt;建模, 再由此得到&lt;span class=&quot;math inline&quot;&gt;\(p(c \vert x)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本章着眼于生成式模型, 由贝叶斯定理可得 &lt;span class=&quot;math display&quot;&gt;\[p(c\vert x)=\frac{p(x\vert c)p(c)}{p(x)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以问题关键就在于求得&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;类条件概率分布(class-conditional density) &lt;span class=&quot;math inline&quot;&gt;\(p(x\vert c)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;先验分布 &lt;span class=&quot;math inline&quot;&gt;\(p(c)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 data-number=&quot;2&quot; id=&quot;概率分布&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;2&lt;/span&gt; 概率分布&lt;/h1&gt;
&lt;p&gt;继续进行之前, 这里先罗列一下本章涉及的一些概率分布.&lt;/p&gt;
&lt;h2 data-number=&quot;2.1&quot; id=&quot;伯努利分布和二项分布&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;2.1&lt;/span&gt; 伯努利分布和二项分布&lt;/h2&gt;
&lt;p&gt;可用来建模掷硬币的结果&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;伯努利分布&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[X \sim Ber(\theta)\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[Ber(x\vert \theta) = \theta^{\mathbb{I}(x=1)}(1-\theta)^{\mathbb{I}(x=0)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;即随机变量&lt;span class=&quot;math inline&quot;&gt;\(X \in \{0,1\}\)&lt;/span&gt;, 且 &lt;span class=&quot;math display&quot;&gt;\[p(X=1) = \theta\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[p(X=0) = 1 - \theta\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;二项分布&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[X \sim Bin(n,\theta)\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[Bin(k\vert n,\theta) = \binom{n}{k} \theta^k (1-\theta)^{n-k} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;即随机变量&lt;span class=&quot;math inline&quot;&gt;\(X \in \{0,\dots, n\}\)&lt;/span&gt;, 且 &lt;span class=&quot;math display&quot;&gt;\[p(X=k) =  \binom{n}{k} \theta^k (1-\theta)^{n-k}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 data-number=&quot;2.2&quot; id=&quot;多伯努利分布和多项分布&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;2.2&lt;/span&gt; 多伯努利分布和多项分布&lt;/h2&gt;
&lt;p&gt;可用来建模掷具有&lt;span class=&quot;math inline&quot;&gt;\(K\)&lt;/span&gt;面的骰子的结果&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;多伯努利分布&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[X \sim Cat(\theta)\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[Cat(x\vert \theta) = \prod_{j=1}^K \theta_j^{\mathbb{I}(x_j=1)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(X\)&lt;/span&gt;取值为&lt;span class=&quot;math inline&quot;&gt;\(K\)&lt;/span&gt;维one-hot编码.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;多项分布&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[X \sim Mu(n, \theta)\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[Mu(x\vert n,\theta) = \binom{n}{x_1 \dots x_K} \prod_{j=1}^K \theta_j^k\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(X\)&lt;/span&gt;是K维向量, 且满足 &lt;span class=&quot;math display&quot;&gt;\[x_k\in\{0,\dots,n\}\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[\sum_{k=1}^Kx_k = n\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 data-number=&quot;2.3&quot; id=&quot;贝塔分布和狄利克雷分布&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;2.3&lt;/span&gt; 贝塔分布和狄利克雷分布&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;贝塔分布&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[X \sim Beta(a,b)\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[Beta(x\vert a,b) = \frac{1}{B(a,b)}x^{a-1}(1-x)^{b-1}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(X\)&lt;/span&gt;取值范围是&lt;span class=&quot;math inline&quot;&gt;\([0,1]\)&lt;/span&gt;. 要求&lt;span class=&quot;math inline&quot;&gt;\(a,b&amp;gt;0\)&lt;/span&gt;. 当&lt;span class=&quot;math inline&quot;&gt;\(a=b=1\)&lt;/span&gt;时退化为&lt;span class=&quot;math inline&quot;&gt;\([0,1]\)&lt;/span&gt;上的均匀分布.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;狄利克雷分布&lt;/p&gt;
&lt;p&gt;贝塔分布的多元扩展, 相当于考虑的是多个随机变量的联合分布.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[X \sim Dir(\alpha_1,\dots, \alpha_k)\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[Dir(x\vert \alpha_1,\dots,\alpha_k) = \frac{1}{B(\alpha_1,\dots,\alpha_k)}\prod_{k=1}^K x_k^{\alpha_k - 1} \mathbb{I}(x\in S_K)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&quot;math display&quot;&gt;\[ S_K = \{x:0\leq x_k \leq 1, \sum_{k=1}^K x_k = 1\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;狄利克雷分布在&lt;span class=&quot;math inline&quot;&gt;\(S_K\)&lt;/span&gt;以外的地方概率为&lt;span class=&quot;math inline&quot;&gt;\(0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 data-number=&quot;3&quot; id=&quot;贝叶斯概念学习&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;3&lt;/span&gt; 贝叶斯概念学习&lt;/h1&gt;
&lt;p&gt;类比于小孩子学习理解单词的含义, 概念学习可以等价于二分类问题, 即学习一个函数&lt;span class=&quot;math inline&quot;&gt;\(f(x)\)&lt;/span&gt;, 如果&lt;span class=&quot;math inline&quot;&gt;\(x\)&lt;/span&gt;是概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;的一个实例, 则&lt;span class=&quot;math inline&quot;&gt;\(f(x)=1\)&lt;/span&gt;, 否则&lt;span class=&quot;math inline&quot;&gt;\(f(x)=0\)&lt;/span&gt;. 一般处理二分类问题时会从正例和反例中同时学习, 但本小节介绍的例子只从正例中学习.&lt;/p&gt;
&lt;h2 data-number=&quot;3.1&quot; id=&quot;数字游戏&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;3.1&lt;/span&gt; 数字游戏&lt;/h2&gt;
&lt;p&gt;首先我会选择一个范围在&lt;span class=&quot;math inline&quot;&gt;\([1,100]\)&lt;/span&gt;的整数代数概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;(不会告诉你), 比如质数, &lt;span class=&quot;math inline&quot;&gt;\(1\)&lt;/span&gt;到&lt;span class=&quot;math inline&quot;&gt;\(10\)&lt;/span&gt;之间的数等等. 然后我会告诉你数字&lt;span class=&quot;math inline&quot;&gt;\(\mathcal{D}=\{x_1,\dots,x_N\}\)&lt;/span&gt;是从概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;中随机挑选出来的. 比如假设我之前选的概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;是奇数, 那么我可能告诉你&lt;span class=&quot;math inline&quot;&gt;\(\{3,17,5,97\}\)&lt;/span&gt;这些数字. 然后接下来问你一些新的数字&lt;span class=&quot;math inline&quot;&gt;\(\tilde{x}\)&lt;/span&gt;是否属于概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;. 其实就是让你在给定一些正例的情况下猜测我之前设定的概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;是什么.&lt;/p&gt;
&lt;p&gt;比如告诉你 &lt;span class=&quot;math inline&quot;&gt;\(\{2,4,8,16,32\}\)&lt;/span&gt; 这些数字属于概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;, 那么问你&lt;span class=&quot;math inline&quot;&gt;\(62\)&lt;/span&gt;属于概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;吗? 我们基于这些数字可能会猜测概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;大概率是&lt;span class=&quot;math inline&quot;&gt;\(2^n\)&lt;/span&gt;, 所以判断&lt;span class=&quot;math inline&quot;&gt;\(62\)&lt;/span&gt;不属于概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;. 那么如果再告诉你&lt;span class=&quot;math inline&quot;&gt;\(30\)&lt;/span&gt;也属于概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;, 那么现在你觉得&lt;span class=&quot;math inline&quot;&gt;\(62\)&lt;/span&gt;属于概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;吗? 这时候我们就可能认为概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;是偶数吧, 然后判断&lt;span class=&quot;math inline&quot;&gt;\(62\)&lt;/span&gt;属于概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;怎么来解释和模拟上述行为呢? 一种方法是用假设空间(hypothesis space)和版本空间(version space). 最开始我们对概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;有一个假设空间&lt;span class=&quot;math inline&quot;&gt;\(\mathcal{H}\)&lt;/span&gt;, 比如质数, 奇数, &lt;span class=&quot;math inline&quot;&gt;\(10\)&lt;/span&gt;的倍数等等. 然后当我们看到一些正例时, 那些符合正例的概念构成的&lt;span class=&quot;math inline&quot;&gt;\(\mathcal{H}\)&lt;/span&gt;的子集就是版本空间. 随着我们看到的正例越多, 版本空间会越来越小(至少不会增大), 表示着我们离正确概念越来越近(如果假设空间包含正确概念的话). 以上面的例子为例, 当我们看到&lt;span class=&quot;math inline&quot;&gt;\(\{2,4,8,16,32,128\}\)&lt;/span&gt;时, 偶数, &lt;span class=&quot;math inline&quot;&gt;\(2^n\)&lt;/span&gt;等等这些概念都在版本空间中, 质数等概念不在版本空间中, 当我们又看到&lt;span class=&quot;math inline&quot;&gt;\(30\)&lt;/span&gt;时, 概念&lt;span class=&quot;math inline&quot;&gt;\(2^n\)&lt;/span&gt;也从版本空间中删除.&lt;/p&gt;
&lt;p&gt;另外一个问题是我们看到&lt;span class=&quot;math inline&quot;&gt;\(\{2,4,8,16,32,128\}\)&lt;/span&gt;时, 其实&lt;span class=&quot;math inline&quot;&gt;\(2^n\)&lt;/span&gt;和偶数都在版本空间中,那么我们为什么会倾向于&lt;span class=&quot;math inline&quot;&gt;\(2^n\)&lt;/span&gt;呢?&lt;/p&gt;
&lt;h2 data-number=&quot;3.2&quot; id=&quot;似然&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;3.2&lt;/span&gt; 似然&lt;/h2&gt;
&lt;p&gt;对于上小节最后一个问题, 直观想法是我们想避免 suspicious coincidence. 即如果概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;是偶数, 那么为什么那么巧我们看到的数都符合&lt;span class=&quot;math inline&quot;&gt;\(2^n\)&lt;/span&gt;. 下面从概率角度解释一下. 我们假定看到的数字是从概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;中均匀随机选取的. 我们引入似然 &lt;span class=&quot;math inline&quot;&gt;\(p(\mathcal{D}\vert h)\)&lt;/span&gt;, 则&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[p(\{2\}\vert h_{2^n}) = \frac{1}{6} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;因为在&lt;span class=&quot;math inline&quot;&gt;\([1,100]\)&lt;/span&gt;整数中只有&lt;span class=&quot;math inline&quot;&gt;\(6\)&lt;/span&gt;个整数满足&lt;span class=&quot;math inline&quot;&gt;\(2^n\)&lt;/span&gt;. 而&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[p(\{2\}\vert h_{even}) = \frac{1}{50}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;同理可以计算&lt;span class=&quot;math display&quot;&gt;\[p(\{2,4,8,16,32\}\vert h_{2^n}) \gg p(\{2,4,8,16,32\}\vert h_{even})\]&lt;/span&gt; 这就解释了我们为什么会倾向于&lt;span class=&quot;math inline&quot;&gt;\(2^n\)&lt;/span&gt;(这里默认了不同概念先验概率相等).&lt;/p&gt;
&lt;h2 data-number=&quot;3.3&quot; id=&quot;先验&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;3.3&lt;/span&gt; 先验&lt;/h2&gt;
&lt;p&gt;还是上面的例子, 看到&lt;span class=&quot;math inline&quot;&gt;\(\{2,4,8,16,32\}\)&lt;/span&gt;这些数, 概念“满足&lt;span class=&quot;math inline&quot;&gt;\(2^n\)&lt;/span&gt;且不是&lt;span class=&quot;math inline&quot;&gt;\(64\)&lt;/span&gt;”比&lt;span class=&quot;math inline&quot;&gt;\(2^n\)&lt;/span&gt;更符合数据, 但是前一个怪怪的不自然, 我们可以给这种概念赋予较小的先验概率, 即 &lt;span class=&quot;math display&quot;&gt;\[p(C=h_{2^n且不包含64}) &amp;lt; p(C=h_{2^n})\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 data-number=&quot;3.4&quot; id=&quot;后验&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;3.4&lt;/span&gt; 后验&lt;/h2&gt;
&lt;p&gt;后验概率正比于似然乘上先验, 即 &lt;span class=&quot;math display&quot;&gt;\[p(h\vert \mathcal{D}) \propto p(\mathcal{D}\vert h)p(h) \]&lt;/span&gt; 当数据量比较多时, 后验概率在某个概念上值比较大, 这个概念就是所谓的&lt;strong&gt;最大后验概率估计(MAP)&lt;/strong&gt;. 即 &lt;span class=&quot;math display&quot;&gt;\[\hat{h}^{MAP} = \arg\max_h \ \ p(h\vert \mathcal{D})\]&lt;/span&gt; 由贝叶斯公式,上式可以写成 &lt;span class=&quot;math display&quot;&gt;\[\hat{h}^{MAP} = \arg\max_h \ \ p(\mathcal{D}\vert h)p(h) = \arg\max_h \ \ [\log p(\mathcal{D}\vert h) + \log p(h)] \]&lt;/span&gt; 似然项会变(指数依赖样本大小&lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt;), 而先验项保持不变. 当数据量变大时, MAP趋向于&lt;strong&gt;最大似然估计(MLE)&lt;/strong&gt;. &lt;span class=&quot;math display&quot;&gt;\[\hat{h}^{MLE} = \arg\max_h \ \ p(\mathcal{D}\vert h) = \arg\max_h \ \ \log p(\mathcal{D}\vert h)\]&lt;/span&gt; 换句话说, 当我们有足够数据量时, 数据会忽略掉先验. MAP会收敛到MLE.&lt;/p&gt;
&lt;h2 data-number=&quot;3.5&quot; id=&quot;后验预测分布&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;3.5&lt;/span&gt; 后验预测分布&lt;/h2&gt;
&lt;p&gt;我们得到的后验概率分布是在观测到现有数据的条件下,假设空间中那些概念是实际概念的概率. 但是现在给我们一个新的数字&lt;span class=&quot;math inline&quot;&gt;\(\tilde{x}\)&lt;/span&gt;, 我们怎么决策它是否属于实际概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;呢? 这就是后验预测分布干的活. 这种情形下的后验预测分布为 &lt;span class=&quot;math display&quot;&gt;\[p(\tilde{x} \in C \vert  \mathcal{D}) = \sum_h p(y=1\vert \tilde{x},h) p(h\vert \mathcal{D})\]&lt;/span&gt; 本质上就是对假设空间中的概念的预测结果做一个加权平均, 权重就是概念对应的后验概率. 这个成为&lt;strong&gt;贝叶斯模型平均(Bayes model averaging)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;当数据量很小时,可能后验概率分布很广泛. 但是当数据量很多, 多到几乎“暴露”出真实概念时, 这个时候后验概率会集中在最大后验概率的那个概念上. 这时候后验预测分布就可以近似为 &lt;span class=&quot;math display&quot;&gt;\[p(\tilde{x} \in C \vert  \mathcal{D}) = p(\tilde{x}\vert \hat{h}^{MAP})\]&lt;/span&gt; 这个被称为预测分布的&lt;strong&gt;plug-in approximation&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 data-number=&quot;3.5.1&quot; id=&quot;bma-vs-plug-in-approximation&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;3.5.1&lt;/span&gt; BMA vs Plug-in Approximation&lt;/h3&gt;
&lt;p&gt;随着数据的增多&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BMA由宽到窄.&lt;/li&gt;
&lt;li&gt;Plug-in Approximation由窄到宽.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;啥意思呢? 就是比如我们上来只看到一个数据&lt;span class=&quot;math inline&quot;&gt;\(\{16\}\)&lt;/span&gt;, 这时候给你一个&lt;span class=&quot;math inline&quot;&gt;\(\tilde{x}=8\)&lt;/span&gt;问你属不属于概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;. BMA这时候一脸懵逼不敢妄下断言, 很多概念的后验概率都非零, 而且没有某个概念后验概率很大, 它的标准放得很宽,更有可能回答是. 但是Plug-in Approximation就不一样了, 它上来就从假设空间中挑一个最大化后验概率的那个概念(假设挑出的是&lt;span class=&quot;math inline&quot;&gt;\(4^n\)&lt;/span&gt;), 那么它的回答就是否. 但是如果接下来告诉我们&lt;span class=&quot;math inline&quot;&gt;\(\{2,4,64\}\)&lt;/span&gt;也是正例, 这时候&lt;span class=&quot;math inline&quot;&gt;\(2^n\)&lt;/span&gt;对应的后验概率可能大于其他概念, 那BMA会进行相应的调整, 收窄自己的标准, 而Plug-in Approximation也会调整(假设就调整成了&lt;span class=&quot;math inline&quot;&gt;\(2^n\)&lt;/span&gt;), 相比于&lt;span class=&quot;math inline&quot;&gt;\(4^n\)&lt;/span&gt;, 它是放宽了自己的标准. 虽然在数据量比较小时两种方法会有偏差, 但当数据量很大时, 两种方法收敛到相同的标准.&lt;/p&gt;
&lt;h1 data-number=&quot;4&quot; id=&quot;the-beta-binomial-model&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;4&lt;/span&gt; The beta-binomial model&lt;/h1&gt;
&lt;p&gt;本小节讨论给定一系列观察到的掷硬币的结果, 预测下一次掷硬币出现正面的概率. 叙述方式与上一节相同 似然 -&amp;gt; 先验 -&amp;gt; 后验 -&amp;gt; 后验预测&lt;/p&gt;
&lt;h2 data-number=&quot;4.1&quot; id=&quot;似然-1&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;4.1&lt;/span&gt; 似然&lt;/h2&gt;
&lt;p&gt;假设 &lt;span class=&quot;math inline&quot;&gt;\(X_i \sim Ber(\theta)\)&lt;/span&gt;, 其中&lt;span class=&quot;math inline&quot;&gt;\(X_i=1\)&lt;/span&gt;表示正面, &lt;span class=&quot;math inline&quot;&gt;\(X_i=0\)&lt;/span&gt;表示反面, &lt;span class=&quot;math inline&quot;&gt;\(\theta \in [0,1]\)&lt;/span&gt;是参数(表示正面的概率). 如果数据 iid, 那么似然为 &lt;span class=&quot;math display&quot;&gt;\[p(\mathcal{D}\vert \theta) = \theta^{N_1}(1-\theta)^{N_0}\]&lt;/span&gt; 实际上是一个二项分布. &amp;gt; 注: 这里的每一个&lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;对应的伯努利分布就相当于上节中的假设空间中的一个概念&lt;span class=&quot;math inline&quot;&gt;\(h\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 data-number=&quot;4.2&quot; id=&quot;先验-1&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;4.2&lt;/span&gt; 先验&lt;/h2&gt;
&lt;p&gt;我们要给&lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;一个先验分布, &lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;取值范围为&lt;span class=&quot;math inline&quot;&gt;\([0,1]\)&lt;/span&gt;, 表示我们在观察数据之前对&lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;的认识. 如果先验分布和上节的似然具有相同的形式, 那么数学上计算会很方便. 综上两点, Beta分布当仁不让. &lt;span class=&quot;math display&quot;&gt;\[Beta(\theta\vert a,b) \propto \theta^{a-1}(1-\theta)^{b-1}\]&lt;/span&gt; 此时后验分布具有和先验分布相同的形式. 同时先验中的&lt;span class=&quot;math inline&quot;&gt;\(a,b\)&lt;/span&gt;是超参数, 需要我们自己设定, &lt;span class=&quot;math inline&quot;&gt;\(a,b\)&lt;/span&gt;的取值融合了我们对&lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;的认识, 比如我们认为&lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;具有均值&lt;span class=&quot;math inline&quot;&gt;\(0.7\)&lt;/span&gt;, 方差&lt;span class=&quot;math inline&quot;&gt;\(0.2\)&lt;/span&gt;, 那么我们就可以设置为&lt;span class=&quot;math inline&quot;&gt;\(a=2.975,b=1.275\)&lt;/span&gt;. 或者我们对&lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;一无所知, 那我们可以设置为均匀分布&lt;span class=&quot;math inline&quot;&gt;\(a=b=1\)&lt;/span&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;共轭先验&lt;/strong&gt;: 如果先验分布和似然函数可以使得先验分布和后验分布有相同的形式，那么就称先验分布与似然函数是共轭的，共轭的结局是让先验与后验具有相同的形式. 再强调一遍, 共轭先验指的是对于似然来说这个先验是共轭先验.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 data-number=&quot;4.3&quot; id=&quot;后验-1&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;4.3&lt;/span&gt; 后验&lt;/h2&gt;
&lt;p&gt;后验概率分布为 &lt;span class=&quot;math display&quot;&gt;\[p(\theta\vert \mathcal{D}) \propto Bin(N_1\vert \theta, N_0+N_1)\ Beta(\theta\vert a,b) \propto Beta(\theta\vert N_1+a, N_0+b)\]&lt;/span&gt; 由Beta分布的性质(mode的表达式)可得 &lt;span class=&quot;math display&quot;&gt;\[\hat{\theta}_{MAP} = \frac{a+N_1-1}{a+b+N_0+N_1-2}\]&lt;/span&gt; 如果先验为均匀分布(&lt;span class=&quot;math inline&quot;&gt;\(a=b=1\)&lt;/span&gt;), 则上式退化为 &lt;span class=&quot;math display&quot;&gt;\[\hat{\theta}_{MLE} = \frac{N_1}{N_0+N_1}\]&lt;/span&gt; 后验概率分布的均值为 &lt;span class=&quot;math display&quot;&gt;\[\bar{\theta} = \frac{a+N_1}{a+b+N_0+N_1}\]&lt;/span&gt; 进一步可以证明, &lt;strong&gt;后验概率分布的均值是先验均分布的均值和最大似然估计的凸组合&lt;/strong&gt;. 即 &lt;span class=&quot;math display&quot;&gt;\[\mathbb{E}[\theta\vert \mathcal{D}] = \lambda \frac{a}{a+b} + (1-\lambda)\hat{\theta}_{MLE}\]&lt;/span&gt; 也就是说, 后验是我们观察数据之前相信的知识和数据想要告诉我们的知识的一个折中.&lt;/p&gt;
&lt;h2 data-number=&quot;4.4&quot; id=&quot;后验预测分布-1&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;4.4&lt;/span&gt; 后验预测分布&lt;/h2&gt;
&lt;p&gt;当我们有了后验概率分布&lt;span class=&quot;math inline&quot;&gt;\(Beta(c,d)\)&lt;/span&gt;后, 我们要预测下一次掷硬币是正面的概率. &lt;span class=&quot;math display&quot;&gt;\[p(\tilde{x}=1\vert \mathcal{D}) = \int_0^1 p(x=1\vert \theta)p(\theta\vert \mathcal{D})d\theta\]&lt;/span&gt; 即把&lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;当成随机变量看待, 对&lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;所有可能情况进行汇总. 带入 &lt;span class=&quot;math inline&quot;&gt;\(p(x=1\vert \theta)=\theta\)&lt;/span&gt;, &lt;span class=&quot;math inline&quot;&gt;\(p(\theta\vert \mathcal{D})=Beta(\theta\vert c,d)\)&lt;/span&gt;可得 &lt;span class=&quot;math display&quot;&gt;\[p(\tilde{x}=1\vert \mathcal{D}) = \int_0^1\theta Beta(\theta\vert c,d) d\theta = \mathbb{E}[\theta\vert \mathcal{D}] = \frac{c}{c+d}\]&lt;/span&gt; 即在这个例子中我们有 &lt;span class=&quot;math display&quot;&gt;\[p(\tilde{x}\vert \mathcal{D})=Ber(\tilde{x}\vert \mathbb{E}[\theta\vert \mathcal{D}])\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 data-number=&quot;4.4.1&quot; id=&quot;过拟合问题&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;4.4.1&lt;/span&gt; 过拟合问题&lt;/h3&gt;
&lt;p&gt;如果我们在预测时不使用上面的BMA, 而是使用 &lt;span class=&quot;math display&quot;&gt;\[p(\tilde{x}\vert \mathcal{D})=Ber(\tilde{x}\vert \hat{\theta}_{MLE})\]&lt;/span&gt; 即 &lt;span class=&quot;math display&quot;&gt;\[p(\tilde{x}=1\vert \mathcal{D}) = \hat{\theta}_{MLE} = \frac{N_1}{N_0+N_1} \]&lt;/span&gt; 在数据集比较小时, 这非常容易过拟合, 比如样本集合为&lt;span class=&quot;math inline&quot;&gt;\(3\)&lt;/span&gt;次观测结果,且全是反面, 那么根据最大似然估计 &lt;span class=&quot;math display&quot;&gt;\[p(\tilde{x}=1\vert \mathcal{D}) = \hat{\theta}_{MLE} = \frac{N_1}{N_0+N_1} = 0 \]&lt;/span&gt; 而贝叶斯方法可以很好的克服这个问题, 比如我们假定先验分布为均匀分布&lt;span class=&quot;math inline&quot;&gt;\(a=b=1\)&lt;/span&gt;, 那么根据后验预测分布我们有 &lt;span class=&quot;math display&quot;&gt;\[p(\tilde{x}=1\vert \mathcal{D}) = \mathbb{E}[\theta\vert \mathcal{D}] = \frac{c}{c+d} = \frac{N_1+1}{N_0+N_1+2} = \frac{1}{5}\]&lt;/span&gt; 这正对应着&lt;strong&gt;加一平滑&lt;/strong&gt;这种技巧.&lt;/p&gt;
&lt;h1 data-number=&quot;5&quot; id=&quot;the-dirichlet-multinomial-model&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;5&lt;/span&gt; The Dirichlet-multinomial model&lt;/h1&gt;
&lt;p&gt;本小节讨论给定一系列观察到的掷具有&lt;span class=&quot;math inline&quot;&gt;\(K\)&lt;/span&gt;面骰子的结果, 预测下一次掷骰子出现每一面的概率. 叙述方式与上一节相同 似然 -&amp;gt; 先验 -&amp;gt; 后验 -&amp;gt; 后验预测&lt;/p&gt;
&lt;h2 data-number=&quot;5.1&quot; id=&quot;似然-2&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;5.1&lt;/span&gt; 似然&lt;/h2&gt;
&lt;p&gt;假设数据 iid, 那么似然为 &lt;span class=&quot;math display&quot;&gt;\[p(\mathcal{D}\vert \theta) = \prod_{i=1}^K \theta_k^{N_k}\]&lt;/span&gt; 实际上是一个多项分布.&lt;/p&gt;
&lt;h2 data-number=&quot;5.2&quot; id=&quot;先验-2&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;5.2&lt;/span&gt; 先验&lt;/h2&gt;
&lt;p&gt;选取先验时我们主要考虑两点, 一是参数&lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;取值范围, 另一个是希望先验对于似然来说是共轭先验. Dirichlet分布满足这两个条件. &lt;span class=&quot;math display&quot;&gt;\[Dir(\theta\vert \alpha_1,\dots,\alpha_K) = \frac{1}{B(\alpha_1,\dots,\alpha_K)}\prod_{k=1}^K \theta_k^ {\alpha_k-1}\mathbb{I}(\theta \in S_K)\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 data-number=&quot;5.3&quot; id=&quot;后验-2&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;5.3&lt;/span&gt; 后验&lt;/h2&gt;
&lt;p&gt;后验概率分布依然是Dirichlet分布 &lt;span class=&quot;math display&quot;&gt;\[p(\theta\vert \mathcal{D}) \propto p(\mathcal{D}\vert \theta)p(\theta) = Dir(\theta \vert  \alpha_1+N_1,\dots, \alpha_K+N_K)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;后验概率分布的MAP估计是 &lt;span class=&quot;math display&quot;&gt;\[\hat{\theta}_k^{MAP} = \frac{N_k+\alpha_k-1}{N_1+\dots+N_k+\alpha_1+\dots+\alpha_K-K}\]&lt;/span&gt; 当先验分布退化为均匀分布时,MAP退化为MLE &lt;span class=&quot;math display&quot;&gt;\[\hat{\theta}_k^{MAP} = \frac{N_k}{N_1+\dots+N_k}\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 data-number=&quot;5.4&quot; id=&quot;后验预测分布-2&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;5.4&lt;/span&gt; 后验预测分布&lt;/h2&gt;
&lt;p&gt;预测下一次掷骰子出现每一面的概率 &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
p(X=j\vert \mathcal{D}) &amp;amp;=&amp;amp; \int p(X=j\vert \theta)p(\theta\vert \mathcal{D})d\theta \\
&amp;amp;=&amp;amp; \frac{\alpha_j+N_j}{\alpha_1+\dots+\alpha_K + N_1+\dots+N_K}
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h1 data-number=&quot;6&quot; id=&quot;朴素贝叶斯&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;6&lt;/span&gt; 朴素贝叶斯&lt;/h1&gt;
&lt;p&gt;朴素贝叶斯可以用来解决&lt;span class=&quot;math inline&quot;&gt;\(K\)&lt;/span&gt;分类问题. 问题定义: &lt;span class=&quot;math inline&quot;&gt;\(X \in \mathbb{R}^D\)&lt;/span&gt;, &lt;span class=&quot;math inline&quot;&gt;\(Y \in \{1, \dots, K\}\)&lt;/span&gt;. 给定数据集&lt;span class=&quot;math inline&quot;&gt;\(\mathcal{D}=\{(X_1,Y_1),\dots, (X_n,Y_n)\}\)&lt;/span&gt;, 给新的样本数据&lt;span class=&quot;math inline&quot;&gt;\(X\)&lt;/span&gt;分类,即求 &lt;span class=&quot;math display&quot;&gt;\[p(Y\vert X,\mathcal{D})\]&lt;/span&gt; 我们可以利用贝叶斯公式,分别求出&lt;span class=&quot;math inline&quot;&gt;\(p(X\vert Y,\mathcal{D})\)&lt;/span&gt;和&lt;span class=&quot;math inline&quot;&gt;\(p(Y)\)&lt;/span&gt;. 但是求解&lt;span class=&quot;math inline&quot;&gt;\(p(X\vert Y,\mathcal{D})\)&lt;/span&gt;复杂度太高, 比如假设&lt;span class=&quot;math inline&quot;&gt;\(X\)&lt;/span&gt;的每一维取值范围为离散值,且大小为&lt;span class=&quot;math inline&quot;&gt;\(S\)&lt;/span&gt;, 那么&lt;span class=&quot;math inline&quot;&gt;\(p(X\vert Y,\mathcal{D})\)&lt;/span&gt;的参数个数将达到&lt;span class=&quot;math inline&quot;&gt;\(KS^D\)&lt;/span&gt;. 为了解决这个问题,朴素贝叶斯做了一个很强的假设:在给定类别的条件下, 特征之间相互独立. 即 &lt;span class=&quot;math display&quot;&gt;\[p(X\vert Y=c) = \prod_{i=1}^D p(X^{(i)}\vert Y=c)\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 data-number=&quot;6.1&quot; id=&quot;model-fitting&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;6.1&lt;/span&gt; Model fitting&lt;/h2&gt;
&lt;h3 data-number=&quot;6.1.1&quot; id=&quot;最大似然估计&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;6.1.1&lt;/span&gt; 最大似然估计&lt;/h3&gt;
&lt;p&gt;对于一个样本&lt;span class=&quot;math inline&quot;&gt;\((X_i,Y_i)\)&lt;/span&gt;, 似然函数为&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
p(X_i,Y_i\vert \vec{\theta}) &amp;amp;=&amp;amp; p(Y_i\vert \vec{\theta})p(X_i\vert Y_i,\vec{\theta}) \\
&amp;amp;=&amp;amp; p(Y_i\vert \vec{\theta}) \prod_{j=1}^{D} p(X_i^{(j)}\vert Y_i,\vec{\theta})
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&quot;math inline&quot;&gt;\(\vec{\theta} = \{\pi_1,\dots,\pi_K, \vec{\theta}_{11},\dots,\vec{\theta}_{DK}\}\)&lt;/span&gt;, 这里&lt;span class=&quot;math inline&quot;&gt;\(\pi_i\)&lt;/span&gt;是针对&lt;span class=&quot;math inline&quot;&gt;\(p(Y=i)\)&lt;/span&gt;的参数, &lt;span class=&quot;math inline&quot;&gt;\(\vec{\theta}_{jc}\)&lt;/span&gt;是针对&lt;span class=&quot;math inline&quot;&gt;\(p(X^{(j)}\vert Y=c)\)&lt;/span&gt;的参数. &lt;span class=&quot;math inline&quot;&gt;\(\vec{\theta}_{jc}\)&lt;/span&gt;取决于第&lt;span class=&quot;math inline&quot;&gt;\(j\)&lt;/span&gt;维特征选取什么样的分布. 对于数据集&lt;span class=&quot;math inline&quot;&gt;\(\mathcal{D}=\{(X_1,Y_1),\dots, (X_n,Y_n)\}\)&lt;/span&gt;, 可计算似然函数为 &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
p(\mathcal{D}\vert \theta) &amp;amp;=&amp;amp; \prod_{i=1}^n p(Y_i\vert \theta) \prod_{i=1}^n\prod_{j=1}^D  p(X_i^{(j)}\vert Y_i,\theta) \\
&amp;amp;=&amp;amp; \prod_{i=1}^n p(Y_i\vert \theta) \prod_{i=1}^n\prod_{j=1}^D  \prod_{c=1}^K p(X_i^{(j)}\vert \theta_{jc})^{\mathbb{I}(Y_i=c)}
\end{align}
\]&lt;/span&gt; 求对数可得 &lt;span class=&quot;math display&quot;&gt;\[\log p(\mathcal{D}\vert \theta) = \sum_{c=1}^K N_c\log \pi_c + \sum_{j=1}^{D}\sum_{c=1}^K\sum_{i: Y_i=c}p(X_i^{(j)}\vert \theta_{jc})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;在求解&lt;span class=&quot;math inline&quot;&gt;\(\vec{\pi}\)&lt;/span&gt;时, 需要带入约束&lt;span class=&quot;math inline&quot;&gt;\(\sum\limits_{c=1}^K\pi_c=1\)&lt;/span&gt;, 利用拉格朗日乘子法可得 &lt;span class=&quot;math display&quot;&gt;\[\pi_c^{MLE} = \frac{N_c}{N_1+\dots+N_K}\]&lt;/span&gt; 而&lt;span class=&quot;math inline&quot;&gt;\(\vec{\theta}_{jc}\)&lt;/span&gt;需要知道特征分布的具体形式才可以求.&lt;/p&gt;
&lt;h3 data-number=&quot;6.1.2&quot; id=&quot;bayesian-naive-bayes&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;6.1.2&lt;/span&gt; Bayesian naive Bayes&lt;/h3&gt;
&lt;p&gt;最大似然估计有个缺点是过拟合, 比如上面对&lt;span class=&quot;math inline&quot;&gt;\(\pi_c\)&lt;/span&gt;的估计, 当&lt;span class=&quot;math inline&quot;&gt;\(N_c=0\)&lt;/span&gt;时, &lt;span class=&quot;math inline&quot;&gt;\(\pi_c^{MLE} = 0\)&lt;/span&gt;. 为了解决过拟合问题, Bayes方法采用合适的先验分布, 比如我们使用下面形式的先验分布 &lt;span class=&quot;math display&quot;&gt;\[p(\theta) = p(\pi)\prod_{j=1}^D\prod_{c=1}^Kp(\theta_{jc})\]&lt;/span&gt; 其中&lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;使用Dirichlet分布, 如果每个特征是伯努利分布, 那我们就取&lt;span class=&quot;math inline&quot;&gt;\(\theta_{jc}\)&lt;/span&gt;为Beta分布. 那么后验分布为 &lt;span class=&quot;math display&quot;&gt;\[p(\theta\vert \mathcal{D}) = p(\pi\vert \mathcal{D})\prod_{j=1}^D\prod_{c=1}^Kp(\theta_{jc}\vert \mathcal{D})\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 data-number=&quot;6.1.3&quot; id=&quot;using-the-model-for-prediction&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;6.1.3&lt;/span&gt; Using the model for prediction&lt;/h3&gt;
&lt;p&gt;最后的目标是用模型进行预测, 即计算 &lt;span class=&quot;math display&quot;&gt;\[p(y=c\vert x,\mathcal{D}) \propto p(y=c\vert \mathcal{D})\prod_{j=1}^Dp(x_j\vert y=c_j,\mathcal{D})\]&lt;/span&gt; 按照Bayes的方法, 我们需要计算 &lt;span class=&quot;math display&quot;&gt;\[p(y=c\vert x,\mathcal{D}) \propto [\int Cat(y=c\vert \pi) p(\pi\vert \mathcal{D})d\pi] \prod_{j=1}^D\int Ber(x_j\vert y=c,\vec{\theta_{jc}})p(\vec{\theta_{jc}}\vert \mathcal{D})d\vec{\theta_{jc}}\]&lt;/span&gt;&lt;/p&gt;</content><author><name>  黄锡昆</name></author><category term="笔记" /><category term="mlapp" /><category term="机器学习" /></entry><entry><title type="html">MLAPP笔记-概率</title><link href="https://xikunhuang.github.io/%E7%AC%94%E8%AE%B0/MLAPP%E7%AC%94%E8%AE%B0-%E6%A6%82%E7%8E%87/" rel="alternate" type="text/html" title="MLAPP笔记-概率" /><published>2018-11-27T00:00:00+00:00</published><updated>2018-11-27T00:00:00+00:00</updated><id>https://xikunhuang.github.io/%E7%AC%94%E8%AE%B0/MLAPP%E7%AC%94%E8%AE%B0-%E6%A6%82%E7%8E%87</id><content type="html" xml:base="https://xikunhuang.github.io/%E7%AC%94%E8%AE%B0/MLAPP%E7%AC%94%E8%AE%B0-%E6%A6%82%E7%8E%87/">&lt;h1 data-number=&quot;1&quot; id=&quot;简介&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;1&lt;/span&gt; 简介&lt;/h1&gt;
&lt;p&gt;概率两大派:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;频率学派(frequentist)&lt;/li&gt;
&lt;li&gt;贝叶斯学派(Bayesian)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;频率学派将概率解释为事件在多次实验下发生的频率(long run frequencies of events). 贝叶斯学派用概率来量化我们对一些事情的不确定性, 因此概率本质上与信息有关, 而与实验次数无关.&lt;/p&gt;
&lt;p&gt;举个例子, 对于“硬币正面朝上的概率为0.5”这句表述, 频率学派是在说如果我们掷硬币很多次, 那么大概有一半的次数硬币朝上. 而贝叶斯学派是在说我们相信下一次掷硬币出现正面和反面的可能性相同.&lt;/p&gt;
&lt;h1 data-number=&quot;2&quot; id=&quot;概率论&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;2&lt;/span&gt; 概率论&lt;/h1&gt;
&lt;p&gt;本小节简单介绍了一些概率论的基本概念和公式, 这里不多赘述. 只记录一些看了有些启发的.&lt;/p&gt;
&lt;h2 data-number=&quot;2.1&quot; id=&quot;贝叶斯公式&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;2.1&lt;/span&gt; 贝叶斯公式&lt;/h2&gt;
&lt;p&gt;贝叶斯公式给出的结果往往反“直觉”, 当然这里的“直觉”是错的. 书中正文和习题中给了三个例子.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;医疗诊断 &amp;gt; 假设如果人类患有某种疾病y, 那么在检查时指标x为阳性的概率为0.8. 现在如果有个人检测到指标x为阳性, 那么请问: 这个人患有疾病y的概率是多少?&lt;/p&gt;
&lt;p&gt;0.8? 正确答案是不知道, 因为给的信息不足以做出判断. 我们再知道两个信息就可以做出判断: 人群中该疾病患病率是多少? 一个没患病的人检测到指标x为阳性的概率是多少? 这里我们假设 &lt;span class=&quot;math inline&quot;&gt;\(p(y=1)=0.004, p(x=1\vert y=0)=0.1\)&lt;/span&gt;, 那么由此可以计算出&lt;span class=&quot;math inline&quot;&gt;\(p(y=1\vert x=1)=0.031\)&lt;/span&gt;. 换句话说, 即使指标x被检测出阳性, 这个人也只有大约3%的概率真正患有疾病y!&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;公诉人与辩护人谬论 &amp;gt; 假设有一起案件, 案发现场发现了凶手的血迹, 经检查该血型是一种及其罕见的类型(比如说人群中只有1%的人拥有这种血型). 那么现在警察抓住了一个嫌疑人, 并且嫌疑人的血型恰好就是这种类型, 那么请问: 在这种情况下, 这个嫌疑人是真正凶手的概率是多少?&lt;/p&gt;
&lt;p&gt;这种情况下我们会大概率认为这个嫌疑人就是凶手了. 其实不然, 还是信息不够, 不足以做出判断.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The Monty Hall problem &amp;gt; 一共三个箱子编号1,2,3. 其中只有一个箱子里面有奖品, 而且主持人知道哪个箱子有礼品. 现在让你猜奖品在哪个箱子里. 假设你选择了1号箱子, 那么主持人接下来打开2号或3号箱子中的一个, 而且主持人会保证不会打开有奖品的箱子(因为如果打开了有奖品的箱子游戏就没法进行下去了…主持人知道哪个箱子有奖品所以可以保证这一点). 现在主持人问你要不要更换自己的选择, 是继续坚持1号箱子,还是换成另一个箱子呢?&lt;/p&gt;
&lt;p&gt;不要犹豫,换! 不换的话赢的概率是1/3,换的话赢的概率是2/3.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 data-number=&quot;2.2&quot; id=&quot;独立性&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;2.2&lt;/span&gt; 独立性&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;无条件独立 &lt;span class=&quot;math display&quot;&gt;\[X \perp Y \iff p(X,Y)=p(X)p(Y)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;条件独立 &lt;span class=&quot;math display&quot;&gt;\[X \perp Y \vert  Z \iff p(X,Y\vert Z)=p(X\vert Z)p(Y\vert Z)\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[X \perp Y \vert  Z \iff 存在函数g和h使得 p(x,y\vert z)=g(x,z)h(y,z)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;两两独立(Pairwise Independent) 和 相互独立(Mutually Indepedent) 相互独立: &lt;span class=&quot;math display&quot;&gt;\[p(X_i\vert X_S)=p(X_i), \forall S\subseteq\{1,\dots,n\}\setminus\{i\}\]&lt;/span&gt; 两两独立推不出相互独立. 反例: X1和X2都是等概率取值{0,1}的随机变量,X3=XOR(X1,X2). 则X1,X2,X3两两独立但是不是相互独立.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;不相关推不出独立. 所以相关系数不能作为衡量独立性的指标, 更好的指标是后面介绍的互信息. 反例: &lt;span class=&quot;math inline&quot;&gt;\(X\sim U(-1,1), Y=X^2\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 data-number=&quot;3&quot; id=&quot;概率分布&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;3&lt;/span&gt; 概率分布&lt;/h1&gt;
&lt;p&gt;书中介绍了一堆常见的离散型和连续型概率分布, 可是记不住啊…还是要应用…&lt;/p&gt;
&lt;h1 data-number=&quot;4&quot; id=&quot;随机变量的变换&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;4&lt;/span&gt; 随机变量的变换&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;change of variables formula&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 data-number=&quot;5&quot; id=&quot;蒙特卡洛近似&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;5&lt;/span&gt; 蒙特卡洛近似&lt;/h1&gt;
&lt;p&gt;通常根据随机变量变换公式来计算随机变量函数的分布是很困难的. 一个简单有效的方法是先从分布&lt;span class=&quot;math inline&quot;&gt;\(X\)&lt;/span&gt;中采样&lt;span class=&quot;math inline&quot;&gt;\(S\)&lt;/span&gt;个样本点&lt;span class=&quot;math inline&quot;&gt;\(x_1,\dots,x_S\)&lt;/span&gt;, 然后用&lt;span class=&quot;math inline&quot;&gt;\(\{f(x_s\}^S_{s=1}\)&lt;/span&gt;的实际分布来近似&lt;span class=&quot;math inline&quot;&gt;\(f(X)\)&lt;/span&gt;的分布. 这种方法就是蒙塔卡洛近似.&lt;/p&gt;
&lt;p&gt;进一步可以搞出蒙特卡洛积分.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[E[f(X)]=\int f(x)p(x)dx \approx \frac{1}{S}\sum_{s=1}^Sf(x_s)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&quot;math inline&quot;&gt;\(x_s \sim p(X)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;蒙特卡洛近似的精度随着样本规模的变大而增大.&lt;/p&gt;
&lt;h1 data-number=&quot;6&quot; id=&quot;信息论&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;6&lt;/span&gt; 信息论&lt;/h1&gt;
&lt;h2 data-number=&quot;6.1&quot; id=&quot;熵entropy&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;6.1&lt;/span&gt; 熵(entropy)&lt;/h2&gt;
&lt;p&gt;含义: 度量随机变量的不确定性. &lt;span class=&quot;math display&quot;&gt;\[\mathbb{H}(X) \triangleq -\sum\limits_{k=1}^Kp(X=k)\log_2 p(X=k)\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 data-number=&quot;6.2&quot; id=&quot;kl散度kl-divergence&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;6.2&lt;/span&gt; KL散度(KL divergence)&lt;/h2&gt;
&lt;p&gt;含义: 度量两个随机变量的差异程度. &lt;span class=&quot;math display&quot;&gt;\[\mathbb{KL}(p\vert \vert q) \triangleq \sum\limits_{k=1}^K p_k \log \frac{p_k}{q_k}\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 data-number=&quot;6.3&quot; id=&quot;交叉熵cross-entropy&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;6.3&lt;/span&gt; 交叉熵(cross entropy)&lt;/h2&gt;
&lt;p&gt;含义: 当数据真实分布是&lt;span class=&quot;math inline&quot;&gt;\(p\)&lt;/span&gt;, 而我们用分布&lt;span class=&quot;math inline&quot;&gt;\(q\)&lt;/span&gt;编码数据时需要的平均位数. &lt;span class=&quot;math display&quot;&gt;\[\mathbb{H}(p,q) \triangleq -\sum\limits_k p_k\log q_k\]&lt;/span&gt; 注意到&lt;span class=&quot;math inline&quot;&gt;\(\mathbb{H}(p,p)=\mathbb{H}(p)\)&lt;/span&gt;, KL散度可以写成 &lt;span class=&quot;math display&quot;&gt;\[\mathbb{KL}(p\vert \vert q) = \mathbb{H}(p,q) - \mathbb{H}(p,p)\]&lt;/span&gt; 也就是说KL散度的含义是由于我们用分布&lt;span class=&quot;math inline&quot;&gt;\(q\)&lt;/span&gt;而不是数据的真实分布&lt;span class=&quot;math inline&quot;&gt;\(p\)&lt;/span&gt;去编码数据时, 平均需要的额外的位数.&lt;/p&gt;
&lt;h2 data-number=&quot;6.4&quot; id=&quot;互信息mutual-information&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;6.4&lt;/span&gt; 互信息(mutual information)&lt;/h2&gt;
&lt;p&gt;含义: 度量联合分布&lt;span class=&quot;math inline&quot;&gt;\(p(X,Y)\)&lt;/span&gt;与&lt;span class=&quot;math inline&quot;&gt;\(p(X)p(Y)\)&lt;/span&gt;的相似程度. &lt;span class=&quot;math display&quot;&gt;\[\mathbb{I}(X;Y) \triangleq \mathbb{KL}(p(X,Y) \vert \vert  p(X)p(Y)) = \sum\limits_x\sum\limits_y p(x,y)\log\frac{p(x,y)}{p(x)p(y)}\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 data-number=&quot;6.5&quot; id=&quot;条件熵conditional-entropy&quot;&gt;&lt;span class=&quot;header-section-number&quot;&gt;6.5&lt;/span&gt; 条件熵(conditional entropy)&lt;/h2&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[\mathbb{H}(Y\vert X) \triangleq \sum\limits_x p(x)\mathbb{H}(Y\vert X=x)\]&lt;/span&gt; 可以证明 &lt;span class=&quot;math display&quot;&gt;\[\mathbb{I}(X;Y) = \mathbb{H}(X) - \mathbb{H}(X\vert Y) = \mathbb{H}(Y) - \mathbb{H}(Y\vert X)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;下面是几个重要结论&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;span class=&quot;math inline&quot;&gt;\(\mathbb{KL}(p\vert \vert q) \geq 0\)&lt;/span&gt; 等号成立当且仅当&lt;span class=&quot;math inline&quot;&gt;\(p=q\)&lt;/span&gt;.&lt;/strong&gt; 提示: 递推法证明凸函数的Jensen不等式, 对-log函数应用该不等式即可.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;离散随机变量为均匀分布时熵最大, 且最大值为$&lt;span class=&quot;math inline&quot;&gt;\(, 其中\)&lt;/span&gt;&lt;span class=&quot;math inline&quot;&gt;\(\vert 是\)&lt;/span&gt;X$的状态数目.&lt;/strong&gt; 提示: 上述结论的重要推论之一. 计算分布p与均匀分布u的KL散度.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;最大似然估计就是最小化模型与实际分布(empirical distribution)的KL散度.&lt;/strong&gt; 提示: 大数定律 &lt;span class=&quot;math inline&quot;&gt;\(E_{x\sim pemp(x)}\log q(x;\theta)=\frac{1}{N}\sum\limits_{i=1}{N}\log q(x;\theta)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>  黄锡昆</name></author><category term="笔记" /><category term="mlapp" /><category term="机器学习" /></entry></feed>