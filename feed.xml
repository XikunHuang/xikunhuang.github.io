<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://xikunhuang.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://xikunhuang.github.io/" rel="alternate" type="text/html" /><updated>2020-07-31T08:32:47+00:00</updated><id>https://xikunhuang.github.io/feed.xml</id><title type="html">Xikun</title><subtitle>A personal website.</subtitle><author><name>  黄锡昆</name></author><entry><title type="html">MLAPP笔记-高斯模型</title><link href="https://xikunhuang.github.io/%E6%8A%80%E6%9C%AF/MLAPP%E7%AC%94%E8%AE%B0-%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/" rel="alternate" type="text/html" title="MLAPP笔记-高斯模型" /><published>2019-01-05T00:00:00+00:00</published><updated>2019-01-05T00:00:00+00:00</updated><id>https://xikunhuang.github.io/%E6%8A%80%E6%9C%AF/MLAPP%E7%AC%94%E8%AE%B0-%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B</id><content type="html" xml:base="https://xikunhuang.github.io/%E6%8A%80%E6%9C%AF/MLAPP%E7%AC%94%E8%AE%B0-%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/">&lt;h2 id=&quot;简介&quot;&gt;简介&lt;/h2&gt;
&lt;p&gt;本章讨论多元高斯或者称多元正态(MVN), 数学要求稍高一些.&lt;/p&gt;
&lt;h3 id=&quot;基础&quot;&gt;基础&lt;/h3&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(D\)&lt;/span&gt;维MVN的pdf为&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[\mathcal{N}(x|\mu,\Sigma) = \frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}}\exp[-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;上述pdf式中指数里的表达式去掉&lt;span class=&quot;math inline&quot;&gt;\(-\frac{1}{2}\)&lt;/span&gt;后实际上就是&lt;span class=&quot;math inline&quot;&gt;\(x\)&lt;/span&gt;和&lt;span class=&quot;math inline&quot;&gt;\(\mu\)&lt;/span&gt;之间的&lt;a href=&quot;https://en.wikipedia.org/wiki/Mahalanobis_distance&quot;&gt;Mahalanobis距离&lt;/a&gt;, 关于这个距离更多的知识可以参考Wikipedia.&lt;/p&gt;
&lt;p&gt;现在考虑这样一个问题: MVN的pdf的等值线是什么样的呢?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果协方差矩阵&lt;span class=&quot;math inline&quot;&gt;\(\Sigma\)&lt;/span&gt;是对角阵, 那么Mahalanobis距离就变成了&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[(x-\mu)^T\Sigma^{-1}(x-\mu) = \sum_{i=1}^{D}\frac{1}{\lambda_i}(x_i-\mu_i)^2 = \sum_{i=1}^{D}\frac{1}{\lambda_i} y_i^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&quot;math inline&quot;&gt;\(\lambda_i\)&lt;/span&gt;是协方差矩阵的对角元素, &lt;span class=&quot;math inline&quot;&gt;\(y_i = x_i-\mu_i\)&lt;/span&gt; 在二维情形下这个等值线就是椭圆. &lt;span class=&quot;math inline&quot;&gt;\(\frac{1}{\lambda_1}y_1^2 + \frac{1}{\lambda_2} y_2^2 = k\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果协方差不是对角阵, 那么由于它是实对称矩阵, 因此可以正交分解为 &lt;span class=&quot;math inline&quot;&gt;\(\Sigma = U\Lambda U^T\)&lt;/span&gt; 此时&lt;span class=&quot;math inline&quot;&gt;\(y_i = u_i^T(x-\mu)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由此也可以看出Mahalanobis距离与欧氏距离的联系.&lt;/p&gt;
&lt;h3 id=&quot;mle-for-an-mvn&quot;&gt;MLE for an MVN&lt;/h3&gt;
&lt;p&gt;最大似然估计是估计MVN的参数的方法之一. 但是最大似然估计有过拟合的缺点,后续会讨论MVN参数的Bayes推断, 这种方法可以消除过拟合, 并且可以为估计提供置信度.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;: 如果&lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt;个&lt;span class=&quot;math inline&quot;&gt;\(iid\)&lt;/span&gt;样本&lt;span class=&quot;math display&quot;&gt;\[\mathbb{x}_i\sim\mathcal{N}(\mu,\Sigma)\]&lt;/span&gt;, 那么MVN参数的最大似然估计为 &lt;span class=&quot;math display&quot;&gt;\[\hat{\mu}_{mle} = \frac{1}{N}\sum_{i=1}^{N}\mathbb{x}_i = \bar{\mathbb{x}}\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[\hat{\Sigma}_{mle} = \frac{1}{N}\sum_{i=1}^{N}(\mathbb{x_i}-\bar{\mathbb{x}})(\mathbb{x_i}-\bar{\mathbb{x}})^T\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;多元高斯的一个有趣性质&quot;&gt;多元高斯的一个有趣性质&lt;/h3&gt;
&lt;p&gt;对于给定数据, 我们从数据中可能仅仅能可靠地估计出均值与方差. 这个时候我们需要一个分布能够具有估计出的均值与方差, 同时又尽量做出最少的假设, 多元高斯就是我们需要的分布. &lt;strong&gt;对于给定的均值与方差,多元高斯分布是熵最大的分布&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;高斯判别分析&quot;&gt;高斯判别分析&lt;/h3&gt;
&lt;p&gt;多元高斯的一个重要应用是在生成式模型中定义类条件分布, 即 &lt;span class=&quot;math display&quot;&gt;\[p(\mathbb{x}|y=c,\theta)=\mathcal{N}(\mathbb{x}|\mu_c,\Sigma_c)\]&lt;/span&gt; 这个被成为高斯判别分析.(实际上是生成模型, 名字容易引起误导). 如果&lt;span class=&quot;math inline&quot;&gt;\(\Sigma_c\)&lt;/span&gt;是对角矩阵, 那么就相当于属性条件独立了, 即等价于朴素贝叶斯. 对于新来的数据, 我们根据下式来进行分类 &lt;span class=&quot;math display&quot;&gt;\[\hat{y}(\mathbb{x}) = \arg\max_c[\log p(y=c|\theta) + \log p(\mathbb{x}|y=c,\theta)] \]&lt;/span&gt; 其中在计算第二项时, 我们实际上是在计算&lt;span class=&quot;math inline&quot;&gt;\(\mathbb{x}\)&lt;/span&gt;与每个类的中心的距离, 只不过是Mahalanobis距离, 这个可以被看成是 nearest centroids classifier.&lt;/p&gt;
&lt;h4 id=&quot;quadratic-discriminant-analysisqda&quot;&gt;Quadratic discriminant analysis(QDA)&lt;/h4&gt;
&lt;p&gt;高斯判别分析分类准则展开之后分类边界是关于&lt;span class=&quot;math inline&quot;&gt;\(\mathbb{x}\)&lt;/span&gt;的二次函数, 这个被称为QDA.&lt;/p&gt;
&lt;h4 id=&quot;linear-discriminant-analysislda&quot;&gt;Linear discriminant analysis(LDA)&lt;/h4&gt;
&lt;p&gt;但是如果假定不同类的协方差矩阵相同, 那么这时候分类边界变成了&lt;span class=&quot;math inline&quot;&gt;\(\mathbb{x}\)&lt;/span&gt;的线性函数, 这个被称为LDA.&lt;/p&gt;
&lt;h4 id=&quot;mle-for-discriminant-analysis&quot;&gt;MLE for discriminant analysis&lt;/h4&gt;
&lt;p&gt;可以利用最大似然估计方法来估计高斯判别分析准则中的参数. &lt;span class=&quot;math display&quot;&gt;\[\hat{\pi}_c = \frac{N_c}{N}\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[\hat{\mu}_c = \frac{1}{N_c}\sum_{i:y_i=c}\mathbb{x}_i\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[\hat{\Sigma_c} = \frac{1}{N_c}\sum_{i:y_i=c}(\mathbb{x}_i-\hat{\mu}_c)(\mathbb{x}_i-\hat{\mu}_c)^T\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&quot;防止过拟合的方法&quot;&gt;防止过拟合的方法&lt;/h4&gt;
&lt;p&gt;MLE虽然简单快速, 但是有过拟合的缺点. 比如当&lt;span class=&quot;math inline&quot;&gt;\(N_c&amp;lt;D\)&lt;/span&gt;时, MLE估计出的协方差矩阵是奇异矩阵, 如果真实协方差矩阵是满秩的, 那么就过拟合了. 对此有一些方法来防止过拟合:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;假设每个类的协方差矩阵是对角阵, 即假设属性之间条件独立, 这等价于朴素贝叶斯.&lt;/li&gt;
&lt;li&gt;假设所有类的协方差矩阵相同, 这等价于LDA.&lt;/li&gt;
&lt;li&gt;假设协方差矩阵是对角阵, 且所有类协方差矩阵相同.&lt;/li&gt;
&lt;li&gt;使用一般的协方差矩阵, 但是引入先验.&lt;/li&gt;
&lt;li&gt;使用MAP估计&lt;/li&gt;
&lt;li&gt;将数据映射到低维空间, 然后在低维空间上使用判别分析.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;inference-in-jointly-gaussian-distributions&quot;&gt;Inference in jointly Gaussian distributions&lt;/h3&gt;
&lt;p&gt;本小节讨论给定联合分布&lt;span class=&quot;math inline&quot;&gt;\(p(\mathbb{x}_1,\mathbb{x}_2)\)&lt;/span&gt;, 边际分布&lt;span class=&quot;math inline&quot;&gt;\(p(\mathbb{x}_1)\)&lt;/span&gt;和条件分布 &lt;span class=&quot;math inline&quot;&gt;\(p(\mathbb{x}_1\mid\mathbb{x}_2)\)&lt;/span&gt; 是怎样的呢? 如果联合分布是多元高斯分布, 那么边际分布和条件分布仍然是高斯分布. 有如下结论:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;假设 &lt;span class=&quot;math inline&quot;&gt;\(\mathbb{x}=(\mathbb{x}_1,\mathbb{x}_2)\)&lt;/span&gt; 是多元高斯分布, 参数为 &lt;span class=&quot;math display&quot;&gt;\[\mu = \left(\begin{matrix} \mu_1 \\ \mu_2 \end{matrix}\right)\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[\Sigma = \left(\begin{matrix} \Sigma_{11} &amp;amp; \Sigma_{12} \\ \Sigma_{21} &amp;amp; \Sigma_{22} \end{matrix}\right)\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[\Lambda = \Sigma^{-1} = \left(\begin{matrix} \Lambda_{11} &amp;amp; \Lambda_{12} \\ \Lambda_{21} &amp;amp; \Lambda_{22} \end{matrix}\right)\]&lt;/span&gt; 那么边际分布为 &lt;span class=&quot;math display&quot;&gt;\[p(\mathbb{x}_1) = \mathcal{N}(\mathbb{x}_1|\mu_1, \Sigma_{11})\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[p(\mathbb{x}_2) = \mathcal{N}(\mathbb{x}_2|\mu_2, \Sigma_{22})\]&lt;/span&gt; 条件分布为 &lt;span class=&quot;math display&quot;&gt;\[p(\mathbb{x}_1|\mathbb{x}_2) = \mathcal{N}(\mathbb{x}_1 | \mu_{1|2}, \Sigma_{1|2})\]&lt;/span&gt; 其中 &lt;span class=&quot;math display&quot;&gt;\[\begin{align}
\mu_{1|2} &amp;amp; = \mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(\mathbb{x}_2-\mu_2) \\
&amp;amp; = \mu_1 - \Lambda_{11}^{-1}\Lambda_{12}(\mathbb{x}_2-\mu_2) \\
&amp;amp; = \Sigma_{1|2}(\Lambda_{11}\mu_1 - \Lambda_{12}(\mathbb{x}_2-\mu_2))
\end{align}\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[\Sigma_{1|2} = \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21} = \Lambda_{11}^{-1}\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;上述结论非常重要. 同时注意到上面用到了不同的参数表示形式. 某些情况下用 moment parameters比较方便, 某些情况下用 canonical parameters 比较方便.&lt;/p&gt;
&lt;h3 id=&quot;linear-gaussian-systems&quot;&gt;Linear Gaussian systems&lt;/h3&gt;
&lt;p&gt;假定有两个变量&lt;span class=&quot;math inline&quot;&gt;\(\mathbb{x}\)&lt;/span&gt;和&lt;span class=&quot;math inline&quot;&gt;\(\mathbb{y}\)&lt;/span&gt;, &lt;span class=&quot;math inline&quot;&gt;\(\mathbb{x}\in \mathbb{R}^{D_x}\)&lt;/span&gt;是隐变量, &lt;span class=&quot;math inline&quot;&gt;\(\mathbb{y}\in \mathbb{R}^{D_y}\)&lt;/span&gt;是 noisy observation of &lt;span class=&quot;math inline&quot;&gt;\(\mathbb{x}\)&lt;/span&gt;. 假定我们有如下先验和似然 &lt;span class=&quot;math display&quot;&gt;\[p(\mathbb{x}) = \mathcal{N}(\mathbb{x}|\mu_x, \Sigma_x)\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[p(\mathbb{y}|\mathbb{x}) = \mathcal{N}(\mathbb{y}|A\mathbb{x} + b, \Sigma_y)\]&lt;/span&gt; 这被成为一个线性高斯系统, 现在想要求&lt;span class=&quot;math inline&quot;&gt;\(\mathbb{x}\)&lt;/span&gt;的后验分布&lt;span class=&quot;math inline&quot;&gt;\(p(\mathbb{x}|\mathbb  {y})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;有如下结论 &lt;span class=&quot;math display&quot;&gt;\[p(\mathbb{x}|\mathbb{y}) = \mathcal{N}(\mathbb{x}|\mu_{x|y}, \Sigma_{x|y})\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[\Sigma_{x|y}^{-1} = \Sigma_{x}^{-1} + A^T\Sigma_{y}^{-1}A\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[\mu_{x|y} = \Sigma_{x|y}[A^T\Sigma_{y}^{-1}(\mathbb{y} - b) + \Sigma_{x}^{-1}\mu_{x}]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;此外 &lt;span class=&quot;math display&quot;&gt;\[p(\mathbb{y}) = \mathcal{N}(\mathbb{y}|A\mu_x + b, \Sigma_y+A\Sigma_xA^T)\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;inferring-the-parameters-for-an-mvn&quot;&gt;Inferring the parameters for an MVN&lt;/h3&gt;
&lt;p&gt;之前小节提到用最大似然方法估计MVN的参数, 但是具有过拟合的缺点. 这节介绍的是利用贝叶斯方法来推断MVN的参数. 即引入共轭先验分布, 然后计算后验分布, 将后验分布的mode或mean作为参数估计值. 本节考虑了三个情况:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;知道协方差矩阵, 推断均值&lt;/li&gt;
&lt;li&gt;知道均值, 推断协方差矩阵&lt;/li&gt;
&lt;li&gt;均值与协方差矩阵都未知, 同时推断&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;知道协方差矩阵-推断均值&quot;&gt;知道协方差矩阵, 推断均值&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;这种情况下共轭先验为高斯分布&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&quot;知道均值-推断协方差矩阵&quot;&gt;知道均值, 推断协方差矩阵&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;这种情况下共轭先验为 inverse Wishart distribution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&quot;均值与协方差矩阵都未知-同时推断&quot;&gt;均值与协方差矩阵都未知, 同时推断&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;这种情况下共轭先验为 Normal-inverse-wishart distribution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;具体结果按下不表. 值得一提的是Bayes这种计算后验分布的方法与频率派最大似然估计方法有着形式上的紧密联系. 如果先验分布是uninformative的话, 那么贝叶斯方法计算的结果与频率学派计算的结果相同. 但是只是形式上的相同, 对于结果的解释不同.&lt;/p&gt;</content><author><name>  黄锡昆</name></author><category term="技术" /><category term="MLAPP" /><category term="机器学习" /><summary type="html">简介 本章讨论多元高斯或者称多元正态(MVN), 数学要求稍高一些. 基础 \(D\)维MVN的pdf为 \[\mathcal{N}(x|\mu,\Sigma) = \frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}}\exp[-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)]\] 上述pdf式中指数里的表达式去掉\(-\frac{1}{2}\)后实际上就是\(x\)和\(\mu\)之间的Mahalanobis距离, 关于这个距离更多的知识可以参考Wikipedia. 现在考虑这样一个问题: MVN的pdf的等值线是什么样的呢? 如果协方差矩阵\(\Sigma\)是对角阵, 那么Mahalanobis距离就变成了 \[(x-\mu)^T\Sigma^{-1}(x-\mu) = \sum_{i=1}^{D}\frac{1}{\lambda_i}(x_i-\mu_i)^2 = \sum_{i=1}^{D}\frac{1}{\lambda_i} y_i^2\] 其中\(\lambda_i\)是协方差矩阵的对角元素, \(y_i = x_i-\mu_i\) 在二维情形下这个等值线就是椭圆. \(\frac{1}{\lambda_1}y_1^2 + \frac{1}{\lambda_2} y_2^2 = k\) 如果协方差不是对角阵, 那么由于它是实对称矩阵, 因此可以正交分解为 \(\Sigma = U\Lambda U^T\) 此时\(y_i = u_i^T(x-\mu)\) 由此也可以看出Mahalanobis距离与欧氏距离的联系. MLE for an MVN 最大似然估计是估计MVN的参数的方法之一. 但是最大似然估计有过拟合的缺点,后续会讨论MVN参数的Bayes推断, 这种方法可以消除过拟合, 并且可以为估计提供置信度. 定理: 如果\(N\)个\(iid\)样本\[\mathbb{x}_i\sim\mathcal{N}(\mu,\Sigma)\], 那么MVN参数的最大似然估计为 \[\hat{\mu}_{mle} = \frac{1}{N}\sum_{i=1}^{N}\mathbb{x}_i = \bar{\mathbb{x}}\] \[\hat{\Sigma}_{mle} = \frac{1}{N}\sum_{i=1}^{N}(\mathbb{x_i}-\bar{\mathbb{x}})(\mathbb{x_i}-\bar{\mathbb{x}})^T\] 多元高斯的一个有趣性质 对于给定数据, 我们从数据中可能仅仅能可靠地估计出均值与方差. 这个时候我们需要一个分布能够具有估计出的均值与方差, 同时又尽量做出最少的假设, 多元高斯就是我们需要的分布. 对于给定的均值与方差,多元高斯分布是熵最大的分布 高斯判别分析 多元高斯的一个重要应用是在生成式模型中定义类条件分布, 即 \[p(\mathbb{x}|y=c,\theta)=\mathcal{N}(\mathbb{x}|\mu_c,\Sigma_c)\] 这个被成为高斯判别分析.(实际上是生成模型, 名字容易引起误导). 如果\(\Sigma_c\)是对角矩阵, 那么就相当于属性条件独立了, 即等价于朴素贝叶斯. 对于新来的数据, 我们根据下式来进行分类 \[\hat{y}(\mathbb{x}) = \arg\max_c[\log p(y=c|\theta) + \log p(\mathbb{x}|y=c,\theta)] \] 其中在计算第二项时, 我们实际上是在计算\(\mathbb{x}\)与每个类的中心的距离, 只不过是Mahalanobis距离, 这个可以被看成是 nearest centroids classifier. Quadratic discriminant analysis(QDA) 高斯判别分析分类准则展开之后分类边界是关于\(\mathbb{x}\)的二次函数, 这个被称为QDA. Linear discriminant analysis(LDA) 但是如果假定不同类的协方差矩阵相同, 那么这时候分类边界变成了\(\mathbb{x}\)的线性函数, 这个被称为LDA. MLE for discriminant analysis 可以利用最大似然估计方法来估计高斯判别分析准则中的参数. \[\hat{\pi}_c = \frac{N_c}{N}\] \[\hat{\mu}_c = \frac{1}{N_c}\sum_{i:y_i=c}\mathbb{x}_i\] \[\hat{\Sigma_c} = \frac{1}{N_c}\sum_{i:y_i=c}(\mathbb{x}_i-\hat{\mu}_c)(\mathbb{x}_i-\hat{\mu}_c)^T\] 防止过拟合的方法 MLE虽然简单快速, 但是有过拟合的缺点. 比如当\(N_c&amp;lt;D\)时, MLE估计出的协方差矩阵是奇异矩阵, 如果真实协方差矩阵是满秩的, 那么就过拟合了. 对此有一些方法来防止过拟合: 假设每个类的协方差矩阵是对角阵, 即假设属性之间条件独立, 这等价于朴素贝叶斯. 假设所有类的协方差矩阵相同, 这等价于LDA. 假设协方差矩阵是对角阵, 且所有类协方差矩阵相同. 使用一般的协方差矩阵, 但是引入先验. 使用MAP估计 将数据映射到低维空间, 然后在低维空间上使用判别分析. Inference in jointly Gaussian distributions 本小节讨论给定联合分布\(p(\mathbb{x}_1,\mathbb{x}_2)\), 边际分布\(p(\mathbb{x}_1)\)和条件分布 \(p(\mathbb{x}_1\mid\mathbb{x}_2)\) 是怎样的呢? 如果联合分布是多元高斯分布, 那么边际分布和条件分布仍然是高斯分布. 有如下结论:</summary></entry><entry><title type="html">MLAPP笔记-离散数据生成模型</title><link href="https://xikunhuang.github.io/%E6%8A%80%E6%9C%AF/MLAPP%E7%AC%94%E8%AE%B0-%E7%A6%BB%E6%95%A3%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/" rel="alternate" type="text/html" title="MLAPP笔记-离散数据生成模型" /><published>2018-11-28T00:00:00+00:00</published><updated>2018-11-28T00:00:00+00:00</updated><id>https://xikunhuang.github.io/%E6%8A%80%E6%9C%AF/MLAPP%E7%AC%94%E8%AE%B0-%E7%A6%BB%E6%95%A3%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B</id><content type="html" xml:base="https://xikunhuang.github.io/%E6%8A%80%E6%9C%AF/MLAPP%E7%AC%94%E8%AE%B0-%E7%A6%BB%E6%95%A3%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/">&lt;h2 id=&quot;简介&quot;&gt;简介&lt;/h2&gt;
&lt;p&gt;先来说一下&lt;strong&gt;生成式模型(generative models)&lt;/strong&gt; 和 &lt;strong&gt;判别式模型(discriminative models)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;对于分类问题, 我们的目标是基于有限的训练样本集尽可能准确地估计出后验概率&lt;span class=&quot;math inline&quot;&gt;\(p(c\vert x)\)&lt;/span&gt;, 而估计该后验概率有两种方式:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;判别式模型: 直接建模&lt;span class=&quot;math inline&quot;&gt;\(p(c \vert x)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;生成式模型: 先对联合分布&lt;span class=&quot;math inline&quot;&gt;\(p(x,c)\)&lt;/span&gt;建模, 再由此得到&lt;span class=&quot;math inline&quot;&gt;\(p(c \vert x)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本章着眼于生成式模型, 由贝叶斯定理可得 &lt;span class=&quot;math display&quot;&gt;\[p(c\vert x)=\frac{p(x\vert c)p(c)}{p(x)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以问题关键就在于求得&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;类条件概率分布(class-conditional density) &lt;span class=&quot;math inline&quot;&gt;\(p(x\vert c)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;先验分布 &lt;span class=&quot;math inline&quot;&gt;\(p(c)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;概率分布&quot;&gt;概率分布&lt;/h2&gt;
&lt;p&gt;继续进行之前, 这里先罗列一下本章涉及的一些概率分布.&lt;/p&gt;
&lt;h3 id=&quot;伯努利分布和二项分布&quot;&gt;伯努利分布和二项分布&lt;/h3&gt;
&lt;p&gt;可用来建模掷硬币的结果&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;伯努利分布&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[X \sim Ber(\theta)\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[Ber(x\vert \theta) = \theta^{\mathbb{I}(x=1)}(1-\theta)^{\mathbb{I}(x=0)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;即随机变量&lt;span class=&quot;math inline&quot;&gt;\(X \in \{0,1\}\)&lt;/span&gt;, 且 &lt;span class=&quot;math display&quot;&gt;\[p(X=1) = \theta\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[p(X=0) = 1 - \theta\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;二项分布&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[X \sim Bin(n,\theta)\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[Bin(k\vert n,\theta) = \binom{n}{k} \theta^k (1-\theta)^{n-k} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;即随机变量&lt;span class=&quot;math inline&quot;&gt;\(X \in \{0,\dots, n\}\)&lt;/span&gt;, 且 &lt;span class=&quot;math display&quot;&gt;\[p(X=k) =  \binom{n}{k} \theta^k (1-\theta)^{n-k}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;多伯努利分布和多项分布&quot;&gt;多伯努利分布和多项分布&lt;/h3&gt;
&lt;p&gt;可用来建模掷具有&lt;span class=&quot;math inline&quot;&gt;\(K\)&lt;/span&gt;面的骰子的结果&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;多伯努利分布&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[X \sim Cat(\theta)\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[Cat(x\vert \theta) = \prod_{j=1}^K \theta_j^{\mathbb{I}(x_j=1)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(X\)&lt;/span&gt;取值为&lt;span class=&quot;math inline&quot;&gt;\(K\)&lt;/span&gt;维one-hot编码.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;多项分布&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[X \sim Mu(n, \theta)\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[Mu(x\vert n,\theta) = \binom{n}{x_1 \dots x_K} \prod_{j=1}^K \theta_j^k\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(X\)&lt;/span&gt;是K维向量, 且满足 &lt;span class=&quot;math display&quot;&gt;\[x_k\in\{0,\dots,n\}\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[\sum_{k=1}^Kx_k = n\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;贝塔分布和狄利克雷分布&quot;&gt;贝塔分布和狄利克雷分布&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;贝塔分布&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[X \sim Beta(a,b)\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[Beta(x\vert a,b) = \frac{1}{B(a,b)}x^{a-1}(1-x)^{b-1}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(X\)&lt;/span&gt;取值范围是&lt;span class=&quot;math inline&quot;&gt;\([0,1]\)&lt;/span&gt;. 要求&lt;span class=&quot;math inline&quot;&gt;\(a,b&amp;gt;0\)&lt;/span&gt;. 当&lt;span class=&quot;math inline&quot;&gt;\(a=b=1\)&lt;/span&gt;时退化为&lt;span class=&quot;math inline&quot;&gt;\([0,1]\)&lt;/span&gt;上的均匀分布.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;狄利克雷分布&lt;/p&gt;
&lt;p&gt;贝塔分布的多元扩展, 相当于考虑的是多个随机变量的联合分布.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[X \sim Dir(\alpha_1,\dots, \alpha_k)\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[Dir(x\vert \alpha_1,\dots,\alpha_k) = \frac{1}{B(\alpha_1,\dots,\alpha_k)}\prod_{k=1}^K x_k^{\alpha_k - 1} \mathbb{I}(x\in S_K)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&quot;math display&quot;&gt;\[ S_K = \{x:0\leq x_k \leq 1, \sum_{k=1}^K x_k = 1\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;狄利克雷分布在&lt;span class=&quot;math inline&quot;&gt;\(S_K\)&lt;/span&gt;以外的地方概率为&lt;span class=&quot;math inline&quot;&gt;\(0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;贝叶斯概念学习&quot;&gt;贝叶斯概念学习&lt;/h2&gt;
&lt;p&gt;类比于小孩子学习理解单词的含义, 概念学习可以等价于二分类问题, 即学习一个函数&lt;span class=&quot;math inline&quot;&gt;\(f(x)\)&lt;/span&gt;, 如果&lt;span class=&quot;math inline&quot;&gt;\(x\)&lt;/span&gt;是概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;的一个实例, 则&lt;span class=&quot;math inline&quot;&gt;\(f(x)=1\)&lt;/span&gt;, 否则&lt;span class=&quot;math inline&quot;&gt;\(f(x)=0\)&lt;/span&gt;. 一般处理二分类问题时会从正例和反例中同时学习, 但本小节介绍的例子只从正例中学习.&lt;/p&gt;
&lt;h3 id=&quot;数字游戏&quot;&gt;数字游戏&lt;/h3&gt;
&lt;p&gt;首先我会选择一个范围在&lt;span class=&quot;math inline&quot;&gt;\([1,100]\)&lt;/span&gt;的整数代数概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;(不会告诉你), 比如质数, &lt;span class=&quot;math inline&quot;&gt;\(1\)&lt;/span&gt;到&lt;span class=&quot;math inline&quot;&gt;\(10\)&lt;/span&gt;之间的数等等. 然后我会告诉你数字&lt;span class=&quot;math inline&quot;&gt;\(\mathcal{D}=\{x_1,\dots,x_N\}\)&lt;/span&gt;是从概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;中随机挑选出来的. 比如假设我之前选的概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;是奇数, 那么我可能告诉你&lt;span class=&quot;math inline&quot;&gt;\(\{3,17,5,97\}\)&lt;/span&gt;这些数字. 然后接下来问你一些新的数字&lt;span class=&quot;math inline&quot;&gt;\(\tilde{x}\)&lt;/span&gt;是否属于概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;. 其实就是让你在给定一些正例的情况下猜测我之前设定的概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;是什么.&lt;/p&gt;
&lt;p&gt;比如告诉你 &lt;span class=&quot;math inline&quot;&gt;\(\{2,4,8,16,32\}\)&lt;/span&gt; 这些数字属于概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;, 那么问你&lt;span class=&quot;math inline&quot;&gt;\(62\)&lt;/span&gt;属于概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;吗? 我们基于这些数字可能会猜测概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;大概率是&lt;span class=&quot;math inline&quot;&gt;\(2^n\)&lt;/span&gt;, 所以判断&lt;span class=&quot;math inline&quot;&gt;\(62\)&lt;/span&gt;不属于概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;. 那么如果再告诉你&lt;span class=&quot;math inline&quot;&gt;\(30\)&lt;/span&gt;也属于概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;, 那么现在你觉得&lt;span class=&quot;math inline&quot;&gt;\(62\)&lt;/span&gt;属于概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;吗? 这时候我们就可能认为概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;是偶数吧, 然后判断&lt;span class=&quot;math inline&quot;&gt;\(62\)&lt;/span&gt;属于概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;怎么来解释和模拟上述行为呢? 一种方法是用假设空间(hypothesis space)和版本空间(version space). 最开始我们对概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;有一个假设空间&lt;span class=&quot;math inline&quot;&gt;\(\mathcal{H}\)&lt;/span&gt;, 比如质数, 奇数, &lt;span class=&quot;math inline&quot;&gt;\(10\)&lt;/span&gt;的倍数等等. 然后当我们看到一些正例时, 那些符合正例的概念构成的&lt;span class=&quot;math inline&quot;&gt;\(\mathcal{H}\)&lt;/span&gt;的子集就是版本空间. 随着我们看到的正例越多, 版本空间会越来越小(至少不会增大), 表示着我们离正确概念越来越近(如果假设空间包含正确概念的话). 以上面的例子为例, 当我们看到&lt;span class=&quot;math inline&quot;&gt;\(\{2,4,8,16,32,128\}\)&lt;/span&gt;时, 偶数, &lt;span class=&quot;math inline&quot;&gt;\(2^n\)&lt;/span&gt;等等这些概念都在版本空间中, 质数等概念不在版本空间中, 当我们又看到&lt;span class=&quot;math inline&quot;&gt;\(30\)&lt;/span&gt;时, 概念&lt;span class=&quot;math inline&quot;&gt;\(2^n\)&lt;/span&gt;也从版本空间中删除.&lt;/p&gt;
&lt;p&gt;另外一个问题是我们看到&lt;span class=&quot;math inline&quot;&gt;\(\{2,4,8,16,32,128\}\)&lt;/span&gt;时, 其实&lt;span class=&quot;math inline&quot;&gt;\(2^n\)&lt;/span&gt;和偶数都在版本空间中,那么我们为什么会倾向于&lt;span class=&quot;math inline&quot;&gt;\(2^n\)&lt;/span&gt;呢?&lt;/p&gt;
&lt;h3 id=&quot;似然&quot;&gt;似然&lt;/h3&gt;
&lt;p&gt;对于上小节最后一个问题, 直观想法是我们想避免 suspicious coincidence. 即如果概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;是偶数, 那么为什么那么巧我们看到的数都符合&lt;span class=&quot;math inline&quot;&gt;\(2^n\)&lt;/span&gt;. 下面从概率角度解释一下. 我们假定看到的数字是从概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;中均匀随机选取的. 我们引入似然 &lt;span class=&quot;math inline&quot;&gt;\(p(\mathcal{D}\vert h)\)&lt;/span&gt;, 则&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[p(\{2\}\vert h_{2^n}) = \frac{1}{6} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;因为在&lt;span class=&quot;math inline&quot;&gt;\([1,100]\)&lt;/span&gt;整数中只有&lt;span class=&quot;math inline&quot;&gt;\(6\)&lt;/span&gt;个整数满足&lt;span class=&quot;math inline&quot;&gt;\(2^n\)&lt;/span&gt;. 而&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[p(\{2\}\vert h_{even}) = \frac{1}{50}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;同理可以计算&lt;span class=&quot;math display&quot;&gt;\[p(\{2,4,8,16,32\}\vert h_{2^n}) \gg p(\{2,4,8,16,32\}\vert h_{even})\]&lt;/span&gt; 这就解释了我们为什么会倾向于&lt;span class=&quot;math inline&quot;&gt;\(2^n\)&lt;/span&gt;(这里默认了不同概念先验概率相等).&lt;/p&gt;
&lt;h3 id=&quot;先验&quot;&gt;先验&lt;/h3&gt;
&lt;p&gt;还是上面的例子, 看到&lt;span class=&quot;math inline&quot;&gt;\(\{2,4,8,16,32\}\)&lt;/span&gt;这些数, 概念“满足&lt;span class=&quot;math inline&quot;&gt;\(2^n\)&lt;/span&gt;且不是&lt;span class=&quot;math inline&quot;&gt;\(64\)&lt;/span&gt;”比&lt;span class=&quot;math inline&quot;&gt;\(2^n\)&lt;/span&gt;更符合数据, 但是前一个怪怪的不自然, 我们可以给这种概念赋予较小的先验概率, 即 &lt;span class=&quot;math display&quot;&gt;\[p(C=h_{2^n且不包含64}) &amp;lt; p(C=h_{2^n})\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;后验&quot;&gt;后验&lt;/h3&gt;
&lt;p&gt;后验概率正比于似然乘上先验, 即 &lt;span class=&quot;math display&quot;&gt;\[p(h\vert \mathcal{D}) \propto p(\mathcal{D}\vert h)p(h) \]&lt;/span&gt; 当数据量比较多时, 后验概率在某个概念上值比较大, 这个概念就是所谓的&lt;strong&gt;最大后验概率估计(MAP)&lt;/strong&gt;. 即 &lt;span class=&quot;math display&quot;&gt;\[\hat{h}^{MAP} = \arg\max_h \ \ p(h\vert \mathcal{D})\]&lt;/span&gt; 由贝叶斯公式,上式可以写成 &lt;span class=&quot;math display&quot;&gt;\[\hat{h}^{MAP} = \arg\max_h \ \ p(\mathcal{D}\vert h)p(h) = \arg\max_h \ \ [\log p(\mathcal{D}\vert h) + \log p(h)] \]&lt;/span&gt; 似然项会变(指数依赖样本大小&lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt;), 而先验项保持不变. 当数据量变大时, MAP趋向于&lt;strong&gt;最大似然估计(MLE)&lt;/strong&gt;. &lt;span class=&quot;math display&quot;&gt;\[\hat{h}^{MLE} = \arg\max_h \ \ p(\mathcal{D}\vert h) = \arg\max_h \ \ \log p(\mathcal{D}\vert h)\]&lt;/span&gt; 换句话说, 当我们有足够数据量时, 数据会忽略掉先验. MAP会收敛到MLE.&lt;/p&gt;
&lt;h3 id=&quot;后验预测分布&quot;&gt;后验预测分布&lt;/h3&gt;
&lt;p&gt;我们得到的后验概率分布是在观测到现有数据的条件下,假设空间中那些概念是实际概念的概率. 但是现在给我们一个新的数字&lt;span class=&quot;math inline&quot;&gt;\(\tilde{x}\)&lt;/span&gt;, 我们怎么决策它是否属于实际概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;呢? 这就是后验预测分布干的活. 这种情形下的后验预测分布为 &lt;span class=&quot;math display&quot;&gt;\[p(\tilde{x} \in C \vert  \mathcal{D}) = \sum_h p(y=1\vert \tilde{x},h) p(h\vert \mathcal{D})\]&lt;/span&gt; 本质上就是对假设空间中的概念的预测结果做一个加权平均, 权重就是概念对应的后验概率. 这个成为&lt;strong&gt;贝叶斯模型平均(Bayes model averaging)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;当数据量很小时,可能后验概率分布很广泛. 但是当数据量很多, 多到几乎“暴露”出真实概念时, 这个时候后验概率会集中在最大后验概率的那个概念上. 这时候后验预测分布就可以近似为 &lt;span class=&quot;math display&quot;&gt;\[p(\tilde{x} \in C \vert  \mathcal{D}) = p(\tilde{x}\vert \hat{h}^{MAP})\]&lt;/span&gt; 这个被称为预测分布的&lt;strong&gt;plug-in approximation&lt;/strong&gt;.&lt;/p&gt;
&lt;h4 id=&quot;bma-vs-plug-in-approximation&quot;&gt;BMA vs Plug-in Approximation&lt;/h4&gt;
&lt;p&gt;随着数据的增多&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BMA由宽到窄.&lt;/li&gt;
&lt;li&gt;Plug-in Approximation由窄到宽.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;啥意思呢? 就是比如我们上来只看到一个数据&lt;span class=&quot;math inline&quot;&gt;\(\{16\}\)&lt;/span&gt;, 这时候给你一个&lt;span class=&quot;math inline&quot;&gt;\(\tilde{x}=8\)&lt;/span&gt;问你属不属于概念&lt;span class=&quot;math inline&quot;&gt;\(C\)&lt;/span&gt;. BMA这时候一脸懵逼不敢妄下断言, 很多概念的后验概率都非零, 而且没有某个概念后验概率很大, 它的标准放得很宽,更有可能回答是. 但是Plug-in Approximation就不一样了, 它上来就从假设空间中挑一个最大化后验概率的那个概念(假设挑出的是&lt;span class=&quot;math inline&quot;&gt;\(4^n\)&lt;/span&gt;), 那么它的回答就是否. 但是如果接下来告诉我们&lt;span class=&quot;math inline&quot;&gt;\(\{2,4,64\}\)&lt;/span&gt;也是正例, 这时候&lt;span class=&quot;math inline&quot;&gt;\(2^n\)&lt;/span&gt;对应的后验概率可能大于其他概念, 那BMA会进行相应的调整, 收窄自己的标准, 而Plug-in Approximation也会调整(假设就调整成了&lt;span class=&quot;math inline&quot;&gt;\(2^n\)&lt;/span&gt;), 相比于&lt;span class=&quot;math inline&quot;&gt;\(4^n\)&lt;/span&gt;, 它是放宽了自己的标准. 虽然在数据量比较小时两种方法会有偏差, 但当数据量很大时, 两种方法收敛到相同的标准.&lt;/p&gt;
&lt;h2 id=&quot;the-beta-binomial-model&quot;&gt;The beta-binomial model&lt;/h2&gt;
&lt;p&gt;本小节讨论给定一系列观察到的掷硬币的结果, 预测下一次掷硬币出现正面的概率. 叙述方式与上一节相同 似然 -&amp;gt; 先验 -&amp;gt; 后验 -&amp;gt; 后验预测&lt;/p&gt;
&lt;h3 id=&quot;似然-1&quot;&gt;似然&lt;/h3&gt;
&lt;p&gt;假设 &lt;span class=&quot;math inline&quot;&gt;\(X_i \sim Ber(\theta)\)&lt;/span&gt;, 其中&lt;span class=&quot;math inline&quot;&gt;\(X_i=1\)&lt;/span&gt;表示正面, &lt;span class=&quot;math inline&quot;&gt;\(X_i=0\)&lt;/span&gt;表示反面, &lt;span class=&quot;math inline&quot;&gt;\(\theta \in [0,1]\)&lt;/span&gt;是参数(表示正面的概率). 如果数据 iid, 那么似然为 &lt;span class=&quot;math display&quot;&gt;\[p(\mathcal{D}\vert \theta) = \theta^{N_1}(1-\theta)^{N_0}\]&lt;/span&gt; 实际上是一个二项分布. &amp;gt; 注: 这里的每一个&lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;对应的伯努利分布就相当于上节中的假设空间中的一个概念&lt;span class=&quot;math inline&quot;&gt;\(h\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&quot;先验-1&quot;&gt;先验&lt;/h3&gt;
&lt;p&gt;我们要给&lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;一个先验分布, &lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;取值范围为&lt;span class=&quot;math inline&quot;&gt;\([0,1]\)&lt;/span&gt;, 表示我们在观察数据之前对&lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;的认识. 如果先验分布和上节的似然具有相同的形式, 那么数学上计算会很方便. 综上两点, Beta分布当仁不让. &lt;span class=&quot;math display&quot;&gt;\[Beta(\theta\vert a,b) \propto \theta^{a-1}(1-\theta)^{b-1}\]&lt;/span&gt; 此时后验分布具有和先验分布相同的形式. 同时先验中的&lt;span class=&quot;math inline&quot;&gt;\(a,b\)&lt;/span&gt;是超参数, 需要我们自己设定, &lt;span class=&quot;math inline&quot;&gt;\(a,b\)&lt;/span&gt;的取值融合了我们对&lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;的认识, 比如我们认为&lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;具有均值&lt;span class=&quot;math inline&quot;&gt;\(0.7\)&lt;/span&gt;, 方差&lt;span class=&quot;math inline&quot;&gt;\(0.2\)&lt;/span&gt;, 那么我们就可以设置为&lt;span class=&quot;math inline&quot;&gt;\(a=2.975,b=1.275\)&lt;/span&gt;. 或者我们对&lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;一无所知, 那我们可以设置为均匀分布&lt;span class=&quot;math inline&quot;&gt;\(a=b=1\)&lt;/span&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;共轭先验&lt;/strong&gt;: 如果先验分布和似然函数可以使得先验分布和后验分布有相同的形式，那么就称先验分布与似然函数是共轭的，共轭的结局是让先验与后验具有相同的形式. 再强调一遍, 共轭先验指的是对于似然来说这个先验是共轭先验.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;后验-1&quot;&gt;后验&lt;/h3&gt;
&lt;p&gt;后验概率分布为 &lt;span class=&quot;math display&quot;&gt;\[p(\theta\vert \mathcal{D}) \propto Bin(N_1\vert \theta, N_0+N_1)\ Beta(\theta\vert a,b) \propto Beta(\theta\vert N_1+a, N_0+b)\]&lt;/span&gt; 由Beta分布的性质(mode的表达式)可得 &lt;span class=&quot;math display&quot;&gt;\[\hat{\theta}_{MAP} = \frac{a+N_1-1}{a+b+N_0+N_1-2}\]&lt;/span&gt; 如果先验为均匀分布(&lt;span class=&quot;math inline&quot;&gt;\(a=b=1\)&lt;/span&gt;), 则上式退化为 &lt;span class=&quot;math display&quot;&gt;\[\hat{\theta}_{MLE} = \frac{N_1}{N_0+N_1}\]&lt;/span&gt; 后验概率分布的均值为 &lt;span class=&quot;math display&quot;&gt;\[\bar{\theta} = \frac{a+N_1}{a+b+N_0+N_1}\]&lt;/span&gt; 进一步可以证明, &lt;strong&gt;后验概率分布的均值是先验均分布的均值和最大似然估计的凸组合&lt;/strong&gt;. 即 &lt;span class=&quot;math display&quot;&gt;\[\mathbb{E}[\theta\vert \mathcal{D}] = \lambda \frac{a}{a+b} + (1-\lambda)\hat{\theta}_{MLE}\]&lt;/span&gt; 也就是说, 后验是我们观察数据之前相信的知识和数据想要告诉我们的知识的一个折中.&lt;/p&gt;
&lt;h3 id=&quot;后验预测分布-1&quot;&gt;后验预测分布&lt;/h3&gt;
&lt;p&gt;当我们有了后验概率分布&lt;span class=&quot;math inline&quot;&gt;\(Beta(c,d)\)&lt;/span&gt;后, 我们要预测下一次掷硬币是正面的概率. &lt;span class=&quot;math display&quot;&gt;\[p(\tilde{x}=1\vert \mathcal{D}) = \int_0^1 p(x=1\vert \theta)p(\theta\vert \mathcal{D})d\theta\]&lt;/span&gt; 即把&lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;当成随机变量看待, 对&lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;所有可能情况进行汇总. 带入 &lt;span class=&quot;math inline&quot;&gt;\(p(x=1\vert \theta)=\theta\)&lt;/span&gt;, &lt;span class=&quot;math inline&quot;&gt;\(p(\theta\vert \mathcal{D})=Beta(\theta\vert c,d)\)&lt;/span&gt;可得 &lt;span class=&quot;math display&quot;&gt;\[p(\tilde{x}=1\vert \mathcal{D}) = \int_0^1\theta Beta(\theta\vert c,d) d\theta = \mathbb{E}[\theta\vert \mathcal{D}] = \frac{c}{c+d}\]&lt;/span&gt; 即在这个例子中我们有 &lt;span class=&quot;math display&quot;&gt;\[p(\tilde{x}\vert \mathcal{D})=Ber(\tilde{x}\vert \mathbb{E}[\theta\vert \mathcal{D}])\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&quot;过拟合问题&quot;&gt;过拟合问题&lt;/h4&gt;
&lt;p&gt;如果我们在预测时不使用上面的BMA, 而是使用 &lt;span class=&quot;math display&quot;&gt;\[p(\tilde{x}\vert \mathcal{D})=Ber(\tilde{x}\vert \hat{\theta}_{MLE})\]&lt;/span&gt; 即 &lt;span class=&quot;math display&quot;&gt;\[p(\tilde{x}=1\vert \mathcal{D}) = \hat{\theta}_{MLE} = \frac{N_1}{N_0+N_1} \]&lt;/span&gt; 在数据集比较小时, 这非常容易过拟合, 比如样本集合为&lt;span class=&quot;math inline&quot;&gt;\(3\)&lt;/span&gt;次观测结果,且全是反面, 那么根据最大似然估计 &lt;span class=&quot;math display&quot;&gt;\[p(\tilde{x}=1\vert \mathcal{D}) = \hat{\theta}_{MLE} = \frac{N_1}{N_0+N_1} = 0 \]&lt;/span&gt; 而贝叶斯方法可以很好的克服这个问题, 比如我们假定先验分布为均匀分布&lt;span class=&quot;math inline&quot;&gt;\(a=b=1\)&lt;/span&gt;, 那么根据后验预测分布我们有 &lt;span class=&quot;math display&quot;&gt;\[p(\tilde{x}=1\vert \mathcal{D}) = \mathbb{E}[\theta\vert \mathcal{D}] = \frac{c}{c+d} = \frac{N_1+1}{N_0+N_1+2} = \frac{1}{5}\]&lt;/span&gt; 这正对应着&lt;strong&gt;加一平滑&lt;/strong&gt;这种技巧.&lt;/p&gt;
&lt;h2 id=&quot;the-dirichlet-multinomial-model&quot;&gt;The Dirichlet-multinomial model&lt;/h2&gt;
&lt;p&gt;本小节讨论给定一系列观察到的掷具有&lt;span class=&quot;math inline&quot;&gt;\(K\)&lt;/span&gt;面骰子的结果, 预测下一次掷骰子出现每一面的概率. 叙述方式与上一节相同 似然 -&amp;gt; 先验 -&amp;gt; 后验 -&amp;gt; 后验预测&lt;/p&gt;
&lt;h3 id=&quot;似然-2&quot;&gt;似然&lt;/h3&gt;
&lt;p&gt;假设数据 iid, 那么似然为 &lt;span class=&quot;math display&quot;&gt;\[p(\mathcal{D}\vert \theta) = \prod_{i=1}^K \theta_k^{N_k}\]&lt;/span&gt; 实际上是一个多项分布.&lt;/p&gt;
&lt;h3 id=&quot;先验-2&quot;&gt;先验&lt;/h3&gt;
&lt;p&gt;选取先验时我们主要考虑两点, 一是参数&lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;取值范围, 另一个是希望先验对于似然来说是共轭先验. Dirichlet分布满足这两个条件. &lt;span class=&quot;math display&quot;&gt;\[Dir(\theta\vert \alpha_1,\dots,\alpha_K) = \frac{1}{B(\alpha_1,\dots,\alpha_K)}\prod_{k=1}^K \theta_k^ {\alpha_k-1}\mathbb{I}(\theta \in S_K)\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;后验-2&quot;&gt;后验&lt;/h3&gt;
&lt;p&gt;后验概率分布依然是Dirichlet分布 &lt;span class=&quot;math display&quot;&gt;\[p(\theta\vert \mathcal{D}) \propto p(\mathcal{D}\vert \theta)p(\theta) = Dir(\theta \vert  \alpha_1+N_1,\dots, \alpha_K+N_K)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;后验概率分布的MAP估计是 &lt;span class=&quot;math display&quot;&gt;\[\hat{\theta}_k^{MAP} = \frac{N_k+\alpha_k-1}{N_1+\dots+N_k+\alpha_1+\dots+\alpha_K-K}\]&lt;/span&gt; 当先验分布退化为均匀分布时,MAP退化为MLE &lt;span class=&quot;math display&quot;&gt;\[\hat{\theta}_k^{MAP} = \frac{N_k}{N_1+\dots+N_k}\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;后验预测分布-2&quot;&gt;后验预测分布&lt;/h3&gt;
&lt;p&gt;预测下一次掷骰子出现每一面的概率 &lt;span class=&quot;math display&quot;&gt;\[
\begin{eqnarray}
p(X=j\vert \mathcal{D}) &amp;amp;=&amp;amp; \int p(X=j\vert \theta)p(\theta\vert \mathcal{D})d\theta \\
&amp;amp;=&amp;amp; \frac{\alpha_j+N_j}{\alpha_1+\dots+\alpha_K + N_1+\dots+N_K}
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;朴素贝叶斯&quot;&gt;朴素贝叶斯&lt;/h2&gt;
&lt;p&gt;朴素贝叶斯可以用来解决&lt;span class=&quot;math inline&quot;&gt;\(K\)&lt;/span&gt;分类问题. 问题定义: &lt;span class=&quot;math inline&quot;&gt;\(X \in \mathbb{R}^D\)&lt;/span&gt;, &lt;span class=&quot;math inline&quot;&gt;\(Y \in \{1, \dots, K\}\)&lt;/span&gt;. 给定数据集&lt;span class=&quot;math inline&quot;&gt;\(\mathcal{D}=\{(X_1,Y_1),\dots, (X_n,Y_n)\}\)&lt;/span&gt;, 给新的样本数据&lt;span class=&quot;math inline&quot;&gt;\(X\)&lt;/span&gt;分类,即求 &lt;span class=&quot;math display&quot;&gt;\[p(Y\vert X,\mathcal{D})\]&lt;/span&gt; 我们可以利用贝叶斯公式,分别求出&lt;span class=&quot;math inline&quot;&gt;\(p(X\vert Y,\mathcal{D})\)&lt;/span&gt;和&lt;span class=&quot;math inline&quot;&gt;\(p(Y)\)&lt;/span&gt;. 但是求解&lt;span class=&quot;math inline&quot;&gt;\(p(X\vert Y,\mathcal{D})\)&lt;/span&gt;复杂度太高, 比如假设&lt;span class=&quot;math inline&quot;&gt;\(X\)&lt;/span&gt;的每一维取值范围为离散值,且大小为&lt;span class=&quot;math inline&quot;&gt;\(S\)&lt;/span&gt;, 那么&lt;span class=&quot;math inline&quot;&gt;\(p(X\vert Y,\mathcal{D})\)&lt;/span&gt;的参数个数将达到&lt;span class=&quot;math inline&quot;&gt;\(KS^D\)&lt;/span&gt;. 为了解决这个问题,朴素贝叶斯做了一个很强的假设:在给定类别的条件下, 特征之间相互独立. 即 &lt;span class=&quot;math display&quot;&gt;\[p(X\vert Y=c) = \prod_{i=1}^D p(X^{(i)}\vert Y=c)\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;model-fitting&quot;&gt;Model fitting&lt;/h3&gt;
&lt;h4 id=&quot;最大似然估计&quot;&gt;最大似然估计&lt;/h4&gt;
&lt;p&gt;对于一个样本&lt;span class=&quot;math inline&quot;&gt;\((X_i,Y_i)\)&lt;/span&gt;, 似然函数为&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
p(X_i,Y_i\vert \vec{\theta}) &amp;amp;=&amp;amp; p(Y_i\vert \vec{\theta})p(X_i\vert Y_i,\vec{\theta}) \\
&amp;amp;=&amp;amp; p(Y_i\vert \vec{\theta}) \prod_{j=1}^{D} p(X_i^{(j)}\vert Y_i,\vec{\theta})
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&quot;math inline&quot;&gt;\(\vec{\theta} = \{\pi_1,\dots,\pi_K, \vec{\theta}_{11},\dots,\vec{\theta}_{DK}\}\)&lt;/span&gt;, 这里&lt;span class=&quot;math inline&quot;&gt;\(\pi_i\)&lt;/span&gt;是针对&lt;span class=&quot;math inline&quot;&gt;\(p(Y=i)\)&lt;/span&gt;的参数, &lt;span class=&quot;math inline&quot;&gt;\(\vec{\theta}_{jc}\)&lt;/span&gt;是针对&lt;span class=&quot;math inline&quot;&gt;\(p(X^{(j)}\vert Y=c)\)&lt;/span&gt;的参数. &lt;span class=&quot;math inline&quot;&gt;\(\vec{\theta}_{jc}\)&lt;/span&gt;取决于第&lt;span class=&quot;math inline&quot;&gt;\(j\)&lt;/span&gt;维特征选取什么样的分布. 对于数据集&lt;span class=&quot;math inline&quot;&gt;\(\mathcal{D}=\{(X_1,Y_1),\dots, (X_n,Y_n)\}\)&lt;/span&gt;, 可计算似然函数为 &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
p(\mathcal{D}\vert \theta) &amp;amp;=&amp;amp; \prod_{i=1}^n p(Y_i\vert \theta) \prod_{i=1}^n\prod_{j=1}^D  p(X_i^{(j)}\vert Y_i,\theta) \\
&amp;amp;=&amp;amp; \prod_{i=1}^n p(Y_i\vert \theta) \prod_{i=1}^n\prod_{j=1}^D  \prod_{c=1}^K p(X_i^{(j)}\vert \theta_{jc})^{\mathbb{I}(Y_i=c)}
\end{align}
\]&lt;/span&gt; 求对数可得 &lt;span class=&quot;math display&quot;&gt;\[\log p(\mathcal{D}\vert \theta) = \sum_{c=1}^K N_c\log \pi_c + \sum_{j=1}^{D}\sum_{c=1}^K\sum_{i: Y_i=c}p(X_i^{(j)}\vert \theta_{jc})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;在求解&lt;span class=&quot;math inline&quot;&gt;\(\vec{\pi}\)&lt;/span&gt;时, 需要带入约束&lt;span class=&quot;math inline&quot;&gt;\(\sum\limits_{c=1}^K\pi_c=1\)&lt;/span&gt;, 利用拉格朗日乘子法可得 &lt;span class=&quot;math display&quot;&gt;\[\pi_c^{MLE} = \frac{N_c}{N_1+\dots+N_K}\]&lt;/span&gt; 而&lt;span class=&quot;math inline&quot;&gt;\(\vec{\theta}_{jc}\)&lt;/span&gt;需要知道特征分布的具体形式才可以求.&lt;/p&gt;
&lt;h4 id=&quot;bayesian-naive-bayes&quot;&gt;Bayesian naive Bayes&lt;/h4&gt;
&lt;p&gt;最大似然估计有个缺点是过拟合, 比如上面对&lt;span class=&quot;math inline&quot;&gt;\(\pi_c\)&lt;/span&gt;的估计, 当&lt;span class=&quot;math inline&quot;&gt;\(N_c=0\)&lt;/span&gt;时, &lt;span class=&quot;math inline&quot;&gt;\(\pi_c^{MLE} = 0\)&lt;/span&gt;. 为了解决过拟合问题, Bayes方法采用合适的先验分布, 比如我们使用下面形式的先验分布 &lt;span class=&quot;math display&quot;&gt;\[p(\theta) = p(\pi)\prod_{j=1}^D\prod_{c=1}^Kp(\theta_{jc})\]&lt;/span&gt; 其中&lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;使用Dirichlet分布, 如果每个特征是伯努利分布, 那我们就取&lt;span class=&quot;math inline&quot;&gt;\(\theta_{jc}\)&lt;/span&gt;为Beta分布. 那么后验分布为 &lt;span class=&quot;math display&quot;&gt;\[p(\theta\vert \mathcal{D}) = p(\pi\vert \mathcal{D})\prod_{j=1}^D\prod_{c=1}^Kp(\theta_{jc}\vert \mathcal{D})\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&quot;using-the-model-for-prediction&quot;&gt;Using the model for prediction&lt;/h4&gt;
&lt;p&gt;最后的目标是用模型进行预测, 即计算 &lt;span class=&quot;math display&quot;&gt;\[p(y=c\vert x,\mathcal{D}) \propto p(y=c\vert \mathcal{D})\prod_{j=1}^Dp(x_j\vert y=c_j,\mathcal{D})\]&lt;/span&gt; 按照Bayes的方法, 我们需要计算 &lt;span class=&quot;math display&quot;&gt;\[p(y=c\vert x,\mathcal{D}) \propto [\int Cat(y=c\vert \pi) p(\pi\vert \mathcal{D})d\pi] \prod_{j=1}^D\int Ber(x_j\vert y=c,\vec{\theta_{jc}})p(\vec{\theta_{jc}}\vert \mathcal{D})d\vec{\theta_{jc}}\]&lt;/span&gt;&lt;/p&gt;</content><author><name>  黄锡昆</name></author><category term="技术" /><category term="MLAPP" /><category term="机器学习" /><summary type="html">简介 先来说一下生成式模型(generative models) 和 判别式模型(discriminative models). 对于分类问题, 我们的目标是基于有限的训练样本集尽可能准确地估计出后验概率\(p(c\vert x)\), 而估计该后验概率有两种方式: 判别式模型: 直接建模\(p(c \vert x)\). 生成式模型: 先对联合分布\(p(x,c)\)建模, 再由此得到\(p(c \vert x)\). 本章着眼于生成式模型, 由贝叶斯定理可得 \[p(c\vert x)=\frac{p(x\vert c)p(c)}{p(x)}\] 所以问题关键就在于求得 类条件概率分布(class-conditional density) \(p(x\vert c)\) 先验分布 \(p(c)\) 概率分布 继续进行之前, 这里先罗列一下本章涉及的一些概率分布. 伯努利分布和二项分布 可用来建模掷硬币的结果 伯努利分布 \[X \sim Ber(\theta)\] \[Ber(x\vert \theta) = \theta^{\mathbb{I}(x=1)}(1-\theta)^{\mathbb{I}(x=0)}\] 即随机变量\(X \in \{0,1\}\), 且 \[p(X=1) = \theta\] \[p(X=0) = 1 - \theta\] 二项分布 \[X \sim Bin(n,\theta)\] \[Bin(k\vert n,\theta) = \binom{n}{k} \theta^k (1-\theta)^{n-k} \] 即随机变量\(X \in \{0,\dots, n\}\), 且 \[p(X=k) = \binom{n}{k} \theta^k (1-\theta)^{n-k}\] 多伯努利分布和多项分布 可用来建模掷具有\(K\)面的骰子的结果 多伯努利分布 \[X \sim Cat(\theta)\] \[Cat(x\vert \theta) = \prod_{j=1}^K \theta_j^{\mathbb{I}(x_j=1)}\] \(X\)取值为\(K\)维one-hot编码. 多项分布 \[X \sim Mu(n, \theta)\] \[Mu(x\vert n,\theta) = \binom{n}{x_1 \dots x_K} \prod_{j=1}^K \theta_j^k\] \(X\)是K维向量, 且满足 \[x_k\in\{0,\dots,n\}\] \[\sum_{k=1}^Kx_k = n\] 贝塔分布和狄利克雷分布 贝塔分布 \[X \sim Beta(a,b)\] \[Beta(x\vert a,b) = \frac{1}{B(a,b)}x^{a-1}(1-x)^{b-1}\] \(X\)取值范围是\([0,1]\). 要求\(a,b&amp;gt;0\). 当\(a=b=1\)时退化为\([0,1]\)上的均匀分布. 狄利克雷分布 贝塔分布的多元扩展, 相当于考虑的是多个随机变量的联合分布. \[X \sim Dir(\alpha_1,\dots, \alpha_k)\] \[Dir(x\vert \alpha_1,\dots,\alpha_k) = \frac{1}{B(\alpha_1,\dots,\alpha_k)}\prod_{k=1}^K x_k^{\alpha_k - 1} \mathbb{I}(x\in S_K)\] 其中\[ S_K = \{x:0\leq x_k \leq 1, \sum_{k=1}^K x_k = 1\}\] 狄利克雷分布在\(S_K\)以外的地方概率为\(0\). 贝叶斯概念学习 类比于小孩子学习理解单词的含义, 概念学习可以等价于二分类问题, 即学习一个函数\(f(x)\), 如果\(x\)是概念\(C\)的一个实例, 则\(f(x)=1\), 否则\(f(x)=0\). 一般处理二分类问题时会从正例和反例中同时学习, 但本小节介绍的例子只从正例中学习. 数字游戏 首先我会选择一个范围在\([1,100]\)的整数代数概念\(C\)(不会告诉你), 比如质数, \(1\)到\(10\)之间的数等等. 然后我会告诉你数字\(\mathcal{D}=\{x_1,\dots,x_N\}\)是从概念\(C\)中随机挑选出来的. 比如假设我之前选的概念\(C\)是奇数, 那么我可能告诉你\(\{3,17,5,97\}\)这些数字. 然后接下来问你一些新的数字\(\tilde{x}\)是否属于概念\(C\). 其实就是让你在给定一些正例的情况下猜测我之前设定的概念\(C\)是什么. 比如告诉你 \(\{2,4,8,16,32\}\) 这些数字属于概念\(C\), 那么问你\(62\)属于概念\(C\)吗? 我们基于这些数字可能会猜测概念\(C\)大概率是\(2^n\), 所以判断\(62\)不属于概念\(C\). 那么如果再告诉你\(30\)也属于概念\(C\), 那么现在你觉得\(62\)属于概念\(C\)吗? 这时候我们就可能认为概念\(C\)是偶数吧, 然后判断\(62\)属于概念\(C\). 怎么来解释和模拟上述行为呢? 一种方法是用假设空间(hypothesis space)和版本空间(version space). 最开始我们对概念\(C\)有一个假设空间\(\mathcal{H}\), 比如质数, 奇数, \(10\)的倍数等等. 然后当我们看到一些正例时, 那些符合正例的概念构成的\(\mathcal{H}\)的子集就是版本空间. 随着我们看到的正例越多, 版本空间会越来越小(至少不会增大), 表示着我们离正确概念越来越近(如果假设空间包含正确概念的话). 以上面的例子为例, 当我们看到\(\{2,4,8,16,32,128\}\)时, 偶数, \(2^n\)等等这些概念都在版本空间中, 质数等概念不在版本空间中, 当我们又看到\(30\)时, 概念\(2^n\)也从版本空间中删除. 另外一个问题是我们看到\(\{2,4,8,16,32,128\}\)时, 其实\(2^n\)和偶数都在版本空间中,那么我们为什么会倾向于\(2^n\)呢? 似然 对于上小节最后一个问题, 直观想法是我们想避免 suspicious coincidence. 即如果概念\(C\)是偶数, 那么为什么那么巧我们看到的数都符合\(2^n\). 下面从概率角度解释一下. 我们假定看到的数字是从概念\(C\)中均匀随机选取的. 我们引入似然 \(p(\mathcal{D}\vert h)\), 则 \[p(\{2\}\vert h_{2^n}) = \frac{1}{6} \] 因为在\([1,100]\)整数中只有\(6\)个整数满足\(2^n\). 而 \[p(\{2\}\vert h_{even}) = \frac{1}{50}\] 同理可以计算\[p(\{2,4,8,16,32\}\vert h_{2^n}) \gg p(\{2,4,8,16,32\}\vert h_{even})\] 这就解释了我们为什么会倾向于\(2^n\)(这里默认了不同概念先验概率相等). 先验 还是上面的例子, 看到\(\{2,4,8,16,32\}\)这些数, 概念“满足\(2^n\)且不是\(64\)”比\(2^n\)更符合数据, 但是前一个怪怪的不自然, 我们可以给这种概念赋予较小的先验概率, 即 \[p(C=h_{2^n且不包含64}) &amp;lt; p(C=h_{2^n})\] 后验 后验概率正比于似然乘上先验, 即 \[p(h\vert \mathcal{D}) \propto p(\mathcal{D}\vert h)p(h) \] 当数据量比较多时, 后验概率在某个概念上值比较大, 这个概念就是所谓的最大后验概率估计(MAP). 即 \[\hat{h}^{MAP} = \arg\max_h \ \ p(h\vert \mathcal{D})\] 由贝叶斯公式,上式可以写成 \[\hat{h}^{MAP} = \arg\max_h \ \ p(\mathcal{D}\vert h)p(h) = \arg\max_h \ \ [\log p(\mathcal{D}\vert h) + \log p(h)] \] 似然项会变(指数依赖样本大小\(N\)), 而先验项保持不变. 当数据量变大时, MAP趋向于最大似然估计(MLE). \[\hat{h}^{MLE} = \arg\max_h \ \ p(\mathcal{D}\vert h) = \arg\max_h \ \ \log p(\mathcal{D}\vert h)\] 换句话说, 当我们有足够数据量时, 数据会忽略掉先验. MAP会收敛到MLE. 后验预测分布 我们得到的后验概率分布是在观测到现有数据的条件下,假设空间中那些概念是实际概念的概率. 但是现在给我们一个新的数字\(\tilde{x}\), 我们怎么决策它是否属于实际概念\(C\)呢? 这就是后验预测分布干的活. 这种情形下的后验预测分布为 \[p(\tilde{x} \in C \vert \mathcal{D}) = \sum_h p(y=1\vert \tilde{x},h) p(h\vert \mathcal{D})\] 本质上就是对假设空间中的概念的预测结果做一个加权平均, 权重就是概念对应的后验概率. 这个成为贝叶斯模型平均(Bayes model averaging). 当数据量很小时,可能后验概率分布很广泛. 但是当数据量很多, 多到几乎“暴露”出真实概念时, 这个时候后验概率会集中在最大后验概率的那个概念上. 这时候后验预测分布就可以近似为 \[p(\tilde{x} \in C \vert \mathcal{D}) = p(\tilde{x}\vert \hat{h}^{MAP})\] 这个被称为预测分布的plug-in approximation. BMA vs Plug-in Approximation 随着数据的增多 BMA由宽到窄. Plug-in Approximation由窄到宽. 啥意思呢? 就是比如我们上来只看到一个数据\(\{16\}\), 这时候给你一个\(\tilde{x}=8\)问你属不属于概念\(C\). BMA这时候一脸懵逼不敢妄下断言, 很多概念的后验概率都非零, 而且没有某个概念后验概率很大, 它的标准放得很宽,更有可能回答是. 但是Plug-in Approximation就不一样了, 它上来就从假设空间中挑一个最大化后验概率的那个概念(假设挑出的是\(4^n\)), 那么它的回答就是否. 但是如果接下来告诉我们\(\{2,4,64\}\)也是正例, 这时候\(2^n\)对应的后验概率可能大于其他概念, 那BMA会进行相应的调整, 收窄自己的标准, 而Plug-in Approximation也会调整(假设就调整成了\(2^n\)), 相比于\(4^n\), 它是放宽了自己的标准. 虽然在数据量比较小时两种方法会有偏差, 但当数据量很大时, 两种方法收敛到相同的标准. The beta-binomial model 本小节讨论给定一系列观察到的掷硬币的结果, 预测下一次掷硬币出现正面的概率. 叙述方式与上一节相同 似然 -&amp;gt; 先验 -&amp;gt; 后验 -&amp;gt; 后验预测 似然 假设 \(X_i \sim Ber(\theta)\), 其中\(X_i=1\)表示正面, \(X_i=0\)表示反面, \(\theta \in [0,1]\)是参数(表示正面的概率). 如果数据 iid, 那么似然为 \[p(\mathcal{D}\vert \theta) = \theta^{N_1}(1-\theta)^{N_0}\] 实际上是一个二项分布. &amp;gt; 注: 这里的每一个\(\theta\)对应的伯努利分布就相当于上节中的假设空间中的一个概念\(h\). 先验 我们要给\(\theta\)一个先验分布, \(\theta\)取值范围为\([0,1]\), 表示我们在观察数据之前对\(\theta\)的认识. 如果先验分布和上节的似然具有相同的形式, 那么数学上计算会很方便. 综上两点, Beta分布当仁不让. \[Beta(\theta\vert a,b) \propto \theta^{a-1}(1-\theta)^{b-1}\] 此时后验分布具有和先验分布相同的形式. 同时先验中的\(a,b\)是超参数, 需要我们自己设定, \(a,b\)的取值融合了我们对\(\theta\)的认识, 比如我们认为\(\theta\)具有均值\(0.7\), 方差\(0.2\), 那么我们就可以设置为\(a=2.975,b=1.275\). 或者我们对\(\theta\)一无所知, 那我们可以设置为均匀分布\(a=b=1\). 共轭先验: 如果先验分布和似然函数可以使得先验分布和后验分布有相同的形式，那么就称先验分布与似然函数是共轭的，共轭的结局是让先验与后验具有相同的形式. 再强调一遍, 共轭先验指的是对于似然来说这个先验是共轭先验. 后验 后验概率分布为 \[p(\theta\vert \mathcal{D}) \propto Bin(N_1\vert \theta, N_0+N_1)\ Beta(\theta\vert a,b) \propto Beta(\theta\vert N_1+a, N_0+b)\] 由Beta分布的性质(mode的表达式)可得 \[\hat{\theta}_{MAP} = \frac{a+N_1-1}{a+b+N_0+N_1-2}\] 如果先验为均匀分布(\(a=b=1\)), 则上式退化为 \[\hat{\theta}_{MLE} = \frac{N_1}{N_0+N_1}\] 后验概率分布的均值为 \[\bar{\theta} = \frac{a+N_1}{a+b+N_0+N_1}\] 进一步可以证明, 后验概率分布的均值是先验均分布的均值和最大似然估计的凸组合. 即 \[\mathbb{E}[\theta\vert \mathcal{D}] = \lambda \frac{a}{a+b} + (1-\lambda)\hat{\theta}_{MLE}\] 也就是说, 后验是我们观察数据之前相信的知识和数据想要告诉我们的知识的一个折中. 后验预测分布 当我们有了后验概率分布\(Beta(c,d)\)后, 我们要预测下一次掷硬币是正面的概率. \[p(\tilde{x}=1\vert \mathcal{D}) = \int_0^1 p(x=1\vert \theta)p(\theta\vert \mathcal{D})d\theta\] 即把\(\theta\)当成随机变量看待, 对\(\theta\)所有可能情况进行汇总. 带入 \(p(x=1\vert \theta)=\theta\), \(p(\theta\vert \mathcal{D})=Beta(\theta\vert c,d)\)可得 \[p(\tilde{x}=1\vert \mathcal{D}) = \int_0^1\theta Beta(\theta\vert c,d) d\theta = \mathbb{E}[\theta\vert \mathcal{D}] = \frac{c}{c+d}\] 即在这个例子中我们有 \[p(\tilde{x}\vert \mathcal{D})=Ber(\tilde{x}\vert \mathbb{E}[\theta\vert \mathcal{D}])\] 过拟合问题 如果我们在预测时不使用上面的BMA, 而是使用 \[p(\tilde{x}\vert \mathcal{D})=Ber(\tilde{x}\vert \hat{\theta}_{MLE})\] 即 \[p(\tilde{x}=1\vert \mathcal{D}) = \hat{\theta}_{MLE} = \frac{N_1}{N_0+N_1} \] 在数据集比较小时, 这非常容易过拟合, 比如样本集合为\(3\)次观测结果,且全是反面, 那么根据最大似然估计 \[p(\tilde{x}=1\vert \mathcal{D}) = \hat{\theta}_{MLE} = \frac{N_1}{N_0+N_1} = 0 \] 而贝叶斯方法可以很好的克服这个问题, 比如我们假定先验分布为均匀分布\(a=b=1\), 那么根据后验预测分布我们有 \[p(\tilde{x}=1\vert \mathcal{D}) = \mathbb{E}[\theta\vert \mathcal{D}] = \frac{c}{c+d} = \frac{N_1+1}{N_0+N_1+2} = \frac{1}{5}\] 这正对应着加一平滑这种技巧. The Dirichlet-multinomial model 本小节讨论给定一系列观察到的掷具有\(K\)面骰子的结果, 预测下一次掷骰子出现每一面的概率. 叙述方式与上一节相同 似然 -&amp;gt; 先验 -&amp;gt; 后验 -&amp;gt; 后验预测 似然 假设数据 iid, 那么似然为 \[p(\mathcal{D}\vert \theta) = \prod_{i=1}^K \theta_k^{N_k}\] 实际上是一个多项分布. 先验 选取先验时我们主要考虑两点, 一是参数\(\theta\)取值范围, 另一个是希望先验对于似然来说是共轭先验. Dirichlet分布满足这两个条件. \[Dir(\theta\vert \alpha_1,\dots,\alpha_K) = \frac{1}{B(\alpha_1,\dots,\alpha_K)}\prod_{k=1}^K \theta_k^ {\alpha_k-1}\mathbb{I}(\theta \in S_K)\] 后验 后验概率分布依然是Dirichlet分布 \[p(\theta\vert \mathcal{D}) \propto p(\mathcal{D}\vert \theta)p(\theta) = Dir(\theta \vert \alpha_1+N_1,\dots, \alpha_K+N_K)\] 后验概率分布的MAP估计是 \[\hat{\theta}_k^{MAP} = \frac{N_k+\alpha_k-1}{N_1+\dots+N_k+\alpha_1+\dots+\alpha_K-K}\] 当先验分布退化为均匀分布时,MAP退化为MLE \[\hat{\theta}_k^{MAP} = \frac{N_k}{N_1+\dots+N_k}\] 后验预测分布 预测下一次掷骰子出现每一面的概率 \[ \begin{eqnarray} p(X=j\vert \mathcal{D}) &amp;amp;=&amp;amp; \int p(X=j\vert \theta)p(\theta\vert \mathcal{D})d\theta \\ &amp;amp;=&amp;amp; \frac{\alpha_j+N_j}{\alpha_1+\dots+\alpha_K + N_1+\dots+N_K} \end{eqnarray} \] 朴素贝叶斯 朴素贝叶斯可以用来解决\(K\)分类问题. 问题定义: \(X \in \mathbb{R}^D\), \(Y \in \{1, \dots, K\}\). 给定数据集\(\mathcal{D}=\{(X_1,Y_1),\dots, (X_n,Y_n)\}\), 给新的样本数据\(X\)分类,即求 \[p(Y\vert X,\mathcal{D})\] 我们可以利用贝叶斯公式,分别求出\(p(X\vert Y,\mathcal{D})\)和\(p(Y)\). 但是求解\(p(X\vert Y,\mathcal{D})\)复杂度太高, 比如假设\(X\)的每一维取值范围为离散值,且大小为\(S\), 那么\(p(X\vert Y,\mathcal{D})\)的参数个数将达到\(KS^D\). 为了解决这个问题,朴素贝叶斯做了一个很强的假设:在给定类别的条件下, 特征之间相互独立. 即 \[p(X\vert Y=c) = \prod_{i=1}^D p(X^{(i)}\vert Y=c)\] Model fitting 最大似然估计 对于一个样本\((X_i,Y_i)\), 似然函数为 \[ \begin{align} p(X_i,Y_i\vert \vec{\theta}) &amp;amp;=&amp;amp; p(Y_i\vert \vec{\theta})p(X_i\vert Y_i,\vec{\theta}) \\ &amp;amp;=&amp;amp; p(Y_i\vert \vec{\theta}) \prod_{j=1}^{D} p(X_i^{(j)}\vert Y_i,\vec{\theta}) \end{align} \] 其中\(\vec{\theta} = \{\pi_1,\dots,\pi_K, \vec{\theta}_{11},\dots,\vec{\theta}_{DK}\}\), 这里\(\pi_i\)是针对\(p(Y=i)\)的参数, \(\vec{\theta}_{jc}\)是针对\(p(X^{(j)}\vert Y=c)\)的参数. \(\vec{\theta}_{jc}\)取决于第\(j\)维特征选取什么样的分布. 对于数据集\(\mathcal{D}=\{(X_1,Y_1),\dots, (X_n,Y_n)\}\), 可计算似然函数为 \[ \begin{align} p(\mathcal{D}\vert \theta) &amp;amp;=&amp;amp; \prod_{i=1}^n p(Y_i\vert \theta) \prod_{i=1}^n\prod_{j=1}^D p(X_i^{(j)}\vert Y_i,\theta) \\ &amp;amp;=&amp;amp; \prod_{i=1}^n p(Y_i\vert \theta) \prod_{i=1}^n\prod_{j=1}^D \prod_{c=1}^K p(X_i^{(j)}\vert \theta_{jc})^{\mathbb{I}(Y_i=c)} \end{align} \] 求对数可得 \[\log p(\mathcal{D}\vert \theta) = \sum_{c=1}^K N_c\log \pi_c + \sum_{j=1}^{D}\sum_{c=1}^K\sum_{i: Y_i=c}p(X_i^{(j)}\vert \theta_{jc})\] 在求解\(\vec{\pi}\)时, 需要带入约束\(\sum\limits_{c=1}^K\pi_c=1\), 利用拉格朗日乘子法可得 \[\pi_c^{MLE} = \frac{N_c}{N_1+\dots+N_K}\] 而\(\vec{\theta}_{jc}\)需要知道特征分布的具体形式才可以求. Bayesian naive Bayes 最大似然估计有个缺点是过拟合, 比如上面对\(\pi_c\)的估计, 当\(N_c=0\)时, \(\pi_c^{MLE} = 0\). 为了解决过拟合问题, Bayes方法采用合适的先验分布, 比如我们使用下面形式的先验分布 \[p(\theta) = p(\pi)\prod_{j=1}^D\prod_{c=1}^Kp(\theta_{jc})\] 其中\(\pi\)使用Dirichlet分布, 如果每个特征是伯努利分布, 那我们就取\(\theta_{jc}\)为Beta分布. 那么后验分布为 \[p(\theta\vert \mathcal{D}) = p(\pi\vert \mathcal{D})\prod_{j=1}^D\prod_{c=1}^Kp(\theta_{jc}\vert \mathcal{D})\] Using the model for prediction 最后的目标是用模型进行预测, 即计算 \[p(y=c\vert x,\mathcal{D}) \propto p(y=c\vert \mathcal{D})\prod_{j=1}^Dp(x_j\vert y=c_j,\mathcal{D})\] 按照Bayes的方法, 我们需要计算 \[p(y=c\vert x,\mathcal{D}) \propto [\int Cat(y=c\vert \pi) p(\pi\vert \mathcal{D})d\pi] \prod_{j=1}^D\int Ber(x_j\vert y=c,\vec{\theta_{jc}})p(\vec{\theta_{jc}}\vert \mathcal{D})d\vec{\theta_{jc}}\]</summary></entry><entry><title type="html">MLAPP笔记-概率</title><link href="https://xikunhuang.github.io/%E6%8A%80%E6%9C%AF/MLAPP%E7%AC%94%E8%AE%B0-%E6%A6%82%E7%8E%87/" rel="alternate" type="text/html" title="MLAPP笔记-概率" /><published>2018-11-27T00:00:00+00:00</published><updated>2018-11-27T00:00:00+00:00</updated><id>https://xikunhuang.github.io/%E6%8A%80%E6%9C%AF/MLAPP%E7%AC%94%E8%AE%B0-%E6%A6%82%E7%8E%87</id><content type="html" xml:base="https://xikunhuang.github.io/%E6%8A%80%E6%9C%AF/MLAPP%E7%AC%94%E8%AE%B0-%E6%A6%82%E7%8E%87/">&lt;h2 id=&quot;简介&quot;&gt;简介&lt;/h2&gt;
&lt;p&gt;概率两大派:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;频率学派(frequentist)&lt;/li&gt;
&lt;li&gt;贝叶斯学派(Bayesian)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;频率学派将概率解释为事件在多次实验下发生的频率(long run frequencies of events). 贝叶斯学派用概率来量化我们对一些事情的不确定性, 因此概率本质上与信息有关, 而与实验次数无关.&lt;/p&gt;
&lt;p&gt;举个例子, 对于“硬币正面朝上的概率为0.5”这句表述, 频率学派是在说如果我们掷硬币很多次, 那么大概有一半的次数硬币朝上. 而贝叶斯学派是在说我们相信下一次掷硬币出现正面和反面的可能性相同.&lt;/p&gt;
&lt;h2 id=&quot;概率论&quot;&gt;概率论&lt;/h2&gt;
&lt;p&gt;本小节简单介绍了一些概率论的基本概念和公式, 这里不多赘述. 只记录一些看了有些启发的.&lt;/p&gt;
&lt;h3 id=&quot;贝叶斯公式&quot;&gt;贝叶斯公式&lt;/h3&gt;
&lt;p&gt;贝叶斯公式给出的结果往往反“直觉”, 当然这里的“直觉”是错的. 书中正文和习题中给了三个例子.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;医疗诊断 &amp;gt; 假设如果人类患有某种疾病y, 那么在检查时指标x为阳性的概率为0.8. 现在如果有个人检测到指标x为阳性, 那么请问: 这个人患有疾病y的概率是多少?&lt;/p&gt;
&lt;p&gt;脱口而出0.8? 正确答案是不知道, 因为给的信息不足以做出判断. 我们再知道两个信息就可以做出判断: 人群中该疾病患病率是多少? 一个没患病的人检测到指标x为阳性的概率是多少? 这里我们假设 &lt;span class=&quot;math inline&quot;&gt;\(p(y=1)=0.004, p(x=1\vert y=0)=0.1\)&lt;/span&gt;, 那么由此可以计算出&lt;span class=&quot;math inline&quot;&gt;\(p(y=1\vert x=1)=0.031\)&lt;/span&gt;. 换句话说, 即使指标x被检测出阳性, 这个人也只有大约3%的概率真正患有疾病y!&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;公诉人与辩护人谬论 &amp;gt; 假设有一起案件, 案发现场发现了凶手的血迹, 经检查该血型是一种及其罕见的类型(比如说人群中只有1%的人拥有这种血型). 那么现在警察抓住了一个嫌疑人, 并且嫌疑人的血型恰好就是这种类型, 那么请问: 在这种情况下, 这个嫌疑人是真正凶手的概率是多少?&lt;/p&gt;
&lt;p&gt;这种情况下我们会大概率认为这个嫌疑人就是凶手了. 其实不然, 还是信息不够, 不足以做出判断.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The Monty Hall problem &amp;gt; 一共三个箱子编号1,2,3. 其中只有一个箱子里面有奖品, 而且主持人知道哪个箱子有礼品. 现在让你猜奖品在哪个箱子里. 假设你选择了1号箱子, 那么主持人接下来打开2号或3号箱子中的一个, 而且主持人会保证不会打开有奖品的箱子(因为如果打开了有奖品的箱子游戏就没法进行下去了…主持人知道哪个箱子有奖品所以可以保证这一点). 现在主持人问你要不要更换自己的选择, 是继续坚持1号箱子,还是换成另一个箱子呢?&lt;/p&gt;
&lt;p&gt;不要犹豫,换! 不换的话赢的概率是1/3,换的话赢的概率是2/3.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;独立性&quot;&gt;独立性&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;无条件独立 &lt;span class=&quot;math display&quot;&gt;\[X \perp Y \iff p(X,Y)=p(X)p(Y)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;条件独立 &lt;span class=&quot;math display&quot;&gt;\[X \perp Y \vert  Z \iff p(X,Y\vert Z)=p(X\vert Z)p(Y\vert Z)\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[X \perp Y \vert  Z \iff 存在函数g和h使得 p(x,y\vert z)=g(x,z)h(y,z)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;两两独立(Pairwise Independent) 和 相互独立(Mutually Indepedent) 相互独立: &lt;span class=&quot;math display&quot;&gt;\[p(X_i\vert X_S)=p(X_i), \forall S\subseteq\{1,\dots,n\}\setminus\{i\}\]&lt;/span&gt; 两两独立推不出相互独立. 反例: X1和X2都是等概率取值{0,1}的随机变量,X3=XOR(X1,X2). 则X1,X2,X3两两独立但是不是相互独立.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;不相关推不出独立. 所以相关系数不能作为衡量独立性的指标, 更好的指标是后面介绍的互信息. 反例: &lt;span class=&quot;math inline&quot;&gt;\(X\sim U(-1,1), Y=X^2\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;概率分布&quot;&gt;概率分布&lt;/h2&gt;
&lt;p&gt;书中介绍了一堆常见的离散型和连续型概率分布, 可是记不住啊…还是要应用…&lt;/p&gt;
&lt;h2 id=&quot;随机变量的变换&quot;&gt;随机变量的变换&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;change of variables formula 对这个公式一直理解不到位…&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;蒙特卡洛近似&quot;&gt;蒙特卡洛近似&lt;/h2&gt;
&lt;p&gt;通常根据随机变量变换公式来计算随机变量函数的分布是很困难的. 一个简单有效的方法是先从分布&lt;span class=&quot;math inline&quot;&gt;\(X\)&lt;/span&gt;中采样&lt;span class=&quot;math inline&quot;&gt;\(S\)&lt;/span&gt;个样本点&lt;span class=&quot;math inline&quot;&gt;\(x_1,\dots,x_S\)&lt;/span&gt;, 然后用&lt;span class=&quot;math inline&quot;&gt;\(\{f(x_s\}^S_{s=1}\)&lt;/span&gt;的实际分布来近似&lt;span class=&quot;math inline&quot;&gt;\(f(X)\)&lt;/span&gt;的分布. 这种方法就是蒙塔卡洛近似.&lt;/p&gt;
&lt;p&gt;进一步可以搞出蒙特卡洛积分.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[E[f(X)]=\int f(x)p(x)dx \approx \frac{1}{S}\sum_{s=1}^Sf(x_s)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&quot;math inline&quot;&gt;\(x_s \sim p(X)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;蒙特卡洛近似的精度随着样本规模的变大而增大.&lt;/p&gt;
&lt;h2 id=&quot;信息论&quot;&gt;信息论&lt;/h2&gt;
&lt;h3 id=&quot;熵entropy&quot;&gt;熵(entropy)&lt;/h3&gt;
&lt;p&gt;含义: 度量随机变量的不确定性. &lt;span class=&quot;math display&quot;&gt;\[\mathbb{H}(X) \triangleq -\sum\limits_{k=1}^Kp(X=k)\log_2 p(X=k)\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;kl散度kl-divergence&quot;&gt;KL散度(KL divergence)&lt;/h3&gt;
&lt;p&gt;含义: 度量两个随机变量的差异程度. &lt;span class=&quot;math display&quot;&gt;\[\mathbb{KL}(p\vert \vert q) \triangleq \sum\limits_{k=1}^K p_k \log \frac{p_k}{q_k}\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;交叉熵cross-entropy&quot;&gt;交叉熵(cross entropy)&lt;/h3&gt;
&lt;p&gt;含义: 当数据真实分布是&lt;span class=&quot;math inline&quot;&gt;\(p\)&lt;/span&gt;, 而我们用分布&lt;span class=&quot;math inline&quot;&gt;\(q\)&lt;/span&gt;编码数据时需要的平均位数. &lt;span class=&quot;math display&quot;&gt;\[\mathbb{H}(p,q) \triangleq -\sum\limits_k p_k\log q_k\]&lt;/span&gt; 注意到&lt;span class=&quot;math inline&quot;&gt;\(\mathbb{H}(p,p)=\mathbb{H}(p)\)&lt;/span&gt;, KL散度可以写成 &lt;span class=&quot;math display&quot;&gt;\[\mathbb{KL}(p\vert \vert q) = \mathbb{H}(p,q) - \mathbb{H}(p,p)\]&lt;/span&gt; 也就是说KL散度的含义是由于我们用分布&lt;span class=&quot;math inline&quot;&gt;\(q\)&lt;/span&gt;而不是数据的真实分布&lt;span class=&quot;math inline&quot;&gt;\(p\)&lt;/span&gt;去编码数据时, 平均需要的额外的位数.&lt;/p&gt;
&lt;h3 id=&quot;互信息mutual-information&quot;&gt;互信息(mutual information)&lt;/h3&gt;
&lt;p&gt;含义: 度量联合分布&lt;span class=&quot;math inline&quot;&gt;\(p(X,Y)\)&lt;/span&gt;与&lt;span class=&quot;math inline&quot;&gt;\(p(X)p(Y)\)&lt;/span&gt;的相似程度. &lt;span class=&quot;math display&quot;&gt;\[\mathbb{I}(X;Y) \triangleq \mathbb{KL}(p(X,Y) \vert \vert  p(X)p(Y)) = \sum\limits_x\sum\limits_y p(x,y)\log\frac{p(x,y)}{p(x)p(y)}\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;条件熵conditional-entropy&quot;&gt;条件熵(conditional entropy)&lt;/h3&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[\mathbb{H}(Y\vert X) \triangleq \sum\limits_x p(x)\mathbb{H}(Y\vert X=x)\]&lt;/span&gt; 可以证明 &lt;span class=&quot;math display&quot;&gt;\[\mathbb{I}(X;Y) = \mathbb{H}(X) - \mathbb{H}(X\vert Y) = \mathbb{H}(Y) - \mathbb{H}(Y\vert X)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;下面是几个重要结论&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;span class=&quot;math inline&quot;&gt;\(\mathbb{KL}(p\vert \vert q) \geq 0\)&lt;/span&gt; 等号成立当且仅当&lt;span class=&quot;math inline&quot;&gt;\(p=q\)&lt;/span&gt;.&lt;/strong&gt; 提示: 递推法证明凸函数的Jensen不等式, 对-log函数应用该不等式即可.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;离散随机变量为均匀分布时熵最大, 且最大值为$&lt;span class=&quot;math inline&quot;&gt;\(, 其中\)&lt;/span&gt;&lt;span class=&quot;math inline&quot;&gt;\(\vert 是\)&lt;/span&gt;X$的状态数目.&lt;/strong&gt; 提示: 上述结论的重要推论之一. 计算分布p与均匀分布u的KL散度.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;最大似然估计就是最小化模型与实际分布(empirical distribution)的KL散度.&lt;/strong&gt; 提示: 大数定律 &lt;span class=&quot;math inline&quot;&gt;\(E_{x\sim pemp(x)}\log q(x;\theta)=\frac{1}{N}\sum\limits_{i=1}{N}\log q(x;\theta)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>  黄锡昆</name></author><category term="技术" /><category term="MLAPP" /><category term="机器学习" /><summary type="html">简介 概率两大派: 频率学派(frequentist) 贝叶斯学派(Bayesian) 频率学派将概率解释为事件在多次实验下发生的频率(long run frequencies of events). 贝叶斯学派用概率来量化我们对一些事情的不确定性, 因此概率本质上与信息有关, 而与实验次数无关. 举个例子, 对于“硬币正面朝上的概率为0.5”这句表述, 频率学派是在说如果我们掷硬币很多次, 那么大概有一半的次数硬币朝上. 而贝叶斯学派是在说我们相信下一次掷硬币出现正面和反面的可能性相同. 概率论 本小节简单介绍了一些概率论的基本概念和公式, 这里不多赘述. 只记录一些看了有些启发的. 贝叶斯公式 贝叶斯公式给出的结果往往反“直觉”, 当然这里的“直觉”是错的. 书中正文和习题中给了三个例子. 医疗诊断 &amp;gt; 假设如果人类患有某种疾病y, 那么在检查时指标x为阳性的概率为0.8. 现在如果有个人检测到指标x为阳性, 那么请问: 这个人患有疾病y的概率是多少? 脱口而出0.8? 正确答案是不知道, 因为给的信息不足以做出判断. 我们再知道两个信息就可以做出判断: 人群中该疾病患病率是多少? 一个没患病的人检测到指标x为阳性的概率是多少? 这里我们假设 \(p(y=1)=0.004, p(x=1\vert y=0)=0.1\), 那么由此可以计算出\(p(y=1\vert x=1)=0.031\). 换句话说, 即使指标x被检测出阳性, 这个人也只有大约3%的概率真正患有疾病y! 公诉人与辩护人谬论 &amp;gt; 假设有一起案件, 案发现场发现了凶手的血迹, 经检查该血型是一种及其罕见的类型(比如说人群中只有1%的人拥有这种血型). 那么现在警察抓住了一个嫌疑人, 并且嫌疑人的血型恰好就是这种类型, 那么请问: 在这种情况下, 这个嫌疑人是真正凶手的概率是多少? 这种情况下我们会大概率认为这个嫌疑人就是凶手了. 其实不然, 还是信息不够, 不足以做出判断. The Monty Hall problem &amp;gt; 一共三个箱子编号1,2,3. 其中只有一个箱子里面有奖品, 而且主持人知道哪个箱子有礼品. 现在让你猜奖品在哪个箱子里. 假设你选择了1号箱子, 那么主持人接下来打开2号或3号箱子中的一个, 而且主持人会保证不会打开有奖品的箱子(因为如果打开了有奖品的箱子游戏就没法进行下去了…主持人知道哪个箱子有奖品所以可以保证这一点). 现在主持人问你要不要更换自己的选择, 是继续坚持1号箱子,还是换成另一个箱子呢? 不要犹豫,换! 不换的话赢的概率是1/3,换的话赢的概率是2/3. 独立性 无条件独立 \[X \perp Y \iff p(X,Y)=p(X)p(Y)\] 条件独立 \[X \perp Y \vert Z \iff p(X,Y\vert Z)=p(X\vert Z)p(Y\vert Z)\] \[X \perp Y \vert Z \iff 存在函数g和h使得 p(x,y\vert z)=g(x,z)h(y,z)\] 两两独立(Pairwise Independent) 和 相互独立(Mutually Indepedent) 相互独立: \[p(X_i\vert X_S)=p(X_i), \forall S\subseteq\{1,\dots,n\}\setminus\{i\}\] 两两独立推不出相互独立. 反例: X1和X2都是等概率取值{0,1}的随机变量,X3=XOR(X1,X2). 则X1,X2,X3两两独立但是不是相互独立. 不相关推不出独立. 所以相关系数不能作为衡量独立性的指标, 更好的指标是后面介绍的互信息. 反例: \(X\sim U(-1,1), Y=X^2\) 概率分布 书中介绍了一堆常见的离散型和连续型概率分布, 可是记不住啊…还是要应用… 随机变量的变换 change of variables formula 对这个公式一直理解不到位… 蒙特卡洛近似 通常根据随机变量变换公式来计算随机变量函数的分布是很困难的. 一个简单有效的方法是先从分布\(X\)中采样\(S\)个样本点\(x_1,\dots,x_S\), 然后用\(\{f(x_s\}^S_{s=1}\)的实际分布来近似\(f(X)\)的分布. 这种方法就是蒙塔卡洛近似. 进一步可以搞出蒙特卡洛积分. \[E[f(X)]=\int f(x)p(x)dx \approx \frac{1}{S}\sum_{s=1}^Sf(x_s)\] 其中\(x_s \sim p(X)\). 蒙特卡洛近似的精度随着样本规模的变大而增大. 信息论 熵(entropy) 含义: 度量随机变量的不确定性. \[\mathbb{H}(X) \triangleq -\sum\limits_{k=1}^Kp(X=k)\log_2 p(X=k)\] KL散度(KL divergence) 含义: 度量两个随机变量的差异程度. \[\mathbb{KL}(p\vert \vert q) \triangleq \sum\limits_{k=1}^K p_k \log \frac{p_k}{q_k}\] 交叉熵(cross entropy) 含义: 当数据真实分布是\(p\), 而我们用分布\(q\)编码数据时需要的平均位数. \[\mathbb{H}(p,q) \triangleq -\sum\limits_k p_k\log q_k\] 注意到\(\mathbb{H}(p,p)=\mathbb{H}(p)\), KL散度可以写成 \[\mathbb{KL}(p\vert \vert q) = \mathbb{H}(p,q) - \mathbb{H}(p,p)\] 也就是说KL散度的含义是由于我们用分布\(q\)而不是数据的真实分布\(p\)去编码数据时, 平均需要的额外的位数. 互信息(mutual information) 含义: 度量联合分布\(p(X,Y)\)与\(p(X)p(Y)\)的相似程度. \[\mathbb{I}(X;Y) \triangleq \mathbb{KL}(p(X,Y) \vert \vert p(X)p(Y)) = \sum\limits_x\sum\limits_y p(x,y)\log\frac{p(x,y)}{p(x)p(y)}\] 条件熵(conditional entropy) \[\mathbb{H}(Y\vert X) \triangleq \sum\limits_x p(x)\mathbb{H}(Y\vert X=x)\] 可以证明 \[\mathbb{I}(X;Y) = \mathbb{H}(X) - \mathbb{H}(X\vert Y) = \mathbb{H}(Y) - \mathbb{H}(Y\vert X)\] 下面是几个重要结论 \(\mathbb{KL}(p\vert \vert q) \geq 0\) 等号成立当且仅当\(p=q\). 提示: 递推法证明凸函数的Jensen不等式, 对-log函数应用该不等式即可. 离散随机变量为均匀分布时熵最大, 且最大值为$\(, 其中\)\(\vert 是\)X$的状态数目. 提示: 上述结论的重要推论之一. 计算分布p与均匀分布u的KL散度. 最大似然估计就是最小化模型与实际分布(empirical distribution)的KL散度. 提示: 大数定律 \(E_{x\sim pemp(x)}\log q(x;\theta)=\frac{1}{N}\sum\limits_{i=1}{N}\log q(x;\theta)\)</summary></entry></feed>