---
title: MLAPP笔记-高斯模型
date: 2019-01-05
categories:
    - 技术
tags:
  - MLAPP
  - 机器学习
mathjax: true
---

## 简介

本章讨论多元高斯或者称多元正态(MVN), 数学要求稍高一些.

### 基础

$D$维MVN的pdf为

$$\mathcal{N}(x|\mu,\Sigma) = \frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}}\exp[-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)]$$

上述pdf式中指数里的表达式去掉$-\frac{1}{2}$后实际上就是$x$和$\mu$之间的[Mahalanobis距离](https://en.wikipedia.org/wiki/Mahalanobis_distance), 关于这个距离更多的知识可以参考Wikipedia.

现在考虑这样一个问题: MVN的pdf的等值线是什么样的呢?

- 如果协方差矩阵$\Sigma$是对角阵, 那么Mahalanobis距离就变成了

$$(x-\mu)^T\Sigma^{-1}(x-\mu) = \sum_{i=1}^{D}\frac{1}{\lambda_i}(x_i-\mu_i)^2 = \sum_{i=1}^{D}\frac{1}{\lambda_i} y_i^2$$

其中$\lambda_i$是协方差矩阵的对角元素, $y_i = x_i-\mu_i$
在二维情形下这个等值线就是椭圆. $\frac{1}{\lambda_1}y_1^2 + \frac{1}{\lambda_2} y_2^2 = k$

- 如果协方差不是对角阵, 那么由于它是实对称矩阵, 因此可以正交分解为 $\Sigma = U\Lambda U^T$
 此时$y_i = u_i^T(x-\mu)$

由此也可以看出Mahalanobis距离与欧氏距离的联系.

### MLE for an MVN

最大似然估计是估计MVN的参数的方法之一. 但是最大似然估计有过拟合的缺点,后续会讨论MVN参数的Bayes推断, 这种方法可以消除过拟合, 并且可以为估计提供置信度.

**定理**: 如果$N$个$iid$样本$$\mathbb{x}_i\sim\mathcal{N}(\mu,\Sigma)$$, 那么MVN参数的最大似然估计为
$$\hat{\mu}_{mle} = \frac{1}{N}\sum_{i=1}^{N}\mathbb{x}_i = \bar{\mathbb{x}}$$
$$\hat{\Sigma}_{mle} = \frac{1}{N}\sum_{i=1}^{N}(\mathbb{x_i}-\bar{\mathbb{x}})(\mathbb{x_i}-\bar{\mathbb{x}})^T$$

### 多元高斯的一个有趣性质

对于给定数据, 我们从数据中可能仅仅能可靠地估计出均值与方差. 这个时候我们需要一个分布能够具有估计出的均值与方差, 同时又尽量做出最少的假设, 多元高斯就是我们需要的分布.
**对于给定的均值与方差,多元高斯分布是熵最大的分布**

### 高斯判别分析

多元高斯的一个重要应用是在生成式模型中定义类条件分布, 即
$$p(\mathbb{x}|y=c,\theta)=\mathcal{N}(\mathbb{x}|\mu_c,\Sigma_c)$$
这个被成为高斯判别分析.(实际上是生成模型, 名字容易引起误导). 如果$\Sigma_c$是对角矩阵, 那么就相当于属性条件独立了, 即等价于朴素贝叶斯.
对于新来的数据, 我们根据下式来进行分类
$$\hat{y}(\mathbb{x}) = \arg\max_c[\log p(y=c|\theta) + \log p(\mathbb{x}|y=c,\theta)] $$
其中在计算第二项时, 我们实际上是在计算$\mathbb{x}$与每个类的中心的距离, 只不过是Mahalanobis距离, 这个可以被看成是 nearest centroids classifier.

#### Quadratic discriminant analysis(QDA)

高斯判别分析分类准则展开之后分类边界是关于$\mathbb{x}$的二次函数, 这个被称为QDA.

#### Linear discriminant analysis(LDA)

但是如果假定不同类的协方差矩阵相同, 那么这时候分类边界变成了$\mathbb{x}$的线性函数, 这个被称为LDA.

#### MLE for discriminant analysis

可以利用最大似然估计方法来估计高斯判别分析准则中的参数.
$$\hat{\pi}_c = \frac{N_c}{N}$$
$$\hat{\mu}_c = \frac{1}{N_c}\sum_{i:y_i=c}\mathbb{x}_i$$
$$\hat{\Sigma_c} = \frac{1}{N_c}\sum_{i:y_i=c}(\mathbb{x}_i-\hat{\mu}_c)(\mathbb{x}_i-\hat{\mu}_c)^T$$

#### 防止过拟合的方法

MLE虽然简单快速, 但是有过拟合的缺点. 比如当$N_c<D$时, MLE估计出的协方差矩阵是奇异矩阵, 如果真实协方差矩阵是满秩的, 那么就过拟合了. 对此有一些方法来防止过拟合:

- 假设每个类的协方差矩阵是对角阵, 即假设属性之间条件独立, 这等价于朴素贝叶斯.
- 假设所有类的协方差矩阵相同, 这等价于LDA.
- 假设协方差矩阵是对角阵, 且所有类协方差矩阵相同.
- 使用一般的协方差矩阵, 但是引入先验.
- 使用MAP估计
- 将数据映射到低维空间, 然后在低维空间上使用判别分析.

### Inference in jointly Gaussian distributions

本小节讨论给定联合分布$p(\mathbb{x}_1,\mathbb{x}_2)$, 边际分布$p(\mathbb{x}_1)$和条件分布 $p(\mathbb{x}_1\mid\mathbb{x}_2)$ 是怎样的呢? 如果联合分布是多元高斯分布, 那么边际分布和条件分布仍然是高斯分布. 有如下结论:


> 假设 $\mathbb{x}=(\mathbb{x}_1,\mathbb{x}_2)$ 是多元高斯分布, 参数为
$$\mu = \left(\begin{matrix} \mu_1 \\ \mu_2 \end{matrix}\right)$$
$$\Sigma = \left(\begin{matrix} \Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22} \end{matrix}\right)$$
$$\Lambda = \Sigma^{-1} = \left(\begin{matrix} \Lambda_{11} & \Lambda_{12} \\ \Lambda_{21} & \Lambda_{22} \end{matrix}\right)$$
那么边际分布为
$$p(\mathbb{x}_1) = \mathcal{N}(\mathbb{x}_1|\mu_1, \Sigma_{11})$$
$$p(\mathbb{x}_2) = \mathcal{N}(\mathbb{x}_2|\mu_2, \Sigma_{22})$$
条件分布为
$$p(\mathbb{x}_1|\mathbb{x}_2) = \mathcal{N}(\mathbb{x}_1 | \mu_{1|2}, \Sigma_{1|2})$$
其中
$$\begin{align}
\mu_{1|2} & = \mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(\mathbb{x}_2-\mu_2) \\
          & = \mu_1 - \Lambda_{11}^{-1}\Lambda_{12}(\mathbb{x}_2-\mu_2) \\
          & = \Sigma_{1|2}(\Lambda_{11}\mu_1 - \Lambda_{12}(\mathbb{x}_2-\mu_2))
\end{align}$$
$$\Sigma_{1|2} = \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21} = \Lambda_{11}^{-1}$$

上述结论非常重要.  同时注意到上面用到了不同的参数表示形式. 某些情况下用 moment parameters比较方便, 某些情况下用 canonical parameters 比较方便.

### Linear Gaussian systems

假定有两个变量$\mathbb{x}$和$\mathbb{y}$, $\mathbb{x}\in \mathbb{R}^{D_x}$是隐变量, $\mathbb{y}\in \mathbb{R}^{D_y}$是 noisy observation of $\mathbb{x}$. 假定我们有如下先验和似然
$$p(\mathbb{x}) = \mathcal{N}(\mathbb{x}|\mu_x, \Sigma_x)$$
$$p(\mathbb{y}|\mathbb{x}) = \mathcal{N}(\mathbb{y}|A\mathbb{x} + b, \Sigma_y)$$
这被成为一个线性高斯系统, 现在想要求$\mathbb{x}$的后验分布$p(\mathbb{x}|\mathbb
  {y})$.

有如下结论
$$p(\mathbb{x}|\mathbb{y}) = \mathcal{N}(\mathbb{x}|\mu_{x|y}, \Sigma_{x|y})$$
$$\Sigma_{x|y}^{-1} = \Sigma_{x}^{-1} + A^T\Sigma_{y}^{-1}A$$
$$\mu_{x|y} = \Sigma_{x|y}[A^T\Sigma_{y}^{-1}(\mathbb{y} - b) + \Sigma_{x}^{-1}\mu_{x}]$$

此外
$$p(\mathbb{y}) = \mathcal{N}(\mathbb{y}|A\mu_x + b, \Sigma_y+A\Sigma_xA^T)$$

### Inferring the parameters for an MVN

之前小节提到用最大似然方法估计MVN的参数, 但是具有过拟合的缺点. 这节介绍的是利用贝叶斯方法来推断MVN的参数. 即引入共轭先验分布, 然后计算后验分布, 将后验分布的mode或mean作为参数估计值. 本节考虑了三个情况:

- 知道协方差矩阵, 推断均值
- 知道均值, 推断协方差矩阵
- 均值与协方差矩阵都未知, 同时推断

#### 知道协方差矩阵, 推断均值

> 这种情况下共轭先验为高斯分布

#### 知道均值, 推断协方差矩阵

> 这种情况下共轭先验为 inverse Wishart distribution.

#### 均值与协方差矩阵都未知, 同时推断

> 这种情况下共轭先验为 Normal-inverse-wishart distribution.

具体结果按下不表.
值得一提的是Bayes这种计算后验分布的方法与频率派最大似然估计方法有着形式上的紧密联系. 如果先验分布是uninformative的话, 那么贝叶斯方法计算的结果与频率学派计算的结果相同. 但是只是形式上的相同, 对于结果的解释不同.
