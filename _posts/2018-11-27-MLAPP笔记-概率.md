---
title: MLAPP笔记-概率
date: 2018-11-27
categories:
    - 技术
tags:
    - MLAPP
    - 机器学习
mathjax: true
---

## 简介

概率两大派:

- 频率学派(frequentist)
- 贝叶斯学派(Bayesian)

频率学派将概率解释为事件在多次实验下发生的频率(long run frequencies of events).
贝叶斯学派用概率来量化我们对一些事情的不确定性, 因此概率本质上与信息有关, 而与实验次数无关.

举个例子,  对于"硬币正面朝上的概率为0.5"这句表述, 频率学派是在说如果我们掷硬币很多次, 那么大概有一半的次数硬币朝上. 而贝叶斯学派是在说我们相信下一次掷硬币出现正面和反面的可能性相同.

## 概率论

本小节简单介绍了一些概率论的基本概念和公式, 这里不多赘述. 只记录一些看了有些启发的.

### 贝叶斯公式

贝叶斯公式给出的结果往往反"直觉", 当然这里的"直觉"是错的. 书中正文和习题中给了三个例子.

- 医疗诊断
    > 假设如果人类患有某种疾病y, 那么在检查时指标x为阳性的概率为0.8. 现在如果有个人检测到指标x为阳性, 那么请问: 这个人患有疾病y的概率是多少?

    脱口而出0.8? 正确答案是不知道, 因为给的信息不足以做出判断. 我们再知道两个信息就可以做出判断: 人群中该疾病患病率是多少? 一个没患病的人检测到指标x为阳性的概率是多少? 这里我们假设 $p(y=1)=0.004, p(x=1\vert y=0)=0.1$, 那么由此可以计算出$p(y=1\vert x=1)=0.031$. 换句话说, 即使指标x被检测出阳性, 这个人也只有大约3%的概率真正患有疾病y!

- 公诉人与辩护人谬论
  > 假设有一起案件, 案发现场发现了凶手的血迹, 经检查该血型是一种及其罕见的类型(比如说人群中只有1%的人拥有这种血型). 那么现在警察抓住了一个嫌疑人, 并且嫌疑人的血型恰好就是这种类型, 那么请问: 在这种情况下, 这个嫌疑人是真正凶手的概率是多少?
  
  这种情况下我们会大概率认为这个嫌疑人就是凶手了. 其实不然, 还是信息不够, 不足以做出判断.

- The Monty Hall problem
  > 一共三个箱子编号1,2,3. 其中只有一个箱子里面有奖品, 而且主持人知道哪个箱子有礼品. 现在让你猜奖品在哪个箱子里. 假设你选择了1号箱子, 那么主持人接下来打开2号或3号箱子中的一个, 而且主持人会保证不会打开有奖品的箱子(因为如果打开了有奖品的箱子游戏就没法进行下去了...主持人知道哪个箱子有奖品所以可以保证这一点). 现在主持人问你要不要更换自己的选择, 是继续坚持1号箱子,还是换成另一个箱子呢?
  
  不要犹豫,换! 不换的话赢的概率是1/3,换的话赢的概率是2/3.

### 独立性

- 无条件独立
  $$X \perp Y \iff p(X,Y)=p(X)p(Y)$$

- 条件独立
  $$X \perp Y \vert  Z \iff p(X,Y\vert Z)=p(X\vert Z)p(Y\vert Z)$$
    $$X \perp Y \vert  Z \iff 存在函数g和h使得 p(x,y\vert z)=g(x,z)h(y,z)$$

- 两两独立(Pairwise Independent) 和 相互独立(Mutually Indepedent)
  相互独立: $$p(X_i\vert X_S)=p(X_i), \forall S\subseteq\{1,\dots,n\}\setminus\{i\}$$
  两两独立推不出相互独立.
  反例: X1和X2都是等概率取值{0,1}的随机变量,X3=XOR(X1,X2). 则X1,X2,X3两两独立但是不是相互独立.

- 不相关推不出独立. 所以相关系数不能作为衡量独立性的指标, 更好的指标是后面介绍的互信息.
  反例: $X\sim U(-1,1), Y=X^2$

## 概率分布

书中介绍了一堆常见的离散型和连续型概率分布, 可是记不住啊...还是要应用...

## 随机变量的变换

- change of variables formula
  对这个公式一直理解不到位...

## 蒙特卡洛近似

通常根据随机变量变换公式来计算随机变量函数的分布是很困难的. 一个简单有效的方法是先从分布$X$中采样$S$个样本点$x_1,\dots,x_S$, 然后用$\{f(x_s\}^S_{s=1}$的实际分布来近似$f(X)$的分布. 这种方法就是蒙塔卡洛近似.

进一步可以搞出蒙特卡洛积分.

$$E[f(X)]=\int f(x)p(x)dx \approx \frac{1}{S}\sum_{s=1}^Sf(x_s)$$

其中$x_s \sim p(X)$.

蒙特卡洛近似的精度随着样本规模的变大而增大.

## 信息论

### 熵(entropy)

含义: 度量随机变量的不确定性.
$$\mathbb{H}(X) \triangleq -\sum\limits_{k=1}^Kp(X=k)\log_2 p(X=k)$$

### KL散度(KL divergence)

含义: 度量两个随机变量的差异程度.
$$\mathbb{KL}(p\vert \vert q) \triangleq \sum\limits_{k=1}^K p_k \log \frac{p_k}{q_k}$$

### 交叉熵(cross entropy)

含义: 当数据真实分布是$p$, 而我们用分布$q$编码数据时需要的平均位数.
$$\mathbb{H}(p,q) \triangleq -\sum\limits_k p_k\log q_k$$
注意到$\mathbb{H}(p,p)=\mathbb{H}(p)$, KL散度可以写成
$$\mathbb{KL}(p\vert \vert q) = \mathbb{H}(p,q) - \mathbb{H}(p,p)$$
也就是说KL散度的含义是由于我们用分布$q$而不是数据的真实分布$p$去编码数据时, 平均需要的额外的位数.

### 互信息(mutual information)

含义: 度量联合分布$p(X,Y)$与$p(X)p(Y)$的相似程度.
$$\mathbb{I}(X;Y) \triangleq \mathbb{KL}(p(X,Y) \vert \vert  p(X)p(Y)) = \sum\limits_x\sum\limits_y p(x,y)\log\frac{p(x,y)}{p(x)p(y)}$$

### 条件熵(conditional entropy)

$$\mathbb{H}(Y\vert X) \triangleq \sum\limits_x p(x)\mathbb{H}(Y\vert X=x)$$
可以证明
$$\mathbb{I}(X;Y) = \mathbb{H}(X) - \mathbb{H}(X\vert Y) = \mathbb{H}(Y) - \mathbb{H}(Y\vert X)$$

下面是几个重要结论

- **$\mathbb{KL}(p\vert \vert q) \geq 0$ 等号成立当且仅当$p=q$.**
  提示: 递推法证明凸函数的Jensen不等式, 对-log函数应用该不等式即可.
- **离散随机变量为均匀分布时熵最大, 且最大值为$\log\vert \mathcal{X}\vert $, 其中$\vert \mathcal{X}$\vert 是$X$的状态数目.**
  提示: 上述结论的重要推论之一. 计算分布p与均匀分布u的KL散度.
- **最大似然估计就是最小化模型与实际分布(empirical distribution)的KL散度.**
  提示: 大数定律 $E_{x\sim pemp(x)}\log q(x;\theta)=\frac{1}{N}\sum\limits_{i=1}{N}\log q(x;\theta)$
